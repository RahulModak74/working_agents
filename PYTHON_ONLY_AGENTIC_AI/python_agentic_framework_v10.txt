Python Files Content Aggregation
Generated on 2025-07-21 at 08:16:06
==================================================

==================================================
FILE: audit_logger.py
==================================================

#!/usr/bin/env python3
"""
Enterprise Audit Logger
SOX and ISO 27001 Compliant Audit Trail System
"""

import os
import json
import sqlite3
import hashlib
import gzip
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from pathlib import Path
import threading
from queue import Queue
import time

class AuditLogger:
    """Enterprise-grade audit logging for compliance"""
    
    def __init__(self, audit_db_path: str = "audit_trail.db"):
        self.audit_db_path = audit_db_path
        self.log_queue = Queue()
        self.worker_thread = None
        self.running = False
        
        # Compliance settings
        self.retention_days = int(os.environ.get('AUDIT_RETENTION_DAYS', '2555'))  # 7 years default
        self.encryption_enabled = os.environ.get('AUDIT_ENCRYPTION', 'true').lower() == 'true'
        self.real_time_alerts = os.environ.get('AUDIT_REAL_TIME_ALERTS', 'true').lower() == 'true'
        
        # Initialize database
        self._init_database()
        
        # Start background worker
        self._start_worker()
    
    def _init_database(self):
        """Initialize audit database with compliance schema"""
        with sqlite3.connect(self.audit_db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS audit_events (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    user_id TEXT,
                    user_email TEXT,
                    session_id TEXT,
                    ip_address TEXT,
                    user_agent TEXT,
                    resource TEXT,
                    action TEXT,
                    outcome TEXT NOT NULL,
                    risk_level TEXT DEFAULT 'low',
                    details TEXT,
                    hash_integrity TEXT,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS compliance_reports (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    report_type TEXT NOT NULL,
                    period_start TEXT NOT NULL,
                    period_end TEXT NOT NULL,
                    total_events INTEGER,
                    security_events INTEGER,
                    failed_logins INTEGER,
                    privilege_escalations INTEGER,
                    data_access_events INTEGER,
                    report_data TEXT,
                    generated_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS alert_rules (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    rule_name TEXT NOT NULL,
                    event_pattern TEXT NOT NULL,
                    threshold_count INTEGER DEFAULT 1,
                    time_window_minutes INTEGER DEFAULT 60,
                    severity TEXT DEFAULT 'medium',
                    enabled INTEGER DEFAULT 1,
                    created_at TEXT DEFAULT CURRENT_TIMESTAMP
                )
            ''')
            
            # Create indexes for performance
            conn.execute('CREATE INDEX IF NOT EXISTS idx_timestamp ON audit_events(timestamp)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_user_id ON audit_events(user_id)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_event_type ON audit_events(event_type)')
            conn.execute('CREATE INDEX IF NOT EXISTS idx_outcome ON audit_events(outcome)')
            
            # Insert default alert rules
            self._insert_default_alert_rules(conn)
    
    def _insert_default_alert_rules(self, conn):
        """Insert default compliance alert rules"""
        default_rules = [
            ("Multiple Failed Logins", "failed_login", 5, 15, "high"),
            ("Privilege Escalation", "privilege_escalation", 1, 60, "critical"),
            ("Unusual Data Access", "data_access", 50, 60, "medium"),
            ("Admin Function Access", "admin_function", 10, 60, "medium"),
            ("API Key Misuse", "api_key_failed", 3, 30, "high"),
            ("Bulk Data Export", "data_export", 5, 30, "high"),
            ("Configuration Changes", "config_change", 1, 60, "medium"),
            ("User Creation/Deletion", "user_management", 1, 60, "medium")
        ]
        
        for rule_name, pattern, threshold, window, severity in default_rules:
            conn.execute('''
                INSERT OR IGNORE INTO alert_rules 
                (rule_name, event_pattern, threshold_count, time_window_minutes, severity)
                VALUES (?, ?, ?, ?, ?)
            ''', (rule_name, pattern, threshold, window, severity))
    
    def _start_worker(self):
        """Start background worker for async logging"""
        self.running = True
        self.worker_thread = threading.Thread(target=self._worker_loop, daemon=True)
        self.worker_thread.start()
    
    def _worker_loop(self):
        """Background worker to process audit logs"""
        while self.running:
            try:
                if not self.log_queue.empty():
                    event = self.log_queue.get(timeout=1)
                    self._write_audit_event(event)
                    self.log_queue.task_done()
                else:
                    time.sleep(0.1)
            except Exception as e:
                print(f"Audit worker error: {e}")
    
    def log_event(self, event_type: str, user_id: str = None, user_email: str = None,
                  session_id: str = None, ip_address: str = None, user_agent: str = None,
                  resource: str = None, action: str = None, outcome: str = "success",
                  risk_level: str = "low", details: Dict[str, Any] = None):
        """Log audit event (async)"""
        
        event = {
            "timestamp": datetime.utcnow().isoformat(),
            "event_type": event_type,
            "user_id": user_id,
            "user_email": user_email,
            "session_id": session_id,
            "ip_address": ip_address or "unknown",
            "user_agent": user_agent or "unknown",
            "resource": resource,
            "action": action,
            "outcome": outcome,
            "risk_level": risk_level,
            "details": json.dumps(details) if details else None
        }
        
        # Add integrity hash
        event["hash_integrity"] = self._calculate_hash(event)
        
        # Queue for async processing
        self.log_queue.put(event)
        
        # Real-time alert checking (if enabled)
        if self.real_time_alerts:
            self._check_alert_rules(event)
    
    def _write_audit_event(self, event: Dict[str, Any]):
        """Write audit event to database"""
        try:
            with sqlite3.connect(self.audit_db_path) as conn:
                conn.execute('''
                    INSERT INTO audit_events 
                    (timestamp, event_type, user_id, user_email, session_id, 
                     ip_address, user_agent, resource, action, outcome, 
                     risk_level, details, hash_integrity)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
                ''', (
                    event["timestamp"], event["event_type"], event["user_id"],
                    event["user_email"], event["session_id"], event["ip_address"],
                    event["user_agent"], event["resource"], event["action"],
                    event["outcome"], event["risk_level"], event["details"],
                    event["hash_integrity"]
                ))
        except Exception as e:
            # Fallback to file logging if database fails
            self._fallback_file_log(event, str(e))
    
    def _fallback_file_log(self, event: Dict[str, Any], error: str):
        """Fallback to file logging if database unavailable"""
        fallback_file = f"audit_fallback_{datetime.now().strftime('%Y%m%d')}.log"
        with open(fallback_file, 'a') as f:
            f.write(f"DB_ERROR: {error}\n")
            f.write(f"EVENT: {json.dumps(event)}\n")
    
    def _calculate_hash(self, event: Dict[str, Any]) -> str:
        """Calculate integrity hash for tamper detection"""
        # Remove hash field if present
        event_copy = {k: v for k, v in event.items() if k != "hash_integrity"}
        
        # Create deterministic string
        event_str = json.dumps(event_copy, sort_keys=True)
        
        # Add secret salt for integrity
        salt = os.environ.get('AUDIT_SALT', 'default_salt_change_in_production')
        salted_str = f"{event_str}{salt}"
        
        return hashlib.sha256(salted_str.encode()).hexdigest()
    
    def verify_integrity(self, event_id: int) -> bool:
        """Verify audit event integrity"""
        with sqlite3.connect(self.audit_db_path) as conn:
            cursor = conn.execute('SELECT * FROM audit_events WHERE id = ?', (event_id,))
            row = cursor.fetchone()
            
            if not row:
                return False
            
            # Reconstruct event
            columns = [desc[0] for desc in cursor.description]
            event = dict(zip(columns, row))
            
            stored_hash = event.pop("hash_integrity")
            event.pop("id")  # Remove auto-increment ID
            event.pop("created_at")  # Remove auto timestamp
            
            calculated_hash = self._calculate_hash(event)
            return stored_hash == calculated_hash
    
    def _check_alert_rules(self, event: Dict[str, Any]):
        """Check if event triggers any alert rules"""
        with sqlite3.connect(self.audit_db_path) as conn:
            cursor = conn.execute('SELECT * FROM alert_rules WHERE enabled = 1')
            rules = cursor.fetchall()
            
            for rule in rules:
                rule_id, rule_name, pattern, threshold, window_minutes, severity, enabled, created_at = rule
                
                # Check if event matches pattern
                if pattern in event["event_type"] or (
                    event["outcome"] == "failure" and pattern == "failed_login" and 
                    event["event_type"] in ["login", "authentication"]
                ):
                    # Count recent events matching this pattern
                    window_start = (datetime.utcnow() - timedelta(minutes=window_minutes)).isoformat()
                    
                    count_cursor = conn.execute('''
                        SELECT COUNT(*) FROM audit_events 
                        WHERE event_type LIKE ? AND timestamp >= ?
                    ''', (f"%{pattern}%", window_start))
                    
                    count = count_cursor.fetchone()[0]
                    
                    if count >= threshold:
                        self._trigger_alert(rule_name, event, count, threshold, severity)
    
    def _trigger_alert(self, rule_name: str, event: Dict[str, Any], 
                      count: int, threshold: int, severity: str):
        """Trigger security alert"""
        alert = {
            "timestamp": datetime.utcnow().isoformat(),
            "rule_name": rule_name,
            "severity": severity,
            "event_count": count,
            "threshold": threshold,
            "triggering_event": event,
            "message": f"Alert: {rule_name} - {count} events exceeded threshold of {threshold}"
        }
        
        # Log the alert itself
        self.log_event(
            event_type="security_alert",
            user_id=event.get("user_id"),
            resource="security_monitoring",
            action="alert_triggered",
            outcome="success",
            risk_level=severity,
            details=alert
        )
        
        # Send to monitoring system (implement based on your needs)
        self._send_alert_notification(alert)
    
    def _send_alert_notification(self, alert: Dict[str, Any]):
        """Send alert notification (email, Slack, etc.)"""
        # Implement notification logic here
        print(f"🚨 SECURITY ALERT: {alert['message']}")
        
        # Example: Send to webhook
        webhook_url = os.environ.get('SECURITY_WEBHOOK_URL')
        if webhook_url:
            try:
                import requests
                requests.post(webhook_url, json=alert, timeout=5)
            except Exception as e:
                print(f"Failed to send alert notification: {e}")
    
    def search_events(self, start_date: str = None, end_date: str = None,
                     user_id: str = None, event_type: str = None,
                     outcome: str = None, risk_level: str = None,
                     limit: int = 1000) -> List[Dict[str, Any]]:
        """Search audit events with filters"""
        
        query = "SELECT * FROM audit_events WHERE 1=1"
        params = []
        
        if start_date:
            query += " AND timestamp >= ?"
            params.append(start_date)
        
        if end_date:
            query += " AND timestamp <= ?"
            params.append(end_date)
        
        if user_id:
            query += " AND user_id = ?"
            params.append(user_id)
        
        if event_type:
            query += " AND event_type LIKE ?"
            params.append(f"%{event_type}%")
        
        if outcome:
            query += " AND outcome = ?"
            params.append(outcome)
        
        if risk_level:
            query += " AND risk_level = ?"
            params.append(risk_level)
        
        query += " ORDER BY timestamp DESC LIMIT ?"
        params.append(limit)
        
        with sqlite3.connect(self.audit_db_path) as conn:
            cursor = conn.execute(query, params)
            columns = [desc[0] for desc in cursor.description]
            
            events = []
            for row in cursor.fetchall():
                event = dict(zip(columns, row))
                if event["details"]:
                    try:
                        event["details"] = json.loads(event["details"])
                    except:
                        pass
                events.append(event)
            
            return events
    
    def generate_compliance_report(self, report_type: str = "sox", 
                                 start_date: str = None, 
                                 end_date: str = None) -> Dict[str, Any]:
        """Generate compliance report (SOX, ISO 27001, etc.)"""
        
        if not start_date:
            start_date = (datetime.utcnow() - timedelta(days=30)).isoformat()
        if not end_date:
            end_date = datetime.utcnow().isoformat()
        
        with sqlite3.connect(self.audit_db_path) as conn:
            # Total events
            total_events = conn.execute('''
                SELECT COUNT(*) FROM audit_events 
                WHERE timestamp BETWEEN ? AND ?
            ''', (start_date, end_date)).fetchone()[0]
            
            # Security events
            security_events = conn.execute('''
                SELECT COUNT(*) FROM audit_events 
                WHERE timestamp BETWEEN ? AND ? 
                AND (risk_level IN ('high', 'critical') OR event_type LIKE '%security%')
            ''', (start_date, end_date)).fetchone()[0]
            
            # Failed logins
            failed_logins = conn.execute('''
                SELECT COUNT(*) FROM audit_events 
                WHERE timestamp BETWEEN ? AND ? 
                AND event_type LIKE '%login%' AND outcome = 'failure'
            ''', (start_date, end_date)).fetchone()[0]
            
            # Privilege escalations
            privilege_escalations = conn.execute('''
                SELECT COUNT(*) FROM audit_events 
                WHERE timestamp BETWEEN ? AND ? 
                AND event_type LIKE '%privilege%'
            ''', (start_date, end_date)).fetchone()[0]
            
            # Data access events
            data_access_events = conn.execute('''
                SELECT COUNT(*) FROM audit_events 
                WHERE timestamp BETWEEN ? AND ? 
                AND (resource LIKE '%data%' OR action LIKE '%export%' OR action LIKE '%download%')
            ''', (start_date, end_date)).fetchone()[0]
            
            # Top users by activity
            top_users = conn.execute('''
                SELECT user_email, COUNT(*) as event_count 
                FROM audit_events 
                WHERE timestamp BETWEEN ? AND ? AND user_email IS NOT NULL
                GROUP BY user_email 
                ORDER BY event_count DESC 
                LIMIT 10
            ''', (start_date, end_date)).fetchall()
            
            # Risk distribution
            risk_distribution = conn.execute('''
                SELECT risk_level, COUNT(*) as count 
                FROM audit_events 
                WHERE timestamp BETWEEN ? AND ?
                GROUP BY risk_level
            ''', (start_date, end_date)).fetchall()
            
            # Event timeline
            daily_events = conn.execute('''
                SELECT DATE(timestamp) as date, COUNT(*) as count
                FROM audit_events 
                WHERE timestamp BETWEEN ? AND ?
                GROUP BY DATE(timestamp)
                ORDER BY date
            ''', (start_date, end_date)).fetchall()
        
        report_data = {
            "report_metadata": {
                "report_type": report_type,
                "period_start": start_date,
                "period_end": end_date,
                "generated_at": datetime.utcnow().isoformat(),
                "compliance_standards": ["SOX", "ISO 27001", "GDPR"] if report_type == "comprehensive" else [report_type.upper()]
            },
            "summary": {
                "total_events": total_events,
                "security_events": security_events,
                "failed_logins": failed_logins,
                "privilege_escalations": privilege_escalations,
                "data_access_events": data_access_events,
                "security_incidents": security_events + failed_logins + privilege_escalations
            },
            "user_activity": {
                "top_users": [{"email": email, "event_count": count} for email, count in top_users],
                "unique_users": len(top_users)
            },
            "risk_analysis": {
                "risk_distribution": [{"level": level, "count": count} for level, count in risk_distribution],
                "high_risk_percentage": round((sum(count for level, count in risk_distribution if level in ['high', 'critical']) / total_events * 100), 2) if total_events > 0 else 0
            },
            "timeline": {
                "daily_events": [{"date": date, "count": count} for date, count in daily_events]
            },
            "compliance_status": self._assess_compliance_status(total_events, security_events, failed_logins)
        }
        
        # Store report in database
        with sqlite3.connect(self.audit_db_path) as conn:
            conn.execute('''
                INSERT INTO compliance_reports 
                (report_type, period_start, period_end, total_events, 
                 security_events, failed_logins, privilege_escalations, 
                 data_access_events, report_data)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                report_type, start_date, end_date, total_events,
                security_events, failed_logins, privilege_escalations,
                data_access_events, json.dumps(report_data)
            ))
        
        return report_data
    
    def _assess_compliance_status(self, total_events: int, security_events: int, 
                                 failed_logins: int) -> Dict[str, Any]:
        """Assess compliance status based on metrics"""
        status = "compliant"
        issues = []
        recommendations = []
        
        # Check security event ratio
        if total_events > 0:
            security_ratio = (security_events / total_events) * 100
            if security_ratio > 10:
                status = "attention_required"
                issues.append(f"High security event ratio: {security_ratio:.1f}%")
                recommendations.append("Review security policies and user training")
        
        # Check failed login attempts
        if failed_logins > 50:
            status = "attention_required"
            issues.append(f"High number of failed logins: {failed_logins}")
            recommendations.append("Implement stronger authentication controls")
        
        # Check for critical gaps
        if total_events == 0:
            status = "non_compliant"
            issues.append("No audit events found - audit logging may be disabled")
            recommendations.append("Verify audit logging is properly configured")
        
        return {
            "status": status,
            "issues": issues,
            "recommendations": recommendations,
            "last_assessment": datetime.utcnow().isoformat()
        }
    
    def export_audit_data(self, start_date: str, end_date: str, 
                         format: str = "json") -> str:
        """Export audit data for external compliance systems"""
        
        events = self.search_events(start_date=start_date, end_date=end_date, limit=10000)
        
        export_data = {
            "export_metadata": {
                "start_date": start_date,
                "end_date": end_date,
                "total_events": len(events),
                "exported_at": datetime.utcnow().isoformat(),
                "format": format
            },
            "events": events
        }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        if format.lower() == "json":
            filename = f"audit_export_{timestamp}.json"
            with open(filename, 'w') as f:
                json.dump(export_data, f, indent=2, default=str)
        
        elif format.lower() == "csv":
            filename = f"audit_export_{timestamp}.csv"
            import csv
            with open(filename, 'w', newline='') as f:
                if events:
                    writer = csv.DictWriter(f, fieldnames=events[0].keys())
                    writer.writeheader()
                    writer.writerows(events)
        
        # Compress if large
        if os.path.getsize(filename) > 10 * 1024 * 1024:  # > 10MB
            with open(filename, 'rb') as f_in:
                with gzip.open(f"{filename}.gz", 'wb') as f_out:
                    f_out.writelines(f_in)
            os.remove(filename)
            filename = f"{filename}.gz"
        
        # Log the export
        self.log_event(
            event_type="audit_export",
            resource="audit_data",
            action="export",
            outcome="success",
            risk_level="medium",
            details={
                "filename": filename,
                "format": format,
                "event_count": len(events),
                "date_range": {"start": start_date, "end": end_date}
            }
        )
        
        return filename
    
    def cleanup_old_records(self):
        """Clean up old audit records based on retention policy"""
        cutoff_date = (datetime.utcnow() - timedelta(days=self.retention_days)).isoformat()
        
        with sqlite3.connect(self.audit_db_path) as conn:
            # Archive old records before deletion (optional)
            archive_file = f"audit_archive_{datetime.now().strftime('%Y%m%d')}.json"
            
            cursor = conn.execute('SELECT * FROM audit_events WHERE timestamp < ?', (cutoff_date,))
            old_records = cursor.fetchall()
            
            if old_records:
                columns = [desc[0] for desc in cursor.description]
                archive_data = [dict(zip(columns, row)) for row in old_records]
                
                with open(archive_file, 'w') as f:
                    json.dump(archive_data, f, default=str)
                
                # Delete old records
                result = conn.execute('DELETE FROM audit_events WHERE timestamp < ?', (cutoff_date,))
                deleted_count = result.rowcount
                
                self.log_event(
                    event_type="audit_cleanup",
                    resource="audit_database",
                    action="cleanup",
                    outcome="success",
                    details={
                        "deleted_records": deleted_count,
                        "cutoff_date": cutoff_date,
                        "archive_file": archive_file
                    }
                )
                
                return {
                    "deleted_records": deleted_count,
                    "archive_file": archive_file,
                    "cutoff_date": cutoff_date
                }
        
        return {"deleted_records": 0, "message": "No old records to clean up"}
    
    def get_user_activity_summary(self, user_id: str, days: int = 30) -> Dict[str, Any]:
        """Get detailed user activity summary"""
        start_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
        
        events = self.search_events(
            start_date=start_date,
            user_id=user_id,
            limit=1000
        )
        
        if not events:
            return {"user_id": user_id, "activity": "no_activity", "events": []}
        
        # Analyze activity patterns
        event_types = {}
        risk_levels = {}
        daily_activity = {}
        
        for event in events:
            # Count by event type
            event_type = event["event_type"]
            event_types[event_type] = event_types.get(event_type, 0) + 1
            
            # Count by risk level
            risk_level = event["risk_level"]
            risk_levels[risk_level] = risk_levels.get(risk_level, 0) + 1
            
            # Count by day
            day = event["timestamp"][:10]  # YYYY-MM-DD
            daily_activity[day] = daily_activity.get(day, 0) + 1
        
        return {
            "user_id": user_id,
            "period_days": days,
            "total_events": len(events),
            "event_types": event_types,
            "risk_levels": risk_levels,
            "daily_activity": daily_activity,
            "most_active_day": max(daily_activity.items(), key=lambda x: x[1]) if daily_activity else None,
            "security_events": len([e for e in events if e["risk_level"] in ["high", "critical"]]),
            "failed_actions": len([e for e in events if e["outcome"] == "failure"])
        }
    
    def shutdown(self):
        """Gracefully shutdown audit logger"""
        self.running = False
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
        
        # Process any remaining items in queue
        while not self.log_queue.empty():
            try:
                event = self.log_queue.get_nowait()
                self._write_audit_event(event)
            except:
                break

# Global audit logger instance
audit_logger = AuditLogger()


==================================================
FILE: auth_handler.py
==================================================

#!/usr/bin/env python3
"""
Enterprise Authentication Handler
Supports OAuth2, SAML, and Role-Based Access Control (RBAC)
"""

import os
import jwt
import time
import hashlib
import secrets
import requests
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional, Set
from functools import wraps
import xml.etree.ElementTree as ET
from urllib.parse import urlencode, parse_qs
import base64
import json

class EnterpriseAuth:
    """Enterprise-grade authentication with OAuth2/SAML and RBAC"""
    
    def __init__(self):
        self.secret_key = os.environ.get('ENTERPRISE_SECRET_KEY', secrets.token_hex(32))
        self.sessions = {}  # In production, use Redis
        self.users = {}     # In production, use database
        self.roles = self._init_default_roles()
        self.audit_log = []
        
    def _init_default_roles(self) -> Dict[str, Dict]:
        """Initialize default enterprise roles"""
        return {
            "super_admin": {
                "permissions": ["*"],  # All permissions
                "description": "Full system access"
            },
            "admin": {
                "permissions": [
                    "user:manage", "workflow:manage", "tool:manage", 
                    "audit:view", "system:configure"
                ],
                "description": "Administrative access"
            },
            "analyst": {
                "permissions": [
                    "workflow:create", "workflow:execute", "tool:use", 
                    "research:access", "data:analyze"
                ],
                "description": "Data analysis and workflow execution"
            },
            "viewer": {
                "permissions": [
                    "workflow:view", "audit:view_own", "dashboard:view"
                ],
                "description": "Read-only access"
            },
            "guest": {
                "permissions": ["dashboard:view"],
                "description": "Limited guest access"
            }
        }
    
    def authenticate_oauth2(self, provider: str, auth_code: str, 
                           redirect_uri: str) -> Dict[str, Any]:
        """OAuth2 authentication flow"""
        try:
            # OAuth2 provider configurations
            providers = {
                "google": {
                    "token_url": "https://oauth2.googleapis.com/token",
                    "user_info_url": "https://www.googleapis.com/oauth2/v2/userinfo",
                    "client_id": os.environ.get('GOOGLE_CLIENT_ID'),
                    "client_secret": os.environ.get('GOOGLE_CLIENT_SECRET')
                },
                "microsoft": {
                    "token_url": "https://login.microsoftonline.com/common/oauth2/v2.0/token",
                    "user_info_url": "https://graph.microsoft.com/v1.0/me",
                    "client_id": os.environ.get('MICROSOFT_CLIENT_ID'),
                    "client_secret": os.environ.get('MICROSOFT_CLIENT_SECRET')
                },
                "okta": {
                    "token_url": f"{os.environ.get('OKTA_DOMAIN')}/oauth2/default/v1/token",
                    "user_info_url": f"{os.environ.get('OKTA_DOMAIN')}/oauth2/default/v1/userinfo",
                    "client_id": os.environ.get('OKTA_CLIENT_ID'),
                    "client_secret": os.environ.get('OKTA_CLIENT_SECRET')
                }
            }
            
            if provider not in providers:
                raise ValueError(f"Unsupported OAuth2 provider: {provider}")
            
            config = providers[provider]
            
            # Exchange authorization code for access token
            token_data = {
                "grant_type": "authorization_code",
                "code": auth_code,
                "redirect_uri": redirect_uri,
                "client_id": config["client_id"],
                "client_secret": config["client_secret"]
            }
            
            token_response = requests.post(config["token_url"], data=token_data, timeout=10)
            token_response.raise_for_status()
            tokens = token_response.json()
            
            # Get user information
            headers = {"Authorization": f"Bearer {tokens['access_token']}"}
            user_response = requests.get(config["user_info_url"], headers=headers, timeout=10)
            user_response.raise_for_status()
            user_info = user_response.json()
            
            # Create or update user
            user_email = user_info.get('email') or user_info.get('userPrincipalName')
            user_name = user_info.get('name') or user_info.get('displayName', user_email)
            
            user = self._create_or_update_user(
                email=user_email,
                name=user_name,
                provider=provider,
                provider_id=user_info.get('id', user_info.get('sub'))
            )
            
            # Create session
            session_token = self._create_session(user)
            
            self._audit_log("oauth2_login", {
                "user_email": user_email,
                "provider": provider,
                "success": True
            })
            
            return {
                "success": True,
                "user": user,
                "session_token": session_token,
                "expires_in": 3600
            }
            
        except Exception as e:
            self._audit_log("oauth2_login", {
                "provider": provider,
                "error": str(e),
                "success": False
            })
            return {
                "success": False,
                "error": f"OAuth2 authentication failed: {str(e)}"
            }
    
    def authenticate_saml(self, saml_response: str) -> Dict[str, Any]:
        """SAML authentication flow"""
        try:
            # Decode SAML response
            saml_decoded = base64.b64decode(saml_response).decode('utf-8')
            
            # Parse SAML XML
            root = ET.fromstring(saml_decoded)
            
            # Extract user attributes (simplified - production needs proper SAML parsing)
            namespaces = {
                'saml': 'urn:oasis:names:tc:SAML:2.0:assertion',
                'samlp': 'urn:oasis:names:tc:SAML:2.0:protocol'
            }
            
            # Find assertion
            assertion = root.find('.//saml:Assertion', namespaces)
            if assertion is None:
                raise ValueError("No SAML assertion found")
            
            # Extract subject (user identifier)
            subject = assertion.find('.//saml:Subject/saml:NameID', namespaces)
            if subject is None:
                raise ValueError("No SAML subject found")
            
            user_email = subject.text
            
            # Extract attributes
            attributes = {}
            for attr in assertion.findall('.//saml:AttributeStatement/saml:Attribute', namespaces):
                attr_name = attr.get('Name')
                attr_value = attr.find('saml:AttributeValue', namespaces)
                if attr_value is not None:
                    attributes[attr_name] = attr_value.text
            
            # Create or update user
            user_name = attributes.get('displayName', attributes.get('name', user_email))
            
            user = self._create_or_update_user(
                email=user_email,
                name=user_name,
                provider="saml",
                provider_id=user_email,
                attributes=attributes
            )
            
            # Create session
            session_token = self._create_session(user)
            
            self._audit_log("saml_login", {
                "user_email": user_email,
                "success": True
            })
            
            return {
                "success": True,
                "user": user,
                "session_token": session_token,
                "expires_in": 3600
            }
            
        except Exception as e:
            self._audit_log("saml_login", {
                "error": str(e),
                "success": False
            })
            return {
                "success": False,
                "error": f"SAML authentication failed: {str(e)}"
            }
    
    def _create_or_update_user(self, email: str, name: str, provider: str, 
                              provider_id: str, attributes: Dict = None) -> Dict[str, Any]:
        """Create or update user in the system"""
        user_id = hashlib.sha256(email.encode()).hexdigest()[:16]
        
        # Determine role based on email domain or attributes
        role = self._determine_user_role(email, attributes or {})
        
        user = {
            "user_id": user_id,
            "email": email,
            "name": name,
            "role": role,
            "provider": provider,
            "provider_id": provider_id,
            "created_at": datetime.utcnow().isoformat(),
            "last_login": datetime.utcnow().isoformat(),
            "attributes": attributes or {},
            "permissions": self.roles[role]["permissions"]
        }
        
        self.users[user_id] = user
        return user
    
    def _determine_user_role(self, email: str, attributes: Dict) -> str:
        """Determine user role based on email domain or SAML attributes"""
        # Check SAML attributes first
        if 'role' in attributes:
            saml_role = attributes['role'].lower()
            if saml_role in self.roles:
                return saml_role
        
        # Check email domain patterns
        domain = email.split('@')[1] if '@' in email else ''
        
        # Enterprise role mapping (customize for your organization)
        role_mappings = {
            'admin.company.com': 'super_admin',
            'company.com': 'admin',
            'contractor.company.com': 'analyst'
        }
        
        return role_mappings.get(domain, 'viewer')  # Default to viewer
    
    def _create_session(self, user: Dict[str, Any]) -> str:
        """Create JWT session token"""
        payload = {
            "user_id": user["user_id"],
            "email": user["email"],
            "role": user["role"],
            "permissions": user["permissions"],
            "iat": int(time.time()),
            "exp": int(time.time()) + 3600  # 1 hour expiry
        }
        
        token = jwt.encode(payload, self.secret_key, algorithm='HS256')
        
        # Store session (in production, use Redis with TTL)
        self.sessions[token] = {
            "user": user,
            "created_at": datetime.utcnow(),
            "last_activity": datetime.utcnow()
        }
        
        return token
    
    def validate_session(self, token: str) -> Dict[str, Any]:
        """Validate JWT session token"""
        try:
            # Decode JWT
            payload = jwt.decode(token, self.secret_key, algorithms=['HS256'])
            
            # Check if session exists
            if token not in self.sessions:
                return {"valid": False, "error": "Session not found"}
            
            session = self.sessions[token]
            
            # Update last activity
            session["last_activity"] = datetime.utcnow()
            
            return {
                "valid": True,
                "user": session["user"],
                "permissions": payload["permissions"]
            }
            
        except jwt.ExpiredSignatureError:
            return {"valid": False, "error": "Token expired"}
        except jwt.InvalidTokenError:
            return {"valid": False, "error": "Invalid token"}
    
    def check_permission(self, user_permissions: List[str], 
                        required_permission: str) -> bool:
        """Check if user has required permission"""
        # Super admin has all permissions
        if "*" in user_permissions:
            return True
        
        # Check exact permission match
        if required_permission in user_permissions:
            return True
        
        # Check wildcard permissions (e.g., "workflow:*" allows "workflow:create")
        for perm in user_permissions:
            if perm.endswith(":*"):
                permission_prefix = perm[:-1]  # Remove "*"
                if required_permission.startswith(permission_prefix):
                    return True
        
        return False
    
    def require_permission(self, permission: str):
        """Decorator to require specific permission"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # Get token from request (this would be from request headers in real app)
                token = kwargs.get('_auth_token') or getattr(func, '_current_token', None)
                
                if not token:
                    return {"error": "Authentication required", "code": 401}
                
                session = self.validate_session(token)
                if not session["valid"]:
                    return {"error": session["error"], "code": 401}
                
                if not self.check_permission(session["permissions"], permission):
                    self._audit_log("permission_denied", {
                        "user_id": session["user"]["user_id"],
                        "permission": permission,
                        "function": func.__name__
                    })
                    return {"error": f"Permission denied: {permission}", "code": 403}
                
                # Add user context to function
                kwargs['_current_user'] = session["user"]
                kwargs['_user_permissions'] = session["permissions"]
                
                result = func(*args, **kwargs)
                
                # Log successful access
                self._audit_log("function_access", {
                    "user_id": session["user"]["user_id"],
                    "function": func.__name__,
                    "permission": permission,
                    "success": True
                })
                
                return result
            return wrapper
        return decorator
    
    def logout(self, token: str) -> Dict[str, Any]:
        """Logout user and invalidate session"""
        if token in self.sessions:
            user = self.sessions[token]["user"]
            del self.sessions[token]
            
            self._audit_log("logout", {
                "user_id": user["user_id"],
                "success": True
            })
            
            return {"success": True, "message": "Logged out successfully"}
        
        return {"success": False, "error": "Session not found"}
    
    def get_auth_urls(self) -> Dict[str, str]:
        """Get OAuth2 authorization URLs for supported providers"""
        base_url = os.environ.get('BASE_URL', 'http://localhost:8000')
        redirect_uri = f"{base_url}/auth/callback"
        
        urls = {}
        
        # Google OAuth2
        if os.environ.get('GOOGLE_CLIENT_ID'):
            google_params = {
                'client_id': os.environ.get('GOOGLE_CLIENT_ID'),
                'redirect_uri': redirect_uri,
                'scope': 'openid email profile',
                'response_type': 'code',
                'access_type': 'offline'
            }
            urls['google'] = f"https://accounts.google.com/o/oauth2/v2/auth?{urlencode(google_params)}"
        
        # Microsoft OAuth2
        if os.environ.get('MICROSOFT_CLIENT_ID'):
            microsoft_params = {
                'client_id': os.environ.get('MICROSOFT_CLIENT_ID'),
                'redirect_uri': redirect_uri,
                'scope': 'openid email profile',
                'response_type': 'code'
            }
            urls['microsoft'] = f"https://login.microsoftonline.com/common/oauth2/v2.0/authorize?{urlencode(microsoft_params)}"
        
        # SAML
        if os.environ.get('SAML_SSO_URL'):
            urls['saml'] = os.environ.get('SAML_SSO_URL')
        
        return urls
    
    def _audit_log(self, action: str, details: Dict[str, Any]):
        """Log audit events for compliance"""
        audit_entry = {
            "timestamp": datetime.utcnow().isoformat(),
            "action": action,
            "details": details,
            "ip_address": details.get("ip_address", "unknown"),
            "user_agent": details.get("user_agent", "unknown")
        }
        
        self.audit_log.append(audit_entry)
        
        # In production, write to secure audit database
        print(f"AUDIT: {audit_entry}")
    
    def get_audit_logs(self, start_date: str = None, end_date: str = None, 
                      user_id: str = None) -> List[Dict[str, Any]]:
        """Retrieve audit logs for compliance reporting"""
        logs = self.audit_log.copy()
        
        # Filter by date range
        if start_date:
            logs = [log for log in logs if log["timestamp"] >= start_date]
        if end_date:
            logs = [log for log in logs if log["timestamp"] <= end_date]
        
        # Filter by user
        if user_id:
            logs = [log for log in logs if log["details"].get("user_id") == user_id]
        
        return logs
    
    def create_api_key(self, user_id: str, name: str, permissions: List[str] = None) -> Dict[str, Any]:
        """Create API key for programmatic access"""
        api_key = f"ent_{secrets.token_urlsafe(32)}"
        
        user = self.users.get(user_id)
        if not user:
            return {"error": "User not found"}
        
        # Use user permissions if not specified
        if permissions is None:
            permissions = user["permissions"]
        
        # Validate permissions don't exceed user's permissions
        for perm in permissions:
            if not self.check_permission(user["permissions"], perm):
                return {"error": f"Cannot grant permission: {perm}"}
        
        api_key_data = {
            "api_key": api_key,
            "name": name,
            "user_id": user_id,
            "permissions": permissions,
            "created_at": datetime.utcnow().isoformat(),
            "last_used": None,
            "active": True
        }
        
        # Store API key (in production, hash the key)
        if not hasattr(self, 'api_keys'):
            self.api_keys = {}
        self.api_keys[api_key] = api_key_data
        
        self._audit_log("api_key_created", {
            "user_id": user_id,
            "api_key_name": name,
            "permissions": permissions
        })
        
        return {
            "success": True,
            "api_key": api_key,
            "expires_at": "never"  # API keys don't expire by default
        }
    
    def validate_api_key(self, api_key: str) -> Dict[str, Any]:
        """Validate API key for programmatic access"""
        if not hasattr(self, 'api_keys') or api_key not in self.api_keys:
            return {"valid": False, "error": "Invalid API key"}
        
        key_data = self.api_keys[api_key]
        
        if not key_data["active"]:
            return {"valid": False, "error": "API key disabled"}
        
        # Update last used
        key_data["last_used"] = datetime.utcnow().isoformat()
        
        return {
            "valid": True,
            "user_id": key_data["user_id"],
            "permissions": key_data["permissions"]
        }

# Global enterprise auth instance
enterprise_auth = EnterpriseAuth()


==================================================
FILE: auto_import_tools.py
==================================================

#!/usr/bin/env python3
"""
Auto Import All Tools from COMPONENT Directory
One import to rule them all!
"""

import sys
import os
import importlib.util
from pathlib import Path
import logging

logger = logging.getLogger("auto_import")

class ToolImporter:
    """Automatically import all tools from COMPONENT directory"""
    
    def __init__(self):
        self.tools = {}
        self.component_dir = Path(__file__).parent / "COMPONENT"
        
    def import_all_tools(self):
        """Import all tools and make them available as simple functions"""
        
        if not self.component_dir.exists():
            print(f"❌ COMPONENT directory not found: {self.component_dir}")
            return {}
        
        # Add COMPONENT to path
        if str(self.component_dir) not in sys.path:
            sys.path.insert(0, str(self.component_dir))
        
        # Find all Python files in COMPONENT
        tool_files = list(self.component_dir.glob("*.py"))
        print(f"🔍 Found {len(tool_files)} tool files in COMPONENT/")
        
        imported_tools = {}
        
        for py_file in tool_files:
            if py_file.name.startswith('__'):
                continue
                
            module_name = py_file.stem
            
            try:
                # Import the module
                spec = importlib.util.spec_from_file_location(module_name, py_file)
                module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(module)
                
                # Get TOOL_REGISTRY if available
                if hasattr(module, 'TOOL_REGISTRY'):
                    registry = module.TOOL_REGISTRY
                    for tool_name, tool_func in registry.items():
                        # Clean up tool name
                        clean_name = tool_name.replace(':', '_')
                        imported_tools[clean_name] = tool_func
                        imported_tools[tool_name] = tool_func  # Keep original too
                
                # Also import any function with docstring (auto-discovery)
                for attr_name in dir(module):
                    if not attr_name.startswith('_'):
                        attr = getattr(module, attr_name)
                        if callable(attr) and hasattr(attr, '__doc__') and attr.__doc__:
                            imported_tools[f"{module_name}_{attr_name}"] = attr
                
                print(f"✅ Imported {module_name}")
                
            except Exception as e:
                print(f"❌ Failed to import {module_name}: {e}")
        
        self.tools = imported_tools
        print(f"🎉 Total tools available: {len(imported_tools)}")
        
        return imported_tools

# Global instance
_tool_importer = ToolImporter()
_tools = _tool_importer.import_all_tools()

# Make all tools available as globals
globals().update(_tools)

# Also create a simple function to call any tool
def call_tool(tool_name, **kwargs):
    """Call any tool by name"""
    if tool_name in _tools:
        return _tools[tool_name](**kwargs)
    else:
        available = [name for name in _tools.keys() if tool_name.lower() in name.lower()]
        return {"error": f"Tool '{tool_name}' not found. Similar: {available[:5]}"}

def list_tools():
    """List all available tools"""
    return sorted(list(_tools.keys()))

def get_tools_by_prefix(prefix):
    """Get tools starting with prefix"""
    return [name for name in _tools.keys() if name.startswith(prefix)]

# Print available tools on import
print(f"🔧 Available tool prefixes:")
prefixes = set()
for tool_name in _tools.keys():
    if '_' in tool_name:
        prefix = tool_name.split('_')[0]
        prefixes.add(prefix)
    elif ':' in tool_name:
        prefix = tool_name.split(':')[0] 
        prefixes.add(prefix)

for prefix in sorted(prefixes):
    count = len(get_tools_by_prefix(prefix))
    print(f"   {prefix}: {count} tools")

print(f"\n💡 Usage examples:")
print(f"   from auto_import_tools import *")
print(f"   result = browser_create(headless=True)")
print(f"   result = research_combined_search(query='AI safety')")
print(f"   result = call_tool('browser:navigate', url='https://example.com')")


==================================================
FILE: config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "qwen2.5:7b",
    "api_key": "",  # Ollama doesn't need API key
    "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    "timeout": 1200  # Increase timeout
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: crypto_workflow.py
==================================================

#!/usr/bin/env python3
"""
Cryptocurrency Analysis using Cognitive Planning + LLM
Fixed version with proper async handling
"""

import asyncio
from auto_import_tools import *
from llm_powered_solution import LLMAgent

async def analyze_crypto_with_cognition():
    """Main async function for crypto analysis"""
    
    print("🧠 Starting cognitive cryptocurrency analysis...")
    
    # Create an LLM agent that uses cognitive planning
    researcher = LLMAgent("CognitiveResearcher", 
        "You are an expert cryptocurrency researcher using advanced cognitive planning techniques.")
    
    print("🎯 Creating cognitive planning session...")
    
    # LLM decides which cognitive pattern to use
    cognitive_session = cognitive_create_session(
        "tree_of_thoughts", 
        problem_description="Analyze cryptocurrency market trends and investment opportunities"
    )
    
    if "error" in cognitive_session:
        print(f"❌ Error creating cognitive session: {cognitive_session['error']}")
        return
    
    print(f"✅ Cognitive session created: {cognitive_session['session_id']}")
    print(f"📋 Pattern: {cognitive_session['pattern']}")
    print(f"📖 Description: {cognitive_session['description']}")
    
    # Work through the cognitive workflow
    step_count = 0
    max_steps = 10  # Safety limit
    
    while step_count < max_steps:
        print(f"\n🔄 Getting next cognitive step ({step_count + 1})...")
        
        next_step = cognitive_get_next_step(cognitive_session["session_id"])
        
        if next_step["status"] == "completed":
            print("✅ Cognitive workflow completed!")
            break
        elif next_step["status"] == "dynamic_step":
            print(f"🎭 Dynamic step - need to select action")
            print(f"Available actions: {next_step['available_actions']}")
            
            # For demo, select first available action
            if next_step['available_actions']:
                selected_action = next_step['available_actions'][0]
                print(f"🎯 Selecting action: {selected_action}")
                
                action_result = cognitive_select_action(
                    cognitive_session["session_id"], 
                    selected_action
                )
                print(f"Action result: {action_result}")
            continue
            
        elif next_step["status"] == "ready":
            print(f"🤖 Processing step: {next_step['agent_name']}")
            
            # Get the step content
            step_details = next_step.get("step_details", {})
            step_content = step_details.get("content", "Analyze cryptocurrency market trends")
            
            print(f"📝 Step prompt: {step_content[:100]}...")
            
            # LLM processes the cognitive step
            try:
                result = await researcher.call_llm(step_content)
                print(f"✅ LLM response received ({len(result)} characters)")
                
                # Submit the result
                submission = cognitive_submit_result(
                    cognitive_session["session_id"], 
                    next_step["agent_name"], 
                    {"analysis": result, "step_content": step_content}
                )
                
                print(f"📤 Result submitted: {submission['message']}")
                
            except Exception as e:
                print(f"❌ Error in LLM processing: {e}")
                # Submit error result
                cognitive_submit_result(
                    cognitive_session["session_id"], 
                    next_step["agent_name"], 
                    {"error": str(e), "step_content": step_content}
                )
        else:
            print(f"⚠️ Unknown step status: {next_step['status']}")
            break
        
        step_count += 1
    
    # Get the complete cognitive analysis
    print("\n📊 Retrieving final cognitive analysis...")
    final_results = cognitive_get_results(cognitive_session["session_id"])
    
    if "error" in final_results:
        print(f"❌ Error getting results: {final_results['error']}")
        return
    
    print("✅ Cognitive analysis complete!")
    print(f"📈 Results from {len(final_results['results'])} cognitive agents")
    
    # Display summary of results
    print("\n🧠 Cognitive Analysis Summary:")
    for agent_name, result in final_results['results'].items():
        print(f"\n  🤖 {agent_name}:")
        if isinstance(result, dict):
            if "analysis" in result:
                analysis = result["analysis"]
                preview = analysis[:200] + "..." if len(analysis) > 200 else analysis
                print(f"     {preview}")
            elif "error" in result:
                print(f"     ❌ Error: {result['error']}")
            else:
                print(f"     📋 Keys: {list(result.keys())}")
        else:
            print(f"     📝 {str(result)[:100]}...")
    
    # Transform to research plan
    print("\n🔬 Transforming to research plan...")
    try:
        research_plan = cognitive_transform_for_research(
            cognitive_session["session_id"], 
            "cryptocurrency market analysis"
        )
        
        if "error" not in research_plan:
            print("✅ Research plan generated!")
            if "research_plan" in research_plan:
                plan = research_plan["research_plan"]
                print(f"🎯 Goal: {plan.get('goal', 'Not specified')}")
                print(f"❓ Key Questions: {len(plan.get('key_questions', []))}")
                print(f"📚 Subtopics: {len(plan.get('subtopics', []))}")
        else:
            print(f"⚠️ Research plan generation failed: {research_plan['error']}")
    
    except Exception as e:
        print(f"⚠️ Research plan transformation failed: {e}")
    
    # Execute basic research using the insights
    print("\n🔍 Executing research based on cognitive insights...")
    try:
        research_results = research_combined_search(
            "cryptocurrency market trends analysis", 
            num_results=5
        )
        
        if "error" not in research_results:
            search_results = research_results.get("search_results", [])
            print(f"✅ Research completed: {len(search_results)} sources found")
            
            # Show research summary
            for i, result in enumerate(search_results[:3]):
                print(f"  📰 Source {i+1}: {result.get('title', 'No title')}")
        else:
            print(f"⚠️ Research failed: {research_results['error']}")
    
    except Exception as e:
        print(f"⚠️ Research execution failed: {e}")
    
    return {
        "cognitive_analysis": final_results,
        "research_results": research_results if 'research_results' in locals() else None,
        "research_plan": research_plan if 'research_plan' in locals() else None
    }

def main():
    """Main entry point - handles existing event loops"""
    print("🚀 Cryptocurrency Cognitive Analysis Framework")
    print("=" * 60)
    
    try:
        # Check if event loop is already running
        try:
            loop = asyncio.get_running_loop()
            print("⚠️ Event loop already running, creating task...")
            # If we're in a running loop, create a task
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, analyze_crypto_with_cognition())
                result = future.result(timeout=300)  # 5 minute timeout
        except RuntimeError:
            # No event loop running, safe to use asyncio.run()
            print("✅ Creating new event loop...")
            result = asyncio.run(analyze_crypto_with_cognition())
        
        if result:
            print("\n🎉 Analysis completed successfully!")
            
            # Save results to file
            import json
            output_file = "crypto_cognitive_analysis.json"
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, default=str)
            
            print(f"📁 Results saved to: {output_file}")
        else:
            print("\n❌ Analysis failed!")
    
    except Exception as e:
        print(f"\n💥 Fatal error: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


==================================================
FILE: diagnostic_data_issues.py
==================================================

#!/usr/bin/env python3
"""
Quick diagnosis script to identify why data collection failed
"""

import sys
from pathlib import Path

# Add framework to path
framework_dir = Path(__file__).parent
sys.path.insert(0, str(framework_dir))

from tool_manager import tool_manager

def diagnose_research_tools():
    """Test research functionality step by step"""
    print("🔍 DIAGNOSING RESEARCH TOOLS")
    print("=" * 50)
    
    # Test 1: Basic research search
    print("\n1. Testing basic research search...")
    result = tool_manager.execute_tool(
        "research:search", 
        query="cryptocurrency market trends", 
        num_results=3
    )
    
    print(f"   Status: {result.get('status', 'unknown')}")
    print(f"   Results: {result.get('num_results', 0)}")
    
    if result.get('status') == 'success' and result.get('results'):
        print("   ✅ Basic search working")
        first_url = result['results'][0].get('link', '')
        print(f"   First URL: {first_url}")
        
        # Test 2: Content fetching
        if first_url:
            print("\n2. Testing content fetching...")
            content_result = tool_manager.execute_tool(
                "research:fetch_content",
                url=first_url
            )
            
            print(f"   Status: {content_result.get('status', 'unknown')}")
            if content_result.get('status') == 'success':
                content_length = len(content_result.get('content', ''))
                print(f"   ✅ Content fetch working: {content_length} chars")
            else:
                print(f"   ❌ Content fetch failed: {content_result.get('error')}")
        
        # Test 3: Combined search (the one used in workflow)
        print("\n3. Testing combined search (used in workflow)...")
        combined_result = tool_manager.execute_tool(
            "research:combined_search",
            query="options trading market trends 2024",
            num_results=5
        )
        
        print(f"   Status: {combined_result.get('status', 'unknown')}")
        print(f"   Search Results: {len(combined_result.get('search_results', []))}")
        print(f"   Content Results: {len(combined_result.get('content_results', []))}")
        
        if combined_result.get('status') == 'success':
            search_count = len(combined_result.get('search_results', []))
            content_count = len(combined_result.get('content_results', []))
            print(f"   ✅ Combined search: {search_count} searches, {content_count} content")
            
            if search_count == 0:
                print("   ⚠️ ISSUE FOUND: No search results for options trading query")
                print("   This explains why your workflow got 0 sources!")
                
                # Test with simpler query
                print("\n   Testing with simpler query...")
                simple_result = tool_manager.execute_tool(
                    "research:search",
                    query="trading",
                    num_results=3
                )
                print(f"   Simple query results: {simple_result.get('num_results', 0)}")
                
        else:
            print(f"   ❌ Combined search failed: {combined_result.get('error')}")
            print("   🎯 ROOT CAUSE FOUND: Combined search not working")
            
    else:
        print(f"   ❌ Basic search failed: {result.get('error')}")
        print("   🎯 ROOT CAUSE FOUND: Basic research broken")

def diagnose_browser_tools():
    """Test browser functionality"""
    print("\n\n🌐 DIAGNOSING BROWSER TOOLS")
    print("=" * 50)
    
    # Test browser creation
    print("\n1. Testing browser creation...")
    result = tool_manager.execute_tool("browser:create", browser_id="diagnosis")
    
    if result.get('status') == 'success':
        print("   ✅ Browser creation working")
        
        # Test navigation to simple site
        print("\n2. Testing navigation to simple site...")
        nav_result = tool_manager.execute_tool(
            "browser:navigate",
            url="https://httpbin.org/html",
            browser_id="diagnosis"
        )
        
        if nav_result.get('status') == 'success':
            print(f"   ✅ Navigation working: {nav_result.get('title')}")
            
            # Test financial site navigation
            print("\n3. Testing financial site navigation...")
            finance_result = tool_manager.execute_tool(
                "browser:navigate",
                url="https://finance.yahoo.com/quote/SPY/options",
                browser_id="diagnosis"
            )
            
            if finance_result.get('status') == 'success':
                print(f"   ✅ Financial site access: {finance_result.get('title')}")
                
                # Test content extraction
                content_result = tool_manager.execute_tool(
                    "browser:get_content",
                    browser_id="diagnosis"
                )
                
                if content_result.get('status') == 'success':
                    content_length = len(content_result.get('content', ''))
                    print(f"   ✅ Content extraction: {content_length} chars")
                    
                    # Check if content contains useful data
                    content = content_result.get('content', '').lower()
                    if any(word in content for word in ['option', 'call', 'put', 'strike']):
                        print("   ✅ Content contains options data")
                    else:
                        print("   ⚠️ Content missing options data - site may be blocking")
                        
                else:
                    print(f"   ❌ Content extraction failed: {content_result.get('error')}")
            else:
                print(f"   ❌ Financial site blocked: {finance_result.get('error')}")
                print("   🎯 ISSUE: Financial sites blocking simple HTTP requests")
        else:
            print(f"   ❌ Navigation failed: {nav_result.get('error')}")
        
        # Cleanup
        tool_manager.execute_tool("browser:close", browser_id="diagnosis")
        
    else:
        print(f"   ❌ Browser creation failed: {result.get('error')}")

def diagnose_tool_discovery():
    """Check if tools are properly discovered"""
    print("\n\n🔧 DIAGNOSING TOOL DISCOVERY")
    print("=" * 50)
    
    all_tools = tool_manager.get_all_tools()
    print(f"Total tools discovered: {len(all_tools)}")
    
    research_tools = tool_manager.get_tools_by_prefix("research")
    browser_tools = tool_manager.get_tools_by_prefix("browser")
    
    print(f"Research tools: {len(research_tools)} - {research_tools}")
    print(f"Browser tools: {len(browser_tools)} - {browser_tools}")
    
    if len(research_tools) == 0:
        print("❌ CRITICAL: No research tools found!")
    if len(browser_tools) == 0:
        print("❌ CRITICAL: No browser tools found!")

def main():
    """Run complete diagnosis"""
    print("🚀 DATA COLLECTION DIAGNOSIS")
    print("=" * 60)
    print("This will identify why your options workflow got 0 data sources")
    print("=" * 60)
    
    try:
        # Discover tools first
        tool_count = tool_manager.discover_tools()
        print(f"Discovered {tool_count} tools")
        
        # Run diagnostics
        diagnose_tool_discovery()
        diagnose_research_tools()
        diagnose_browser_tools()
        
        print("\n" + "=" * 60)
        print("📋 DIAGNOSIS SUMMARY")
        print("=" * 60)
        print("Check the output above for:")
        print("❌ Failed tests = Root cause of data collection failure")
        print("✅ Successful tests = These components are working")
        print("\nMost likely issues:")
        print("1. DuckDuckGo API returning empty results for financial queries")
        print("2. Financial websites blocking simple HTTP requests")
        print("3. Missing API keys for better search engines")
        
    except Exception as e:
        print(f"❌ Diagnosis failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


==================================================
FILE: encryption.py
==================================================

#!/usr/bin/env python3
"""
Enterprise Encryption Manager
Data encryption at rest and in transit for compliance
"""

import os
import json
import base64
import hashlib
import secrets
from cryptography.fernet import Fernet
from cryptography.hazmat.primitives import hashes, serialization
from cryptography.hazmat.primitives.asymmetric import rsa, padding
from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from typing import Dict, Any, Optional, Tuple
import sqlite3
from datetime import datetime, timedelta

class EnterpriseEncryption:
    """Enterprise-grade encryption for data protection"""
    
    def __init__(self):
        self.key_db_path = "encryption_keys.db"
        self.master_key = self._get_or_create_master_key()
        self.encryption_keys = {}
        
        # Initialize key database
        self._init_key_database()
        
        # Load encryption keys
        self._load_encryption_keys()
    
    def _init_key_database(self):
        """Initialize encryption key database"""
        with sqlite3.connect(self.key_db_path) as conn:
            conn.execute('''
                CREATE TABLE IF NOT EXISTS encryption_keys (
                    key_id TEXT PRIMARY KEY,
                    key_purpose TEXT NOT NULL,
                    encrypted_key BLOB NOT NULL,
                    key_algorithm TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    expires_at TEXT,
                    rotation_count INTEGER DEFAULT 0,
                    status TEXT DEFAULT 'active'
                )
            ''')
            
            conn.execute('''
                CREATE TABLE IF NOT EXISTS key_usage_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    key_id TEXT NOT NULL,
                    operation TEXT NOT NULL,
                    user_id TEXT,
                    timestamp TEXT NOT NULL,
                    success INTEGER NOT NULL
                )
            ''')
    
    def _get_or_create_master_key(self) -> bytes:
        """Get or create master encryption key"""
        master_key_file = "master.key"
        
        if os.path.exists(master_key_file):
            # Load existing master key
            with open(master_key_file, 'rb') as f:
                encrypted_master = f.read()
            
            # Derive key from environment variable or prompt
            password = os.environ.get('MASTER_KEY_PASSWORD')
            if not password:
                # In production, use HSM or secure key management
                password = "change_this_in_production_use_env_var"
            
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=b'enterprise_salt_change_in_prod',
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(password.encode()))
            
            try:
                fernet = Fernet(key)
                master_key = fernet.decrypt(encrypted_master)
                return master_key
            except:
                raise ValueError("Invalid master key password")
        else:
            # Create new master key
            master_key = Fernet.generate_key()
            
            # Encrypt master key with password
            password = os.environ.get('MASTER_KEY_PASSWORD', "change_this_in_production_use_env_var")
            
            kdf = PBKDF2HMAC(
                algorithm=hashes.SHA256(),
                length=32,
                salt=b'enterprise_salt_change_in_prod',
                iterations=100000,
            )
            key = base64.urlsafe_b64encode(kdf.derive(password.encode()))
            
            fernet = Fernet(key)
            encrypted_master = fernet.encrypt(master_key)
            
            # Save encrypted master key
            with open(master_key_file, 'wb') as f:
                f.write(encrypted_master)
            
            # Set restrictive permissions
            os.chmod(master_key_file, 0o600)
            
            return master_key
    
    def _load_encryption_keys(self):
        """Load encryption keys from database"""
        with sqlite3.connect(self.key_db_path) as conn:
            cursor = conn.execute('''
                SELECT key_id, key_purpose, encrypted_key, key_algorithm 
                FROM encryption_keys 
                WHERE status = 'active'
            ''')
            
            master_fernet = Fernet(self.master_key)
            
            for row in cursor.fetchall():
                key_id, purpose, encrypted_key, algorithm = row
                try:
                    decrypted_key = master_fernet.decrypt(encrypted_key)
                    self.encryption_keys[key_id] = {
                        "key": decrypted_key,
                        "purpose": purpose,
                        "algorithm": algorithm
                    }
                except Exception as e:
                    print(f"Failed to decrypt key {key_id}: {e}")
    
    def create_encryption_key(self, purpose: str, algorithm: str = "fernet", 
                            expires_days: Optional[int] = None) -> str:
        """Create new encryption key"""
        key_id = f"{purpose}_{secrets.token_hex(8)}"
        
        # Generate key based on algorithm
        if algorithm == "fernet":
            encryption_key = Fernet.generate_key()
        elif algorithm == "aes256":
            encryption_key = secrets.token_bytes(32)  # 256-bit key
        else:
            raise ValueError(f"Unsupported algorithm: {algorithm}")
        
        # Encrypt key with master key
        master_fernet = Fernet(self.master_key)
        encrypted_key = master_fernet.encrypt(encryption_key)
        
        # Calculate expiry
        expires_at = None
        if expires_days:
            expires_at = (datetime.utcnow() + timedelta(days=expires_days)).isoformat()
        
        # Store in database
        with sqlite3.connect(self.key_db_path) as conn:
            conn.execute('''
                INSERT INTO encryption_keys 
                (key_id, key_purpose, encrypted_key, key_algorithm, created_at, expires_at)
                VALUES (?, ?, ?, ?, ?, ?)
            ''', (key_id, purpose, encrypted_key, algorithm, datetime.utcnow().isoformat(), expires_at))
        
        # Store in memory
        self.encryption_keys[key_id] = {
            "key": encryption_key,
            "purpose": purpose,
            "algorithm": algorithm
        }
        
        self._log_key_usage(key_id, "created", success=True)
        
        return key_id
    
    def encrypt_data(self, data: Any, key_id: str = None, purpose: str = "general") -> Dict[str, Any]:
        """Encrypt data with specified key"""
        
        # Get or create encryption key
        if key_id is None:
            # Find key by purpose or create new one
            key_id = self._find_key_by_purpose(purpose)
            if not key_id:
                key_id = self.create_encryption_key(purpose)
        
        if key_id not in self.encryption_keys:
            raise ValueError(f"Encryption key not found: {key_id}")
        
        key_info = self.encryption_keys[key_id]
        
        try:
            # Convert data to bytes
            if isinstance(data, str):
                data_bytes = data.encode('utf-8')
            elif isinstance(data, dict) or isinstance(data, list):
                data_bytes = json.dumps(data).encode('utf-8')
            else:
                data_bytes = str(data).encode('utf-8')
            
            # Encrypt based on algorithm
            if key_info["algorithm"] == "fernet":
                fernet = Fernet(key_info["key"])
                encrypted_data = fernet.encrypt(data_bytes)
                encrypted_b64 = base64.b64encode(encrypted_data).decode('utf-8')
            
            elif key_info["algorithm"] == "aes256":
                # AES-256-GCM encryption
                iv = secrets.token_bytes(16)
                cipher = Cipher(algorithms.AES(key_info["key"]), modes.GCM(iv))
                encryptor = cipher.encryptor()
                
                ciphertext = encryptor.update(data_bytes) + encryptor.finalize()
                
                # Combine IV + tag + ciphertext
                encrypted_data = iv + encryptor.tag + ciphertext
                encrypted_b64 = base64.b64encode(encrypted_data).decode('utf-8')
            
            else:
                raise ValueError(f"Unsupported algorithm: {key_info['algorithm']}")
            
            self._log_key_usage(key_id, "encrypt", success=True)
            
            return {
                "encrypted_data": encrypted_b64,
                "key_id": key_id,
                "algorithm": key_info["algorithm"],
                "timestamp": datetime.utcnow().isoformat()
            }
            
        except Exception as e:
            self._log_key_usage(key_id, "encrypt", success=False)
            raise Exception(f"Encryption failed: {str(e)}")
    
    def decrypt_data(self, encrypted_package: Dict[str, Any]) -> Any:
        """Decrypt data using encrypted package"""
        
        key_id = encrypted_package["key_id"]
        encrypted_b64 = encrypted_package["encrypted_data"]
        algorithm = encrypted_package["algorithm"]
        
        if key_id not in self.encryption_keys:
            raise ValueError(f"Decryption key not found: {key_id}")
        
        key_info = self.encryption_keys[key_id]
        
        try:
            encrypted_data = base64.b64decode(encrypted_b64.encode('utf-8'))
            
            # Decrypt based on algorithm
            if algorithm == "fernet":
                fernet = Fernet(key_info["key"])
                decrypted_bytes = fernet.decrypt(encrypted_data)
            
            elif algorithm == "aes256":
                # Extract IV, tag, and ciphertext
                iv = encrypted_data[:16]
                tag = encrypted_data[16:32]
                ciphertext = encrypted_data[32:]
                
                cipher = Cipher(algorithms.AES(key_info["key"]), modes.GCM(iv, tag))
                decryptor = cipher.decryptor()
                decrypted_bytes = decryptor.update(ciphertext) + decryptor.finalize()
            
            else:
                raise ValueError(f"Unsupported algorithm: {algorithm}")
            
            # Try to parse as JSON, fallback to string
            try:
                decrypted_str = decrypted_bytes.decode('utf-8')
                return json.loads(decrypted_str)
            except json.JSONDecodeError:
                return decrypted_bytes.decode('utf-8')
            except UnicodeDecodeError:
                return decrypted_bytes
            
            self._log_key_usage(key_id, "decrypt", success=True)
            
        except Exception as e:
            self._log_key_usage(key_id, "decrypt", success=False)
            raise Exception(f"Decryption failed: {str(e)}")
    
    def _find_key_by_purpose(self, purpose: str) -> Optional[str]:
        """Find active key by purpose"""
        for key_id, key_info in self.encryption_keys.items():
            if key_info["purpose"] == purpose:
                return key_id
        return None
    
    def _log_key_usage(self, key_id: str, operation: str, user_id: str = None, success: bool = True):
        """Log key usage for audit"""
        with sqlite3.connect(self.key_db_path) as conn:
            conn.execute('''
                INSERT INTO key_usage_log (key_id, operation, user_id, timestamp, success)
                VALUES (?, ?, ?, ?, ?)
            ''', (key_id, operation, user_id, datetime.utcnow().isoformat(), int(success)))
    
    def rotate_key(self, key_id: str) -> str:
        """Rotate encryption key"""
        if key_id not in self.encryption_keys:
            raise ValueError(f"Key not found: {key_id}")
        
        old_key_info = self.encryption_keys[key_id]
        
        # Create new key with same purpose
        new_key_id = self.create_encryption_key(
            purpose=old_key_info["purpose"],
            algorithm=old_key_info["algorithm"]
        )
        
        # Mark old key as rotated
        with sqlite3.connect(self.key_db_path) as conn:
            conn.execute('''
                UPDATE encryption_keys 
                SET status = 'rotated', rotation_count = rotation_count + 1
                WHERE key_id = ?
            ''', (key_id,))
        
        # Remove from memory (but keep in database for historical decryption)
        del self.encryption_keys[key_id]
        
        self._log_key_usage(key_id, "rotated", success=True)
        self._log_key_usage(new_key_id, "rotation_created", success=True)
        
        return new_key_id
    
    def encrypt_file(self, file_path: str, key_id: str = None) -> str:
        """Encrypt entire file"""
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        
        # Read file
        with open(file_path, 'rb') as f:
            file_data = f.read()
        
        # Encrypt data
        encrypted_package = self.encrypt_data(file_data, key_id, purpose="file_encryption")
        
        # Save encrypted file
        encrypted_file_path = f"{file_path}.encrypted"
        with open(encrypted_file_path, 'w') as f:
            json.dump(encrypted_package, f)
        
        return encrypted_file_path
    
    def decrypt_file(self, encrypted_file_path: str, output_path: str = None) -> str:
        """Decrypt entire file"""
        if not os.path.exists(encrypted_file_path):
            raise FileNotFoundError(f"Encrypted file not found: {encrypted_file_path}")
        
        # Read encrypted package
        with open(encrypted_file_path, 'r') as f:
            encrypted_package = json.load(f)
        
        # Decrypt data
        decrypted_data = self.decrypt_data(encrypted_package)
        
        # Determine output path
        if output_path is None:
            if encrypted_file_path.endswith('.encrypted'):
                output_path = encrypted_file_path[:-10]  # Remove .encrypted
            else:
                output_path = f"{encrypted_file_path}.decrypted"
        
        # Write decrypted data
        if isinstance(decrypted_data, bytes):
            with open(output_path, 'wb') as f:
                f.write(decrypted_data)
        else:
            with open(output_path, 'w') as f:
                if isinstance(decrypted_data, str):
                    f.write(decrypted_data)
                else:
                    json.dump(decrypted_data, f, indent=2)
        
        return output_path
    
    def encrypt_database_field(self, table_name: str, field_name: str, 
                              value: Any, record_id: str) -> Dict[str, Any]:
        """Encrypt specific database field"""
        purpose = f"db_{table_name}_{field_name}"
        key_id = self._find_key_by_purpose(purpose)
        
        if not key_id:
            key_id = self.create_encryption_key(purpose)
        
        encrypted_package = self.encrypt_data(value, key_id, purpose)
        
        # Add metadata for database encryption
        encrypted_package.update({
            "table_name": table_name,
            "field_name": field_name,
            "record_id": record_id,
            "encryption_version": "1.0"
        })
        
        return encrypted_package
    
    def decrypt_database_field(self, encrypted_field_data: Dict[str, Any]) -> Any:
        """Decrypt database field"""
        return self.decrypt_data(encrypted_field_data)
    
    def create_rsa_keypair(self, key_size: int = 2048) -> Tuple[str, str]:
        """Create RSA public/private key pair for asymmetric encryption"""
        
        # Generate private key
        private_key = rsa.generate_private_key(
            public_exponent=65537,
            key_size=key_size
        )
        
        # Get public key
        public_key = private_key.public_key()
        
        # Serialize keys
        private_pem = private_key.private_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PrivateFormat.PKCS8,
            encryption_algorithm=serialization.NoEncryption()
        )
        
        public_pem = public_key.public_bytes(
            encoding=serialization.Encoding.PEM,
            format=serialization.PublicFormat.SubjectPublicKeyInfo
        )
        
        # Store keys securely
        key_id = f"rsa_{secrets.token_hex(8)}"
        
        # Encrypt private key with master key
        master_fernet = Fernet(self.master_key)
        encrypted_private_key = master_fernet.encrypt(private_pem)
        
        with sqlite3.connect(self.key_db_path) as conn:
            # Store private key
            conn.execute('''
                INSERT INTO encryption_keys 
                (key_id, key_purpose, encrypted_key, key_algorithm, created_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (f"{key_id}_private", "rsa_private", encrypted_private_key, "rsa", 
                  datetime.utcnow().isoformat()))
            
            # Store public key (not encrypted as it's meant to be public)
            conn.execute('''
                INSERT INTO encryption_keys 
                (key_id, key_purpose, encrypted_key, key_algorithm, created_at)
                VALUES (?, ?, ?, ?, ?)
            ''', (f"{key_id}_public", "rsa_public", public_pem, "rsa", 
                  datetime.utcnow().isoformat()))
        
        return key_id, public_pem.decode('utf-8')
    
    def rsa_encrypt(self, data: str, public_key_pem: str) -> str:
        """Encrypt data with RSA public key"""
        public_key = serialization.load_pem_public_key(public_key_pem.encode('utf-8'))
        
        # RSA can only encrypt small amounts of data
        data_bytes = data.encode('utf-8')
        
        if len(data_bytes) > 190:  # RSA-2048 limit minus padding
            # For large data, use hybrid encryption (RSA + AES)
            # Generate random AES key
            aes_key = secrets.token_bytes(32)
            
            # Encrypt data with AES
            iv = secrets.token_bytes(16)
            cipher = Cipher(algorithms.AES(aes_key), modes.GCM(iv))
            encryptor = cipher.encryptor()
            ciphertext = encryptor.update(data_bytes) + encryptor.finalize()
            
            # Encrypt AES key with RSA
            encrypted_aes_key = public_key.encrypt(
                aes_key,
                padding.OAEP(
                    mgf=padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None
                )
            )
            
            # Combine encrypted key + IV + tag + ciphertext
            hybrid_data = {
                "encrypted_key": base64.b64encode(encrypted_aes_key).decode('utf-8'),
                "iv": base64.b64encode(iv).decode('utf-8'),
                "tag": base64.b64encode(encryptor.tag).decode('utf-8'),
                "ciphertext": base64.b64encode(ciphertext).decode('utf-8'),
                "method": "hybrid_rsa_aes"
            }
            
            return base64.b64encode(json.dumps(hybrid_data).encode('utf-8')).decode('utf-8')
        
        else:
            # Direct RSA encryption for small data
            encrypted = public_key.encrypt(
                data_bytes,
                padding.OAEP(
                    mgf=padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None
                )
            )
            
            return base64.b64encode(encrypted).decode('utf-8')
    
    def rsa_decrypt(self, encrypted_data: str, private_key_id: str) -> str:
        """Decrypt data with RSA private key"""
        
        # Get private key from database
        with sqlite3.connect(self.key_db_path) as conn:
            cursor = conn.execute('''
                SELECT encrypted_key FROM encryption_keys 
                WHERE key_id = ? AND key_algorithm = 'rsa'
            ''', (private_key_id,))
            
            result = cursor.fetchone()
            if not result:
                raise ValueError(f"RSA private key not found: {private_key_id}")
            
            # Decrypt private key
            master_fernet = Fernet(self.master_key)
            private_key_pem = master_fernet.decrypt(result[0])
        
        private_key = serialization.load_pem_private_key(private_key_pem, password=None)
        
        try:
            # Try to decode as hybrid encryption first
            decoded_data = base64.b64decode(encrypted_data.encode('utf-8'))
            hybrid_data = json.loads(decoded_data.decode('utf-8'))
            
            if hybrid_data.get("method") == "hybrid_rsa_aes":
                # Hybrid decryption
                encrypted_aes_key = base64.b64decode(hybrid_data["encrypted_key"].encode('utf-8'))
                iv = base64.b64decode(hybrid_data["iv"].encode('utf-8'))
                tag = base64.b64decode(hybrid_data["tag"].encode('utf-8'))
                ciphertext = base64.b64decode(hybrid_data["ciphertext"].encode('utf-8'))
                
                # Decrypt AES key with RSA
                aes_key = private_key.decrypt(
                    encrypted_aes_key,
                    padding.OAEP(
                        mgf=padding.MGF1(algorithm=hashes.SHA256()),
                        algorithm=hashes.SHA256(),
                        label=None
                    )
                )
                
                # Decrypt data with AES
                cipher = Cipher(algorithms.AES(aes_key), modes.GCM(iv, tag))
                decryptor = cipher.decryptor()
                decrypted_bytes = decryptor.update(ciphertext) + decryptor.finalize()
                
                return decrypted_bytes.decode('utf-8')
        
        except (json.JSONDecodeError, KeyError):
            # Direct RSA decryption
            encrypted_bytes = base64.b64decode(encrypted_data.encode('utf-8'))
            
            decrypted = private_key.decrypt(
                encrypted_bytes,
                padding.OAEP(
                    mgf=padding.MGF1(algorithm=hashes.SHA256()),
                    algorithm=hashes.SHA256(),
                    label=None
                )
            )
            
            return decrypted.decode('utf-8')
    
    def generate_data_hash(self, data: Any) -> str:
        """Generate secure hash for data integrity"""
        if isinstance(data, str):
            data_bytes = data.encode('utf-8')
        elif isinstance(data, dict) or isinstance(data, list):
            data_bytes = json.dumps(data, sort_keys=True).encode('utf-8')
        else:
            data_bytes = str(data).encode('utf-8')
        
        # Use SHA-256 for integrity checking
        return hashlib.sha256(data_bytes).hexdigest()
    
    def verify_data_integrity(self, data: Any, expected_hash: str) -> bool:
        """Verify data integrity using hash"""
        actual_hash = self.generate_data_hash(data)
        return actual_hash == expected_hash
    
    def secure_delete_key(self, key_id: str) -> bool:
        """Securely delete encryption key"""
        try:
            # Mark key as deleted in database
            with sqlite3.connect(self.key_db_path) as conn:
                conn.execute('''
                    UPDATE encryption_keys 
                    SET status = 'deleted', encrypted_key = NULL
                    WHERE key_id = ?
                ''', (key_id,))
            
            # Remove from memory
            if key_id in self.encryption_keys:
                # Overwrite key in memory (best effort)
                key_data = self.encryption_keys[key_id]["key"]
                if isinstance(key_data, bytes):
                    # Overwrite with random data
                    overwrite = secrets.token_bytes(len(key_data))
                    key_data = overwrite
                
                del self.encryption_keys[key_id]
            
            self._log_key_usage(key_id, "deleted", success=True)
            return True
            
        except Exception as e:
            self._log_key_usage(key_id, "deleted", success=False)
            return False
    
    def get_key_usage_stats(self, days: int = 30) -> Dict[str, Any]:
        """Get encryption key usage statistics"""
        start_date = (datetime.utcnow() - timedelta(days=days)).isoformat()
        
        with sqlite3.connect(self.key_db_path) as conn:
            # Usage by operation
            cursor = conn.execute('''
                SELECT operation, COUNT(*) as count
                FROM key_usage_log 
                WHERE timestamp >= ?
                GROUP BY operation
            ''', (start_date,))
            
            operation_stats = dict(cursor.fetchall())
            
            # Usage by key
            cursor = conn.execute('''
                SELECT key_id, COUNT(*) as count
                FROM key_usage_log 
                WHERE timestamp >= ?
                GROUP BY key_id
                ORDER BY count DESC
                LIMIT 10
            ''', (start_date,))
            
            key_usage = [{"key_id": key_id, "count": count} for key_id, count in cursor.fetchall()]
            
            # Success rate
            cursor = conn.execute('''
                SELECT success, COUNT(*) as count
                FROM key_usage_log 
                WHERE timestamp >= ?
                GROUP BY success
            ''', (start_date,))
            
            success_stats = dict(cursor.fetchall())
            total_operations = sum(success_stats.values())
            success_rate = (success_stats.get(1, 0) / total_operations * 100) if total_operations > 0 else 0
            
            # Active keys
            cursor = conn.execute('''
                SELECT COUNT(*) FROM encryption_keys WHERE status = 'active'
            ''')
            active_keys = cursor.fetchone()[0]
            
            return {
                "period_days": days,
                "operation_stats": operation_stats,
                "top_keys": key_usage,
                "success_rate": round(success_rate, 2),
                "total_operations": total_operations,
                "active_keys": active_keys
            }
    
    def export_public_keys(self) -> Dict[str, str]:
        """Export all public keys for external use"""
        public_keys = {}
        
        with sqlite3.connect(self.key_db_path) as conn:
            cursor = conn.execute('''
                SELECT key_id, encrypted_key FROM encryption_keys 
                WHERE key_purpose = 'rsa_public' AND status = 'active'
            ''')
            
            for key_id, public_key_pem in cursor.fetchall():
                # Remove '_public' suffix from key_id
                clean_key_id = key_id.replace('_public', '')
                public_keys[clean_key_id] = public_key_pem.decode('utf-8') if isinstance(public_key_pem, bytes) else public_key_pem
        
        return public_keys
    
    def backup_encryption_keys(self, backup_password: str) -> str:
        """Create encrypted backup of all encryption keys"""
        
        # Get all keys
        with sqlite3.connect(self.key_db_path) as conn:
            cursor = conn.execute('''
                SELECT key_id, key_purpose, encrypted_key, key_algorithm, created_at, expires_at
                FROM encryption_keys 
                WHERE status = 'active'
            ''')
            
            keys_data = []
            for row in cursor.fetchall():
                keys_data.append({
                    "key_id": row[0],
                    "key_purpose": row[1],
                    "encrypted_key": base64.b64encode(row[2]).decode('utf-8') if isinstance(row[2], bytes) else row[2],
                    "key_algorithm": row[3],
                    "created_at": row[4],
                    "expires_at": row[5]
                })
        
        backup_data = {
            "backup_timestamp": datetime.utcnow().isoformat(),
            "key_count": len(keys_data),
            "keys": keys_data
        }
        
        # Encrypt backup with provided password
        kdf = PBKDF2HMAC(
            algorithm=hashes.SHA256(),
            length=32,
            salt=b'backup_salt_change_in_prod',
            iterations=100000,
        )
        backup_key = base64.urlsafe_b64encode(kdf.derive(backup_password.encode()))
        
        backup_fernet = Fernet(backup_key)
        encrypted_backup = backup_fernet.encrypt(json.dumps(backup_data).encode('utf-8'))
        
        # Save backup file
        backup_filename = f"key_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.enc"
        with open(backup_filename, 'wb') as f:
            f.write(encrypted_backup)
        
        return backup_filename

# Global encryption manager instance
encryption_manager = EnterpriseEncryption()


==================================================
FILE: fix_json_serialization.py
==================================================

#!/usr/bin/env python3
"""
Quick fix for JSON serialization issue in smart_auto_download_workflow.py
"""

import json
from datetime import datetime
from pathlib import Path

def safe_json_serialize(obj):
    """Safely serialize objects to JSON, handling non-serializable types"""
    
    def convert_obj(item):
        """Convert problematic objects to serializable format"""
        
        # Handle common non-serializable types
        if hasattr(item, '__class__'):
            class_name = item.__class__.__name__
            
            # Exception objects
            if 'Error' in class_name or 'Exception' in class_name:
                return {
                    "type": "exception", 
                    "class": class_name,
                    "message": str(item)
                }
            
            # HTTP Response objects
            if hasattr(item, 'status_code') and hasattr(item, 'text'):
                return {
                    "type": "http_response",
                    "status_code": getattr(item, 'status_code', None),
                    "text": str(item)[:200] + "..." if len(str(item)) > 200 else str(item)
                }
            
            # Other complex objects
            if not isinstance(item, (str, int, float, bool, list, dict, type(None))):
                return {
                    "type": "object",
                    "class": class_name,
                    "repr": str(item)[:200] + "..." if len(str(item)) > 200 else str(item)
                }
        
        # Handle collections
        if isinstance(item, dict):
            return {k: convert_obj(v) for k, v in item.items()}
        elif isinstance(item, (list, tuple)):
            return [convert_obj(i) for i in item]
        
        # Return as-is for serializable types
        return item
    
    return json.dumps(convert_obj(obj), indent=2, default=str)

def fix_stage_5_llm_final_analysis(self, execution_results):
    """Fixed version of stage_5_llm_final_analysis method"""
    
    self.log("\n📊 STAGE 5: LLM Final Analysis")
    print("=" * 60)
    
    # Prepare execution summary for LLM with safe serialization
    execution_summary = {
        "use_case": self.use_case,
        "tools_downloaded": len(self.results.get("downloaded_tools", {})),
        "new_tools_added": len(self.results.get("new_tools", [])),
        "workflow_steps_executed": len(execution_results),
        "successful_steps": sum(1 for step in execution_results if step["success"]),
        "execution_results": self._sanitize_execution_results(execution_results[:3])  # First 3 for LLM analysis
    }
    
    # Create the prompt with safe JSON serialization
    try:
        serialized_summary = safe_json_serialize(execution_summary)
    except Exception as e:
        self.log(f"⚠️ JSON serialization still failing: {e}")
        # Fallback: create a simple summary
        serialized_summary = json.dumps({
            "use_case": self.use_case,
            "tools_downloaded": execution_summary["tools_downloaded"],
            "new_tools_added": execution_summary["new_tools_added"], 
            "workflow_steps_executed": execution_summary["workflow_steps_executed"],
            "successful_steps": execution_summary["successful_steps"],
            "note": "Detailed execution results omitted due to serialization complexity"
        }, indent=2)
    
    final_analysis_prompt = f"""
Analyze the results of this auto-download workflow execution:

{serialized_summary}

Provide insights on:
1. How well the auto-downloaded tools worked
2. What was accomplished vs. the original use case
3. Recommendations for improvement
4. Value of the auto-download approach
5. Next steps for better results

Format as JSON:
{{
  "effectiveness_score": "1-10",
  "auto_download_value": "assessment of auto-download approach",
  "accomplishments": ["what was achieved"],
  "limitations": ["what didn't work well"],
  "recommendations": ["specific improvements"],
  "next_steps": ["actionable next steps"]
}}
"""
    
    # Continue with the rest of the original method...
    # (The LLM call and parsing logic remains the same)

def add_sanitize_method_to_workflow():
    """Method to add to SmartAutoDownloadWorkflow class"""
    
    def _sanitize_execution_results(self, execution_results):
        """Sanitize execution results for JSON serialization"""
        
        sanitized = []
        
        for step in execution_results:
            sanitized_step = {
                "step": step.get("step"),
                "action": step.get("action"),
                "tools_attempted": step.get("tools_attempted", []),
                "success": step.get("success", False),
                "results": {}
            }
            
            # Sanitize tool results
            for tool_name, result in step.get("results", {}).items():
                if isinstance(result, dict):
                    sanitized_result = {}
                    for k, v in result.items():
                        # Convert non-serializable values
                        if hasattr(v, '__class__') and 'Error' in v.__class__.__name__:
                            sanitized_result[k] = {
                                "type": "exception",
                                "class": v.__class__.__name__,
                                "message": str(v)
                            }
                        else:
                            try:
                                json.dumps(v)  # Test if serializable
                                sanitized_result[k] = v
                            except TypeError:
                                sanitized_result[k] = str(v)
                    
                    sanitized_step["results"][tool_name] = sanitized_result
                else:
                    sanitized_step["results"][tool_name] = str(result)
            
            sanitized.append(sanitized_step)
        
        return sanitized
    
    return _sanitize_execution_results

# Quick patch script
def apply_quick_fix():
    """Apply the quick fix to smart_auto_download_workflow.py"""
    
    workflow_file = Path("smart_auto_download_workflow.py")
    
    if not workflow_file.exists():
        print("❌ smart_auto_download_workflow.py not found")
        return False
    
    print("🔧 Applying JSON serialization fix...")
    
    # Read the original file
    with open(workflow_file, 'r', encoding='utf-8') as f:
        content = f.read()
    
    # Add the sanitize method to the class
    sanitize_method = '''
    def _sanitize_execution_results(self, execution_results):
        """Sanitize execution results for JSON serialization"""
        
        sanitized = []
        
        for step in execution_results:
            sanitized_step = {
                "step": step.get("step"),
                "action": step.get("action"),
                "tools_attempted": step.get("tools_attempted", []),
                "success": step.get("success", False),
                "results": {}
            }
            
            # Sanitize tool results
            for tool_name, result in step.get("results", {}).items():
                if isinstance(result, dict):
                    sanitized_result = {}
                    for k, v in result.items():
                        # Convert non-serializable values
                        if hasattr(v, '__class__') and 'Error' in v.__class__.__name__:
                            sanitized_result[k] = {
                                "type": "exception",
                                "class": v.__class__.__name__,
                                "message": str(v)
                            }
                        else:
                            try:
                                json.dumps(v)  # Test if serializable
                                sanitized_result[k] = v
                            except TypeError:
                                sanitized_result[k] = str(v)
                    
                    sanitized_step["results"][tool_name] = sanitized_result
                else:
                    sanitized_step["results"][tool_name] = str(result)
            
            sanitized.append(sanitized_step)
        
        return sanitized
'''
    
    # Find where to insert the method (before the run_complete_workflow method)
    insertion_point = content.find("    async def run_complete_workflow(self):")
    
    if insertion_point == -1:
        print("❌ Could not find insertion point")
        return False
    
    # Insert the sanitize method
    new_content = (content[:insertion_point] + 
                   sanitize_method + "\n" + 
                   content[insertion_point:])
    
    # Fix the problematic line in stage_5_llm_final_analysis
    old_line = '"execution_results": execution_results[:3]  # First 3 for LLM analysis'
    new_line = '"execution_results": self._sanitize_execution_results(execution_results[:3])  # First 3 for LLM analysis'
    
    new_content = new_content.replace(old_line, new_line)
    
    # Backup original file
    backup_file = Path("smart_auto_download_workflow_backup.py")
    with open(backup_file, 'w', encoding='utf-8') as f:
        f.write(content)
    
    # Write the fixed file
    with open(workflow_file, 'w', encoding='utf-8') as f:
        f.write(new_content)
    
    print("✅ JSON serialization fix applied!")
    print(f"📁 Original backed up to: {backup_file}")
    print("\n🚀 Now run: python smart_auto_download_workflow.py")
    
    return True

if __name__ == "__main__":
    apply_quick_fix()


==================================================
FILE: middleware.py
==================================================

#!/usr/bin/env python3
"""
Enterprise Security Middleware
Request authentication, rate limiting, and security controls
"""

import time
import json
import hashlib
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, Callable
from functools import wraps
import re
from collections import defaultdict, deque

class SecurityMiddleware:
    """Enterprise security middleware for request processing"""
    
    def __init__(self):
        # Rate limiting
        self.rate_limits = defaultdict(lambda: deque())
        self.rate_limit_rules = {
            "default": {"requests": 100, "window": 3600},  # 100 requests per hour
            "auth": {"requests": 5, "window": 300},         # 5 auth attempts per 5 minutes
            "api": {"requests": 1000, "window": 3600},      # 1000 API calls per hour
            "admin": {"requests": 50, "window": 3600}       # 50 admin actions per hour
        }
        
        # Security settings
        self.blocked_ips = set()
        self.suspicious_patterns = [
            r"<script[^>]*>.*?</script>",  # XSS attempts
            r"union\s+select",             # SQL injection
            r"\.\.\/",                     # Path traversal
            r"exec\(",                     # Code execution
            r"eval\(",                     # Code evaluation
        ]
        
        # Session management
        self.active_sessions = {}
        self.session_timeout = 3600  # 1 hour
        
        # Failed attempt tracking
        self.failed_attempts = defaultdict(lambda: {"count": 0, "last_attempt": None})
        self.lockout_threshold = 5
        self.lockout_duration = 900  # 15 minutes
    
    def authenticate_request(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """Authenticate incoming request"""
        
        # Extract request information
        ip_address = request_data.get("ip_address", "unknown")
        user_agent = request_data.get("user_agent", "")
        auth_token = request_data.get("auth_token")
        api_key = request_data.get("api_key")
        endpoint = request_data.get("endpoint", "/")
        
        # Check if IP is blocked
        if ip_address in self.blocked_ips:
            return {
                "authenticated": False,
                "error": "IP address blocked",
                "code": 403
            }
        
        # Check for account lockout
        if self._is_locked_out(ip_address):
            return {
                "authenticated": False,
                "error": "Account temporarily locked due to failed attempts",
                "code": 423
            }
        
        # Security pattern detection
        security_check = self._check_security_patterns(request_data)
        if not security_check["safe"]:
            self._record_failed_attempt(ip_address, "security_pattern")
            return {
                "authenticated": False,
                "error": f"Request blocked: {security_check['reason']}",
                "code": 400
            }
        
        # Authentication check
        auth_result = None
        
        if api_key:
            # API key authentication
            from enterprise.auth_handler import enterprise_auth
            auth_result = enterprise_auth.validate_api_key(api_key)
            
        elif auth_token:
            # Session token authentication
            from enterprise.auth_handler import enterprise_auth
            auth_result = enterprise_auth.validate_session(auth_token)
        
        else:
            # No authentication provided
            return {
                "authenticated": False,
                "error": "Authentication required",
                "code": 401
            }
        
        if not auth_result or not auth_result.get("valid"):
            self._record_failed_attempt(ip_address, "invalid_auth")
            return {
                "authenticated": False,
                "error": auth_result.get("error", "Invalid authentication"),
                "code": 401
            }
        
        # Rate limiting check
        rate_limit_check = self._check_rate_limit(ip_address, endpoint, auth_result)
        if not rate_limit_check["allowed"]:
            return {
                "authenticated": False,
                "error": rate_limit_check["error"],
                "code": 429
            }
        
        # Clear failed attempts on successful auth
        if ip_address in self.failed_attempts:
            del self.failed_attempts[ip_address]
        
        # Log successful authentication
        from enterprise.audit_logger import audit_logger
        audit_logger.log_event(
            event_type="request_authenticated",
            user_id=auth_result.get("user_id"),
            ip_address=ip_address,
            user_agent=user_agent,
            resource=endpoint,
            action="authenticate",
            outcome="success"
        )
        
        return {
            "authenticated": True,
            "user": auth_result.get("user"),
            "permissions": auth_result.get("permissions", []),
            "rate_limit_remaining": rate_limit_check.get("remaining", 0)
        }
    
    def _check_security_patterns(self, request_data: Dict[str, Any]) -> Dict[str, Any]:
        """Check for malicious patterns in request"""
        
        # Combine all request data for pattern checking
        request_str = json.dumps(request_data, default=str).lower()
        
        for pattern in self.suspicious_patterns:
            if re.search(pattern, request_str, re.IGNORECASE):
                return {
                    "safe": False,
                    "reason": f"Suspicious pattern detected: {pattern}"
                }
        
        # Check for unusual request size
        if len(request_str) > 1000000:  # 1MB limit
            return {
                "safe": False,
                "reason": "Request size too large"
            }
        
        return {"safe": True}
    
    def _check_rate_limit(self, ip_address: str, endpoint: str, 
                         auth_result: Dict[str, Any]) -> Dict[str, Any]:
        """Check rate limiting rules"""
        
        current_time = time.time()
        
        # Determine rate limit rule based on endpoint and user role
        rule_name = "default"
        
        if "/auth/" in endpoint:
            rule_name = "auth"
        elif "/api/" in endpoint:
            rule_name = "api"
        elif "/admin/" in endpoint:
            rule_name = "admin"
        
        # Get user role for more specific limits
        user_permissions = auth_result.get("permissions", [])
        if "*" in user_permissions or "super_admin" in str(user_permissions):
            # Super admins get higher limits
            rule = {"requests": self.rate_limit_rules[rule_name]["requests"] * 5, 
                   "window": self.rate_limit_rules[rule_name]["window"]}
        else:
            rule = self.rate_limit_rules[rule_name]
        
        # Create rate limit key
        rate_key = f"{ip_address}:{rule_name}"
        
        # Clean old entries
        window_start = current_time - rule["window"]
        while self.rate_limits[rate_key] and self.rate_limits[rate_key][0] < window_start:
            self.rate_limits[rate_key].popleft()
        
        # Check current count
        current_count = len(self.rate_limits[rate_key])
        
        if current_count >= rule["requests"]:
            return {
                "allowed": False,
                "error": f"Rate limit exceeded: {current_count}/{rule['requests']} requests per {rule['window']} seconds",
                "remaining": 0
            }
        
        # Add current request
        self.rate_limits[rate_key].append(current_time)
        
        return {
            "allowed": True,
            "remaining": rule["requests"] - current_count - 1
        }
    
    def _record_failed_attempt(self, ip_address: str, reason: str):
        """Record failed authentication attempt"""
        
        current_time = time.time()
        
        if ip_address in self.failed_attempts:
            self.failed_attempts[ip_address]["count"] += 1
        else:
            self.failed_attempts[ip_address]["count"] = 1
        
        self.failed_attempts[ip_address]["last_attempt"] = current_time
        
        # Check if we should block the IP
        if self.failed_attempts[ip_address]["count"] >= self.lockout_threshold:
            # Log security event
            from enterprise.audit_logger import audit_logger
            audit_logger.log_event(
                event_type="account_lockout",
                ip_address=ip_address,
                resource="authentication",
                action="lockout",
                outcome="success",
                risk_level="high",
                details={
                    "failed_attempts": self.failed_attempts[ip_address]["count"],
                    "reason": reason,
                    "lockout_duration": self.lockout_duration
                }
            )
    
    def _is_locked_out(self, ip_address: str) -> bool:
        """Check if IP address is locked out"""
        
        if ip_address not in self.failed_attempts:
            return False
        
        failed_data = self.failed_attempts[ip_address]
        
        if failed_data["count"] < self.lockout_threshold:
            return False
        
        # Check if lockout period has expired
        if failed_data["last_attempt"]:
            time_since_last = time.time() - failed_data["last_attempt"]
            if time_since_last > self.lockout_duration:
                # Clear failed attempts
                del self.failed_attempts[ip_address]
                return False
        
        return True
    
    def require_permission(self, permission: str):
        """Decorator to require specific permission for endpoint"""
        def decorator(func):
            @wraps(func)
            def wrapper(*args, **kwargs):
                # Get authentication context
                auth_context = kwargs.get("_auth_context")
                if not auth_context or not auth_context.get("authenticated"):
                    return {"error": "Authentication required", "code": 401}
                
                # Check permission
                user_permissions = auth_context.get("permissions", [])
                from enterprise.auth_handler import enterprise_auth
                
                if not enterprise_auth.check_permission(user_permissions, permission):
                    # Log permission denial
                    from enterprise.audit_logger import audit_logger
                    audit_logger.log_event(
                        event_type="permission_denied",
                        user_id=auth_context.get("user", {}).get("user_id"),
                        resource=func.__name__,
                        action="access_attempt",
                        outcome="failure",
                        risk_level="medium",
                        details={"required_permission": permission}
                    )
                    
                    return {"error": f"Permission denied: {permission}", "code": 403}
                
                # Log successful access
                from enterprise.audit_logger import audit_logger
                audit_logger.log_event(
                    event_type="function_access",
                    user_id=auth_context.get("user", {}).get("user_id"),
                    resource=func.__name__,
                    action="execute",
                    outcome="success",
                    details={"permission": permission}
                )
                
                return func(*args, **kwargs)
            return wrapper
        return decorator
    
    def secure_headers(self, response_headers: Dict[str, str]) -> Dict[str, str]:
        """Add security headers to response"""
        
        security_headers = {
            "X-Content-Type-Options": "nosniff",
            "X-Frame-Options": "DENY", 
            "X-XSS-Protection": "1; mode=block",
            "Strict-Transport-Security": "max-age=31536000; includeSubDomains",
            "Content-Security-Policy": "default-src 'self'; script-src 'self' 'unsafe-inline'; style-src 'self' 'unsafe-inline'",
            "Referrer-Policy": "strict-origin-when-cross-origin",
            "Permissions-Policy": "geolocation=(), microphone=(), camera=()"
        }
        
        # Merge with existing headers
        response_headers.update(security_headers)
        return response_headers
    
    def validate_input(self, data: Dict[str, Any], schema: Dict[str, Any]) -> Dict[str, Any]:
        """Validate input data against schema"""
        
        errors = []
        validated_data = {}
        
        for field, rules in schema.items():
            value = data.get(field)
            
            # Required field check
            if rules.get("required", False) and value is None:
                errors.append(f"Field '{field}' is required")
                continue
            
            if value is not None:
                # Type validation
                expected_type = rules.get("type")
                if expected_type and not isinstance(value, expected_type):
                    errors.append(f"Field '{field}' must be of type {expected_type.__name__}")
                    continue
                
                # Length validation for strings
                if isinstance(value, str):
                    min_length = rules.get("min_length", 0)
                    max_length = rules.get("max_length", float('inf'))
                    
                    if len(value) < min_length:
                        errors.append(f"Field '{field}' must be at least {min_length} characters")
                        continue
                    
                    if len(value) > max_length:
                        errors.append(f"Field '{field}' must be no more than {max_length} characters")
                        continue
                
                # Pattern validation
                pattern = rules.get("pattern")
                if pattern and isinstance(value, str):
                    if not re.match(pattern, value):
                        errors.append(f"Field '{field}' does not match required pattern")
                        continue
                
                # Custom validator
                validator = rules.get("validator")
                if validator and callable(validator):
                    if not validator(value):
                        errors.append(f"Field '{field}' failed custom validation")
                        continue
                
                validated_data[field] = value
        
        if errors:
            return {"valid": False, "errors": errors}
        
        return {"valid": True, "data": validated_data}
    
    def sanitize_input(self, data: Any) -> Any:
        """Sanitize input data to prevent XSS and injection attacks"""
        
        if isinstance(data, str):
            # Remove potential XSS patterns
            data = re.sub(r'<script[^>]*>.*?</script>', '', data, flags=re.IGNORECASE | re.DOTALL)
            data = re.sub(r'javascript:', '', data, flags=re.IGNORECASE)
            data = re.sub(r'on\w+\s*=', '', data, flags=re.IGNORECASE)
            
            # Escape HTML entities
            data = data.replace('<', '&lt;').replace('


==================================================
FILE: minimalistic_frontend.py
==================================================

#!/usr/bin/env python3

from sanic import Sanic, response
import json
import os
import uuid
import subprocess
from datetime import datetime
from pathlib import Path

app = Sanic("MinimalWorkflowAPI")

# Directories
UPLOAD_DIR = "./uploads"
WORKFLOWS_DIR = "./workflows" 
RESULTS_DIR = "./results"

# Ensure directories exist
for dir_path in [UPLOAD_DIR, WORKFLOWS_DIR, RESULTS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# Execution tracking
executions = {}

@app.route("/", methods=["GET"])
async def web_ui(request):
    """Serve minimal web interface"""
    html = """
<!DOCTYPE html>
<html>
<head>
    <title>Agentic Workflow Runner</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 40px auto; padding: 20px; }
        .section { background: #f5f5f5; padding: 20px; margin: 20px 0; border-radius: 8px; }
        button { background: #007cba; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; }
        button:hover { background: #005a8b; }
        input[type="file"] { margin: 10px 0; }
        .results { background: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 4px; }
        .error { background: #ffebee; color: #c62828; }
        .success { background: #e8f5e8; color: #2e7d32; }
        pre { background: #f0f0f0; padding: 10px; overflow-x: auto; border-radius: 4px; }
    </style>
</head>
<body>
    <h1>🤖 Agentic Workflow Runner</h1>
    
    <!-- Workflow Upload -->
    <div class="section">
        <h3>1. Upload Workflow</h3>
        <input type="file" id="workflowFile" accept=".json" />
        <button onclick="uploadWorkflow()">Upload JSON Workflow</button>
        <div id="workflowResult"></div>
    </div>
    
    <!-- Data Upload (Optional) -->
    <div class="section">
        <h3>2. Upload Data Files (Optional)</h3>
        <input type="file" id="dataFiles" multiple accept=".csv,.xlsx,.json" />
        <button onclick="uploadData()">Upload Data Files</button>
        <div id="dataResult"></div>
    </div>
    
    <!-- Execution -->
    <div class="section">
        <h3>3. Execute Workflow</h3>
        <button onclick="executeWorkflow()">▶️ Run Workflow</button>
        <button onclick="executeWithData()">▶️ Run with Data</button>
        <div id="executionResult"></div>
    </div>
    
    <!-- Results -->
    <div class="section">
        <h3>4. Results</h3>
        <button onclick="checkStatus()">🔄 Check Status</button>
        <button onclick="downloadResults()">📥 Download Results</button>
        <div id="statusResult"></div>
    </div>

    <script>
        let currentWorkflowId = null;
        let currentDataFiles = [];
        let currentExecutionId = null;

        function showResult(elementId, message, isError = false) {
            const el = document.getElementById(elementId);
            el.innerHTML = `<div class="results ${isError ? 'error' : 'success'}">${message}</div>`;
        }

        async function uploadWorkflow() {
            const fileInput = document.getElementById('workflowFile');
            if (!fileInput.files[0]) {
                showResult('workflowResult', 'Please select a JSON workflow file', true);
                return;
            }

            const formData = new FormData();
            formData.append('workflow', fileInput.files[0]);

            try {
                const response = await fetch('/upload-workflow', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentWorkflowId = result.workflow_id;
                    showResult('workflowResult', 
                        `✅ Workflow uploaded: ${result.steps} steps, ${result.agents.length} agents`);
                } else {
                    showResult('workflowResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('workflowResult', `❌ Upload failed: ${error.message}`, true);
            }
        }

        async function uploadData() {
            const fileInput = document.getElementById('dataFiles');
            if (!fileInput.files.length) {
                showResult('dataResult', 'Please select data files', true);
                return;
            }

            const formData = new FormData();
            Array.from(fileInput.files).forEach((file, index) => {
                formData.append(`data_${index}`, file);
            });

            try {
                const response = await fetch('/upload-data', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentDataFiles = result.uploaded_files;
                    showResult('dataResult', 
                        `✅ Uploaded ${result.uploaded_files.length} data files`);
                } else {
                    showResult('dataResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('dataResult', `❌ Upload failed: ${error.message}`, true);
            }
        }

        async function executeWorkflow() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', true);
                return;
            }

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        use_data: false
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution started: ${result.execution_id}`);
                } else {
                    showResult('executionResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, true);
            }
        }

        async function executeWithData() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', true);
                return;
            }
            if (!currentDataFiles.length) {
                showResult('executionResult', 'Please upload data files first', true);
                return;
            }

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        data_files: currentDataFiles,
                        use_data: true
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution with data started: ${result.execution_id}`);
                } else {
                    showResult('executionResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, true);
            }
        }

        async function checkStatus() {
            if (!currentExecutionId) {
                showResult('statusResult', 'No execution in progress', true);
                return;
            }

            try {
                const response = await fetch(`/status/${currentExecutionId}`);
                const result = await response.json();
                
                if (response.ok) {
                    showResult('statusResult', 
                        `Status: ${result.status}<br>
                         Started: ${result.start_time}<br>
                         ${result.results_file ? '✅ Results ready' : '⏳ Running...'}`);
                } else {
                    showResult('statusResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('statusResult', `❌ Status check failed: ${error.message}`, true);
            }
        }

        async function downloadResults() {
            if (!currentExecutionId) {
                showResult('statusResult', 'No execution to download', true);
                return;
            }

            try {
                const response = await fetch(`/download/${currentExecutionId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `results_${currentExecutionId}.json`;
                    a.click();
                    showResult('statusResult', '✅ Results downloaded');
                } else {
                    const result = await response.json();
                    showResult('statusResult', `❌ ${result.error}`, true);
                }
            } catch (error) {
                showResult('statusResult', `❌ Download failed: ${error.message}`, true);
            }
        }
    </script>
</body>
</html>
    """
    return response.html(html)

@app.route("/upload-workflow", methods=["POST"])
async def upload_workflow(request):
    """Upload JSON workflow"""
    try:
        if 'workflow' not in request.files:
            return response.json({"error": "No workflow file"}, status=400)
        
        file = request.files['workflow'][0]
        workflow_id = str(uuid.uuid4())
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        with open(workflow_path, 'wb') as f:
            f.write(file.body)
        
        # Analyze workflow
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        steps = workflow_data if isinstance(workflow_data, list) else workflow_data.get("steps", [])
        agents = [step.get("agent", "unknown") for step in steps if step.get("agent")]
        
        return response.json({
            "workflow_id": workflow_id,
            "steps": len(steps),
            "agents": agents[:10],
            "filename": file.name
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/upload-data", methods=["POST"])
async def upload_data(request):
    """Upload data files"""
    try:
        uploaded_files = []
        for field_name, file_list in request.files.items():
            for file_obj in file_list:
                file_id = str(uuid.uuid4())
                file_path = os.path.join(UPLOAD_DIR, f"{file_id}_{file_obj.name}")
                
                with open(file_path, 'wb') as f:
                    f.write(file_obj.body)
                
                uploaded_files.append({
                    "file_id": file_id,
                    "original_name": file_obj.name,
                    "file_path": file_path
                })
        
        return response.json({"uploaded_files": uploaded_files})
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/execute", methods=["POST"])
async def execute_workflow(request):
    """Execute workflow"""
    try:
        data = request.json
        workflow_id = data['workflow_id']
        use_data = data.get('use_data', False)
        data_files = data.get('data_files', [])
        
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        execution_id = str(uuid.uuid4())
        
        # Prepare command
        if use_data and data_files:
            # Use workflow_runner_v2.py with data
            cmd = ["python", "workflow_runner_v2.py", "--workflow", workflow_path]
            cmd.extend(["--data"] + [df["file_path"] for df in data_files[:3]])
        else:
            # Use workflow_runner_v1.py
            cmd = ["python", "workflow_runner_v1.py", workflow_path]
        
        # Execute workflow
        process = subprocess.Popen(cmd, 
                                 stdout=subprocess.PIPE, 
                                 stderr=subprocess.PIPE,
                                 cwd=".")
        
        executions[execution_id] = {
            "status": "running",
            "workflow_id": workflow_id,
            "start_time": datetime.now().isoformat(),
            "process": process,
            "use_data": use_data
        }
        
        return response.json({
            "execution_id": execution_id,
            "status": "started"
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/status/<execution_id>", methods=["GET"])
async def get_status(request, execution_id):
    """Get execution status"""
    if execution_id not in executions:
        return response.json({"error": "Execution not found"}, status=404)
    
    exec_info = executions[execution_id]
    process = exec_info.get("process")
    
    if process:
        if process.poll() is None:
            status = "running"
            results_file = None
        else:
            status = "completed" if process.returncode == 0 else "failed"
            # Look for results file
            workflow_name = Path(exec_info["workflow_id"]).stem
            results_file = f"{workflow_name}_results.json"
            if os.path.exists(results_file):
                exec_info["results_file"] = results_file
    else:
        status = exec_info["status"]
        results_file = exec_info.get("results_file")
    
    return response.json({
        "execution_id": execution_id,
        "status": status,
        "start_time": exec_info["start_time"],
        "results_file": results_file
    })

@app.route("/download/<execution_id>", methods=["GET"])
async def download_results(request, execution_id):
    """Download results file"""
    if execution_id not in executions:
        return response.json({"error": "Execution not found"}, status=404)
    
    exec_info = executions[execution_id]
    workflow_name = exec_info["workflow_id"]
    results_file = f"{workflow_name}_results.json"
    
    if os.path.exists(results_file):
        with open(results_file, 'rb') as f:
            file_content = f.read()
        return response.raw(file_content, 
                          headers={"Content-Disposition": f"attachment; filename={results_file}"},
                          content_type="application/json")
    else:
        return response.json({"error": "Results file not found"}, status=404)

if __name__ == "__main__":
    print("🚀 Minimal Workflow API Server")
    print("📋 Endpoints:")
    print("  GET  /           - Web Interface")
    print("  POST /upload-workflow - Upload JSON workflow")
    print("  POST /upload-data     - Upload data files")  
    print("  POST /execute         - Execute workflow")
    print("  GET  /status/<id>     - Check status")
    print("  GET  /download/<id>   - Download results")
    print("🌐 Access at: http://localhost:8000")
    
    app.run(host="0.0.0.0", port=8000, debug=True)


==================================================
FILE: minimalistic_frontend_v2.py
==================================================

#!/usr/bin/env python3

from sanic import Sanic, response
import json
import os
import uuid
import subprocess
import csv
import io
from datetime import datetime
from pathlib import Path
import time
import threading
import sys
if sys.platform.startswith('win'):
    os.environ['PYTHONIOENCODING'] = 'utf-8'

app = Sanic("MinimalWorkflowAPI")

# Directories
UPLOAD_DIR = "./uploads"
WORKFLOWS_DIR = "./workflows" 
RESULTS_DIR = "./results"

# Ensure directories exist
for dir_path in [UPLOAD_DIR, WORKFLOWS_DIR, RESULTS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# Execution tracking
executions = {}

def monitor_execution(execution_id):
    """Monitor execution in background thread"""
    try:
        exec_info = executions[execution_id]
        process = exec_info.get("process")
        
        if process:
            print(f"⏳ Monitoring execution {execution_id}")
            print(f"📝 Command: {exec_info.get('command', 'Unknown')}")
            
            # Wait for process to complete
            stdout, stderr = process.communicate()
            
            print(f"✅ Process completed for {execution_id}")
            print(f"📤 Return code: {process.returncode}")
            print(f"📄 Stdout length: {len(stdout) if stdout else 0}")
            print(f"🚨 Stderr length: {len(stderr) if stderr else 0}")
            
            # Update execution info
            exec_info["status"] = "completed" if process.returncode == 0 else "failed"
            exec_info["end_time"] = datetime.now().isoformat()
            exec_info["return_code"] = process.returncode
            exec_info["stdout"] = stdout.decode() if stdout else ""
            exec_info["stderr"] = stderr.decode() if stderr else ""
            
            if process.returncode != 0:
                print(f"❌ Execution failed with return code {process.returncode}")
                print(f"🚨 Full Stderr:")
                print(stderr.decode() if stderr else 'No stderr')
                print(f"📤 Full Stdout:")
                print(stdout.decode() if stdout else 'No stdout')
            
            # Look for results file
            workflow_name = Path(exec_info["workflow_id"]).stem
            possible_results = [
                f"{workflow_name}_results.json",
                f"results_{workflow_name}.json",
                f"{exec_info['workflow_id']}_results.json",
                f"results.json",
                f"output.json"
            ]
            
            for results_file in possible_results:
                if os.path.exists(results_file):
                    exec_info["results_file"] = results_file
                    print(f"📄 Found results file: {results_file}")
                    break
            else:
                print(f"⚠️ No results file found. Checked: {possible_results}")
            
            # Remove process reference
            exec_info.pop("process", None)
            
    except Exception as e:
        print(f"❌ Error monitoring execution {execution_id}: {e}")
        exec_info["status"] = "failed"
        exec_info["error"] = str(e)

@app.route("/", methods=["GET"])
async def web_ui(request):
    """Serve minimal web interface"""
    html = """
<!DOCTYPE html>
<html>
<head>
    <title>Agentic Workflow Runner</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: Arial, sans-serif; max-width: 800px; margin: 40px auto; padding: 20px; }
        .section { background: #f5f5f5; padding: 20px; margin: 20px 0; border-radius: 8px; }
        button { background: #007cba; color: white; padding: 10px 20px; border: none; border-radius: 4px; cursor: pointer; }
        button:hover { background: #005a8b; }
        button:disabled { background: #ccc; cursor: not-allowed; }
        input[type="file"] { margin: 10px 0; }
        .results { background: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 4px; }
        .error { background: #ffebee; color: #c62828; }
        .success { background: #e8f5e8; color: #2e7d32; }
        .warning { background: #fff3cd; color: #856404; }
        pre { background: #f0f0f0; padding: 10px; overflow-x: auto; border-radius: 4px; }
        .spinner { display: inline-block; width: 16px; height: 16px; border: 2px solid #f3f3f3; border-top: 2px solid #007cba; border-radius: 50%; animation: spin 1s linear infinite; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
    </style>
</head>
<body>
    <h1>🤖 Agentic Workflow Runner</h1>
    
    <!-- Workflow Upload -->
    <div class="section">
        <h3>1. Upload Workflow</h3>
        <input type="file" id="workflowFile" accept=".json" />
        <button onclick="uploadWorkflow()">Upload JSON Workflow</button>
        <button id="convertBtn" onclick="convertToCSV()" style="display:none; margin-left: 10px; background: #28a745;">📊 Convert to CSV</button>
        <div id="workflowResult"></div>
    </div>
    
    <!-- Data Upload (Optional) -->
    <div class="section">
        <h3>2. Upload Data Files (Optional)</h3>
        <input type="file" id="dataFiles" multiple accept=".csv,.xlsx,.json" />
        <button onclick="uploadData()">Upload Data Files</button>
        <div id="dataResult"></div>
    </div>
    
    <!-- Execution -->
    <div class="section">
        <h3>3. Execute Workflow</h3>
        <button id="executeBtn" onclick="executeWorkflow()">▶️ Run Workflow</button>
        <button id="executeDataBtn" onclick="executeWithData()">▶️ Run with Data</button>
        <div id="executionResult"></div>
    </div>
    
    <!-- Results -->
    <div class="section">
        <h3>4. Results</h3>
        <button id="statusBtn" onclick="checkStatus()">🔄 Check Status</button>
        <button id="downloadBtn" onclick="downloadResults()">📥 Download Results</button>
        <div id="statusResult"></div>
    </div>

    <script>
        let currentWorkflowId = null;
        let currentDataFiles = [];
        let currentExecutionId = null;
        let statusInterval = null;

        function showResult(elementId, message, type = 'success') {
            const el = document.getElementById(elementId);
            const className = type === 'error' ? 'error' : type === 'warning' ? 'warning' : 'success';
            el.innerHTML = `<div class="results ${className}">${message}</div>`;
        }

        function setButtonState(buttonId, enabled, text = null) {
            const btn = document.getElementById(buttonId);
            if (btn) {
                btn.disabled = !enabled;
                if (text) btn.textContent = text;
            }
        }

        function startStatusMonitoring() {
            if (statusInterval) clearInterval(statusInterval);
            statusInterval = setInterval(async () => {
                await checkStatus(false); // Silent check
            }, 2000);
        }

        function stopStatusMonitoring() {
            if (statusInterval) {
                clearInterval(statusInterval);
                statusInterval = null;
            }
        }

        async function uploadWorkflow() {
            const fileInput = document.getElementById('workflowFile');
            if (!fileInput.files[0]) {
                showResult('workflowResult', 'Please select a JSON workflow file', 'error');
                return;
            }

            const formData = new FormData();
            formData.append('workflow', fileInput.files[0]);

            try {
                const response = await fetch('/upload-workflow', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentWorkflowId = result.workflow_id;
                    document.getElementById('convertBtn').style.display = 'inline-block';
                    showResult('workflowResult', 
                        `✅ Workflow uploaded: ${result.steps} steps, ${result.agents.length} agents`);
                } else {
                    showResult('workflowResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('workflowResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function uploadData() {
            const fileInput = document.getElementById('dataFiles');
            if (!fileInput.files.length) {
                showResult('dataResult', 'Please select data files', 'error');
                return;
            }

            const formData = new FormData();
            Array.from(fileInput.files).forEach((file, index) => {
                formData.append(`data_${index}`, file);
            });

            try {
                const response = await fetch('/upload-data', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentDataFiles = result.uploaded_files;
                    showResult('dataResult', 
                        `✅ Uploaded ${result.uploaded_files.length} data files`);
                } else {
                    showResult('dataResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('dataResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function executeWorkflow() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', 'error');
                return;
            }

            setButtonState('executeBtn', false, '⏳ Starting...');
            setButtonState('executeDataBtn', false);

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        use_data: false
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution started: ${result.execution_id}<br><div class="spinner"></div> Running...`);
                    startStatusMonitoring();
                } else {
                    showResult('executionResult', `❌ ${result.error}`, 'error');
                    setButtonState('executeBtn', true, '▶️ Run Workflow');
                    setButtonState('executeDataBtn', true);
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, 'error');
                setButtonState('executeBtn', true, '▶️ Run Workflow');
                setButtonState('executeDataBtn', true);
            }
        }

        async function executeWithData() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', 'error');
                return;
            }
            if (!currentDataFiles.length) {
                showResult('executionResult', 'Please upload data files first', 'error');
                return;
            }

            setButtonState('executeBtn', false);
            setButtonState('executeDataBtn', false, '⏳ Starting...');

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        data_files: currentDataFiles,
                        use_data: true
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution with data started: ${result.execution_id}<br><div class="spinner"></div> Running...`);
                    startStatusMonitoring();
                } else {
                    showResult('executionResult', `❌ ${result.error}`, 'error');
                    setButtonState('executeBtn', true);
                    setButtonState('executeDataBtn', true, '▶️ Run with Data');
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, 'error');
                setButtonState('executeBtn', true);
                setButtonState('executeDataBtn', true, '▶️ Run with Data');
            }
        }

        async function checkStatus(showMessage = true) {
            if (!currentExecutionId) {
                if (showMessage) showResult('statusResult', 'No execution in progress', 'error');
                return;
            }

            try {
                const response = await fetch(`/status/${currentExecutionId}`);
                const result = await response.json();
                
                if (response.ok) {
                    const statusMessage = `Status: ${result.status}<br>
                         Started: ${result.start_time}<br>
                         ${result.end_time ? `Ended: ${result.end_time}<br>` : ''}
                         ${result.results_file ? '✅ Results ready' : '⏳ Running...'}`;
                    
                    if (showMessage) showResult('statusResult', statusMessage);
                    
                    // Update execution status display
                    if (result.status === 'completed') {
                        stopStatusMonitoring();
                        showResult('executionResult', 
                            `✅ Execution completed: ${currentExecutionId}<br>📄 Results available for download`);
                        setButtonState('executeBtn', true, '▶️ Run Workflow');
                        setButtonState('executeDataBtn', true, '▶️ Run with Data');
                        setButtonState('downloadBtn', true);
                    } else if (result.status === 'failed') {
                        stopStatusMonitoring();
                        showResult('executionResult', 
                            `❌ Execution failed: ${currentExecutionId}<br>${result.error || 'Check logs for details'}`, 'error');
                        setButtonState('executeBtn', true, '▶️ Run Workflow');
                        setButtonState('executeDataBtn', true, '▶️ Run with Data');
                    }
                } else {
                    if (showMessage) showResult('statusResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                if (showMessage) showResult('statusResult', `❌ Status check failed: ${error.message}`, 'error');
            }
        }

        async function downloadResults() {
            if (!currentExecutionId) {
                showResult('statusResult', 'No execution to download', 'error');
                return;
            }

            try {
                const response = await fetch(`/download/${currentExecutionId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `results_${currentExecutionId}.json`;
                    a.click();
                    window.URL.revokeObjectURL(url);
                    showResult('statusResult', '✅ Results downloaded');
                } else {
                    const result = await response.json();
                    showResult('statusResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('statusResult', `❌ Download failed: ${error.message}`, 'error');
            }
        }

        async function convertToCSV() {
            if (!currentWorkflowId) {
                showResult('workflowResult', 'No workflow to convert', 'error');
                return;
            }

            try {
                const response = await fetch(`/convert-to-csv/${currentWorkflowId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `workflow_${currentWorkflowId}.csv`;
                    a.click();
                    window.URL.revokeObjectURL(url);
                    
                    const originalText = document.getElementById('workflowResult').textContent;
                    const stepsText = originalText.split('agents')[0] + 'agents';
                    showResult('workflowResult', 
                        `${stepsText}<br>📊 CSV downloaded successfully!`);
                } else {
                    const result = await response.json();
                    showResult('workflowResult', `❌ Conversion failed: ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('workflowResult', `❌ CSV conversion failed: ${error.message}`, 'error');
            }
        }

        // Clean up on page unload
        window.addEventListener('beforeunload', () => {
            stopStatusMonitoring();
        });
    </script>
</body>
</html>
    """
    return response.html(html)

@app.route("/upload-workflow", methods=["POST"])
async def upload_workflow(request):
    """Upload JSON workflow"""
    try:
        if 'workflow' not in request.files:
            return response.json({"error": "No workflow file"}, status=400)
        
        file = request.files['workflow'][0]
        workflow_id = str(uuid.uuid4())
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        with open(workflow_path, 'wb') as f:
            f.write(file.body)
        
        # Analyze workflow
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        steps = workflow_data if isinstance(workflow_data, list) else workflow_data.get("steps", [])
        agents = [step.get("agent", "unknown") for step in steps if step.get("agent")]
        
        return response.json({
            "workflow_id": workflow_id,
            "steps": len(steps),
            "agents": list(set(agents))[:10],  # Remove duplicates and limit
            "filename": file.name
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/convert-to-csv/<workflow_id>", methods=["GET"])
async def convert_to_csv(request, workflow_id):
    """Convert JSON workflow to CSV format"""
    try:
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        if not os.path.exists(workflow_path):
            return response.json({"error": "Workflow not found"}, status=404)
        
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        # Handle both list and dict formats
        steps = workflow_data if isinstance(workflow_data, list) else workflow_data.get("steps", [])
        
        # Create CSV in memory
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Write header
        if steps:
            # Get all possible keys from all steps
            all_keys = set()
            for step in steps:
                all_keys.update(step.keys())
            
            headers = sorted(list(all_keys))
            writer.writerow(headers)
            
            # Write data rows
            for step in steps:
                row = [step.get(key, '') for key in headers]
                writer.writerow(row)
        else:
            # Empty workflow
            writer.writerow(['step', 'agent', 'action', 'input', 'output'])
        
        csv_content = output.getvalue().encode('utf-8')
        output.close()
        
        return response.raw(
            csv_content,
            headers={
                "Content-Disposition": f"attachment; filename=workflow_{workflow_id}.csv",
                "Content-Type": "text/csv"
            }
        )
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/upload-data", methods=["POST"])
async def upload_data(request):
    """Upload data files"""
    try:
        uploaded_files = []
        for field_name, file_list in request.files.items():
            for file_obj in file_list:
                file_id = str(uuid.uuid4())
                file_path = os.path.join(UPLOAD_DIR, f"{file_id}_{file_obj.name}")
                
                with open(file_path, 'wb') as f:
                    f.write(file_obj.body)
                
                uploaded_files.append({
                    "file_id": file_id,
                    "original_name": file_obj.name,
                    "file_path": file_path
                })
        
        return response.json({"uploaded_files": uploaded_files})
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/execute", methods=["POST"])
async def execute_workflow(request):
    """Execute workflow"""
    try:
        data = request.json
        workflow_id = data['workflow_id']
        use_data = data.get('use_data', False)
        data_files = data.get('data_files', [])
        
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        if not os.path.exists(workflow_path):
            return response.json({"error": "Workflow not found"}, status=404)
        
        execution_id = str(uuid.uuid4())
        
        # Prepare command with better error checking
        if use_data and data_files:
            # Use workflow_runner_v2.py with data
            runner_script = "workflow_runner_v2.py"
            cmd = ["python", runner_script, "--workflow", workflow_path]
            cmd.extend(["--data"] + [df["file_path"] for df in data_files[:3]])
        else:
            # Use workflow_runner_v1.py
            runner_script = "workflow_runner_v1.py"
            cmd = ["python", runner_script, workflow_path]
        
        # Check if runner script exists
        if not os.path.exists(runner_script):
            return response.json({
                "error": f"Runner script '{runner_script}' not found. Please ensure the script exists in the current directory."
            }, status=404)
        
        print(f"🚀 Executing command: {' '.join(cmd)}")
        print(f"📁 Working directory: {os.getcwd()}")
        print(f"📄 Workflow file exists: {os.path.exists(workflow_path)}")
        print(f"🐍 Runner script exists: {os.path.exists(runner_script)}")
        
        # Execute workflow
        process = subprocess.Popen(cmd, 
                                 stdout=subprocess.PIPE, 
                                 stderr=subprocess.PIPE,
                                 cwd=".")
        
        executions[execution_id] = {
            "status": "running",
            "workflow_id": workflow_id,
            "start_time": datetime.now().isoformat(),
            "process": process,
            "use_data": use_data,
            "command": ' '.join(cmd),
            "runner_script": runner_script
        }
        
        # Start monitoring in background thread
        thread = threading.Thread(target=monitor_execution, args=(execution_id,))
        thread.daemon = True
        thread.start()
        
        return response.json({
            "execution_id": execution_id,
            "status": "started",
            "command": ' '.join(cmd)
        })
        
    except Exception as e:
        print(f"❌ Execution error: {str(e)}")
        return response.json({"error": str(e)}, status=500)

@app.route("/status/<execution_id>", methods=["GET"])
async def get_status(request, execution_id):
    """Get execution status"""
    if execution_id not in executions:
        return response.json({"error": "Execution not found"}, status=404)
    
    exec_info = executions[execution_id]
    
    return response.json({
        "execution_id": execution_id,
        "status": exec_info["status"],
        "start_time": exec_info["start_time"],
        "end_time": exec_info.get("end_time"),
        "results_file": exec_info.get("results_file"),
        "error": exec_info.get("error"),
        "return_code": exec_info.get("return_code"),
        "command": exec_info.get("command"),
        "runner_script": exec_info.get("runner_script"),
        "stdout": exec_info.get("stdout", "")[:5000],  # Increased for full debugging
        "stderr": exec_info.get("stderr", "")[:5000]
    })

@app.route("/download/<execution_id>", methods=["GET"])
async def download_results(request, execution_id):
    """Download results file"""
    if execution_id not in executions:
        return response.json({"error": "Execution not found"}, status=404)
    
    exec_info = executions[execution_id]
    results_file = exec_info.get("results_file")
    
    if not results_file:
        # Try to find results file
        workflow_name = exec_info["workflow_id"]
        possible_files = [
            f"{workflow_name}_results.json",
            f"results_{workflow_name}.json",
            f"{execution_id}_results.json"
        ]
        
        for filename in possible_files:
            if os.path.exists(filename):
                results_file = filename
                break
    
    if results_file and os.path.exists(results_file):
        try:
            with open(results_file, 'rb') as f:
                file_content = f.read()
            return response.raw(file_content, 
                              headers={"Content-Disposition": f"attachment; filename={os.path.basename(results_file)}"},
                              content_type="application/json")
        except Exception as e:
            return response.json({"error": f"Failed to read results file: {str(e)}"}, status=500)
    else:
        return response.json({"error": "Results file not found"}, status=404)

if __name__ == "__main__":
    print("🚀 Minimal Workflow API Server")
    print("📋 Endpoints:")
    print("  GET  /                    - Web Interface")
    print("  POST /upload-workflow     - Upload JSON workflow")
    print("  GET  /convert-to-csv/<id> - Convert workflow to CSV")
    print("  POST /upload-data         - Upload data files")  
    print("  POST /execute             - Execute workflow")
    print("  GET  /status/<id>         - Check status")
    print("  GET  /download/<id>       - Download results")
    print("🌐 Access at: http://localhost:8000")
    
    app.run(host="0.0.0.0", port=8000, debug=True)


==================================================
FILE: minimalistic_frontend_v3.py
==================================================

#!/usr/bin/env python3

from sanic import Sanic, response
import json
import os
import uuid
import subprocess
import csv
import io
from datetime import datetime
from pathlib import Path
import time
import threading
import sys
if sys.platform.startswith('win'):
    os.environ['PYTHONIOENCODING'] = 'utf-8'

app = Sanic("BusinessWorkflowAPI")

# Directories
UPLOAD_DIR = "./uploads"
WORKFLOWS_DIR = "./workflows" 
RESULTS_DIR = "./results"

# Ensure directories exist
for dir_path in [UPLOAD_DIR, WORKFLOWS_DIR, RESULTS_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# Execution tracking
executions = {}

def convert_workflow_to_business_csv(workflow_data):
    """Convert JSON workflow to business-friendly CSV format"""
    business_rows = []
    step_counter = 1
    variables = {}
    
    # First pass: collect variables
    for item in workflow_data:
        if item.get("type") == "state" and item.get("operation") == "set_variable":
            variables[item.get("variable_name")] = item.get("value")
    
    # Second pass: convert agent steps
    for item in workflow_data:
        if item.get("type") == "state":
            continue  # Skip state items for business CSV
            
        # Convert agent step to business-friendly format
        row = {
            "Step": step_counter,
            "Agent_Name": item.get("agent", ""),
            "What_It_Does": item.get("content", ""),
            "Tools_Used": ", ".join(item.get("tools", [])) if isinstance(item.get("tools"), list) else str(item.get("tools", "")),
            "Key_Parameters": format_parameters_for_business(item.get("parameters", {})),
            "Waits_For": format_dependencies(item.get("readFrom", [])),
            "Expected_Output": item.get("output_format", ""),
            "Business_Priority": item.get("priority", "MEDIUM"),
            "Estimated_Time": item.get("estimated_time", "2 min"),
            "Notes": item.get("notes", generate_business_notes(item)),
            "Custom_Field_1": item.get("custom_1", ""),
            "Custom_Field_2": item.get("custom_2", ""),
            "Enabled": item.get("enabled", "YES")
        }
        
        business_rows.append(row)
        step_counter += 1
    
    return business_rows, variables

def format_parameters_for_business(params):
    """Convert JSON parameters to business English"""
    if not params:
        return "Default settings"
    
    if isinstance(params, str):
        return params
    
    business_params = []
    for key, value in params.items():
        if key == "query":
            business_params.append(f"Search for: {value}")
        elif key == "url":
            business_params.append(f"Visit: {value}")
        elif key == "num_results":
            business_params.append(f"Get {value} results")
        elif key == "confidence_level":
            business_params.append(f"{float(value)*100}% confidence")
        elif key == "host":
            business_params.append(f"Connect to: {value}")
        elif key == "timeout":
            business_params.append(f"Wait {value} seconds max")
        elif key == "format":
            business_params.append(f"Output as: {value}")
        else:
            business_params.append(f"{key.replace('_', ' ').title()}: {value}")
    
    return " | ".join(business_params)

def format_dependencies(read_from):
    """Format dependencies in business English"""
    if not read_from:
        return "NONE"
    elif isinstance(read_from, list):
        if len(read_from) == 1:
            return read_from[0]
        else:
            return " + ".join(read_from)
    else:
        return str(read_from)

def generate_business_notes(item):
    """Generate helpful business context"""
    tools = item.get("tools", [])
    if not tools:
        return "Data processing step"
    
    first_tool = tools[0] if isinstance(tools, list) else str(tools)
    
    tool_notes = {
        "research:search": "Web research and analysis",
        "research:combined_search": "Comprehensive web research",
        "browser:navigate": "Live website data collection", 
        "browser:create": "Initialize web browser",
        "browser:get_content": "Extract website content",
        "ml:predict": "AI prediction and modeling",
        "ml:train_model": "Train machine learning model",
        "optimization:multi_objective": "Mathematical optimization",
        "cognitive:create_session": "Multi-perspective analysis",
        "memory:create_system": "Create knowledge database",
        "clickhouse_ssh:connect": "Database connection",
        "clickhouse_ssh:execute_query": "Database query",
        "email:send": "Send email notification",
        "slack:message": "Send Slack message",
        "file:read": "Read file content",
        "file:write": "Save data to file",
        "api:call": "Call external API"
    }
    
    return tool_notes.get(first_tool, f"Uses {first_tool.replace(':', ' - ')}")

def convert_business_csv_to_workflow(csv_data, variables=None):
    """Convert business CSV back to JSON workflow with full flexibility"""
    workflow = []
    
    # Add variables first if they exist
    if variables:
        for var_name, var_value in variables.items():
            workflow.append({
                "type": "state",
                "operation": "set_variable",
                "variable_name": var_name,
                "value": var_value
            })
    
    # Convert business rows back to workflow steps
    for row in csv_data:
        if not row.get("Agent_Name") or not row.get("Agent_Name").strip():  # Skip empty rows
            continue
        
        # Skip instruction rows
        if str(row.get("Step", "")).startswith("INSTRUCTIONS") or str(row.get("Step", "")).endswith("."):
            continue
            
        # Check if step is enabled
        enabled = str(row.get("Enabled", "YES")).upper()
        if enabled in ["NO", "FALSE", "DISABLED", "0"]:
            continue  # Skip disabled steps
        
        # Parse tools
        tools_str = row.get("Tools_Used", "").strip()
        if tools_str and tools_str != "":
            tools = [tool.strip() for tool in tools_str.split(",") if tool.strip()]
        else:
            tools = []
        
        # Parse parameters
        params = parse_business_parameters(row.get("Key_Parameters", ""))
        
        # Parse dependencies
        dependencies = parse_business_dependencies(row.get("Waits_For", ""))
        
        # Create workflow step with all possible fields
        step = {
            "agent": row.get("Agent_Name", "").strip(),
            "content": row.get("What_It_Does", "").strip(),
            "tools": tools,
            "output_format": row.get("Expected_Output", "").strip()
        }
        
        # Add optional fields if they exist
        if params:
            step["parameters"] = params
            
        if dependencies:
            step["readFrom"] = dependencies
        
        # Add business fields
        priority = row.get("Business_Priority", "").strip()
        if priority:
            step["priority"] = priority
            
        estimated_time = row.get("Estimated_Time", "").strip()
        if estimated_time:
            step["estimated_time"] = estimated_time
            
        notes = row.get("Notes", "").strip()
        if notes and notes != generate_business_notes(step):
            step["notes"] = notes
            
        # Add custom fields
        custom_1 = row.get("Custom_Field_1", "").strip()
        if custom_1:
            step["custom_1"] = custom_1
            
        custom_2 = row.get("Custom_Field_2", "").strip()
        if custom_2:
            step["custom_2"] = custom_2
        
        # Add any additional fields from CSV that aren't in standard format
        for key, value in row.items():
            if key not in ["Step", "Agent_Name", "What_It_Does", "Tools_Used", "Key_Parameters", 
                          "Waits_For", "Expected_Output", "Business_Priority", "Estimated_Time", 
                          "Notes", "Custom_Field_1", "Custom_Field_2", "Enabled"] and str(value).strip():
                # Convert CSV column name to JSON field name
                json_key = key.lower().replace(" ", "_").replace("-", "_")
                step[json_key] = str(value).strip()
        
        workflow.append(step)
    
    return workflow

def parse_business_parameters(params_str):
    """Parse business-friendly parameters back to JSON with enhanced flexibility"""
    if not params_str or params_str.strip() == "Default settings":
        return {}
    
    params = {}
    parts = [part.strip() for part in params_str.split(" | ") if part.strip()]
    
    for part in parts:
        if part.startswith("Search for: "):
            params["query"] = part.replace("Search for: ", "")
        elif part.startswith("Visit: "):
            params["url"] = part.replace("Visit: ", "")
        elif part.startswith("Get ") and " results" in part:
            try:
                num_str = part.replace("Get ", "").replace(" results", "").strip()
                params["num_results"] = int(num_str)
            except:
                pass
        elif part.startswith("Connect to: "):
            params["host"] = part.replace("Connect to: ", "")
        elif part.startswith("Wait ") and " seconds max" in part:
            try:
                timeout_str = part.replace("Wait ", "").replace(" seconds max", "").strip()
                params["timeout"] = int(timeout_str)
            except:
                pass
        elif part.startswith("Output as: "):
            params["format"] = part.replace("Output as: ", "")
        elif part.endswith("% confidence"):
            try:
                conf_str = part.replace("% confidence", "").strip()
                params["confidence_level"] = float(conf_str) / 100.0
            except:
                pass
        elif ": " in part:
            key, value = part.split(": ", 1)
            # Convert business key back to technical key
            key = key.lower().replace(" ", "_")
            
            # Try to convert to appropriate type
            try:
                if value.lower() in ["true", "false"]:
                    params[key] = value.lower() == "true"
                elif value.lower() in ["yes", "no"]:
                    params[key] = value.lower() == "yes"
                elif value.isdigit():
                    params[key] = int(value)
                elif "." in value and value.replace(".", "").replace("-", "").isdigit():
                    params[key] = float(value)
                else:
                    params[key] = value
            except:
                params[key] = value
        else:
            # Handle free-form parameters
            if "=" in part:
                key, value = part.split("=", 1)
                params[key.strip()] = value.strip()
    
    return params

def parse_business_dependencies(deps_str):
    """Parse business dependencies back to list with enhanced flexibility"""
    if not deps_str or deps_str.strip().upper() == "NONE":
        return []
    
    deps_str = deps_str.strip()
    
    # Handle various separators
    separators = [" + ", " & ", " AND ", ",", ";"]
    dependencies = [deps_str]
    
    for sep in separators:
        new_deps = []
        for dep in dependencies:
            new_deps.extend([d.strip() for d in dep.split(sep) if d.strip()])
        dependencies = new_deps
    
    return [dep for dep in dependencies if dep and dep.upper() != "NONE"]

def create_template_csv():
    """Create a template CSV for business users"""
    template_rows = [
        {
            "Step": 1,
            "Agent_Name": "DataCollector",
            "What_It_Does": "Gather market research data from web sources",
            "Tools_Used": "research:search, browser:navigate",
            "Key_Parameters": "Search for: market trends 2025 | Get 10 results",
            "Waits_For": "NONE",
            "Expected_Output": "JSON with market data and trends",
            "Business_Priority": "HIGH",
            "Estimated_Time": "3 min",
            "Notes": "Focus on latest industry reports",
            "Custom_Field_1": "",
            "Custom_Field_2": "",
            "Enabled": "YES"
        },
        {
            "Step": 2,
            "Agent_Name": "DataProcessor",
            "What_It_Does": "Analyze collected data and extract key insights",
            "Tools_Used": "ml:predict",
            "Key_Parameters": "Confidence Level: 85% | Output as: structured_summary",
            "Waits_For": "DataCollector",
            "Expected_Output": "Structured analysis with recommendations",
            "Business_Priority": "HIGH",
            "Estimated_Time": "2 min",
            "Notes": "Use advanced analytics",
            "Custom_Field_1": "Department: Marketing",
            "Custom_Field_2": "Stakeholder: CMO",
            "Enabled": "YES"
        }
    ]
    
    return template_rows

def monitor_execution(execution_id):
    """Monitor execution in background thread"""
    try:
        exec_info = executions[execution_id]
        process = exec_info.get("process")
        
        if process:
            print(f"⏳ Monitoring execution {execution_id}")
            print(f"📝 Command: {exec_info.get('command', 'Unknown')}")
            
            # Wait for process to complete
            stdout, stderr = process.communicate()
            
            print(f"✅ Process completed for {execution_id}")
            print(f"📤 Return code: {process.returncode}")
            
            # Update execution info
            exec_info["status"] = "completed" if process.returncode == 0 else "failed"
            exec_info["end_time"] = datetime.now().isoformat()
            exec_info["return_code"] = process.returncode
            exec_info["stdout"] = stdout.decode() if stdout else ""
            exec_info["stderr"] = stderr.decode() if stderr else ""
            
            if process.returncode != 0:
                print(f"❌ Execution failed with return code {process.returncode}")
                print(f"🚨 Stderr: {stderr.decode() if stderr else 'No stderr'}")
            
            # Look for results file
            workflow_name = Path(exec_info["workflow_id"]).stem
            possible_results = [
                f"{workflow_name}_results.json",
                f"results_{workflow_name}.json",
                f"{exec_info['workflow_id']}_results.json",
                f"results.json"
            ]
            
            for results_file in possible_results:
                if os.path.exists(results_file):
                    exec_info["results_file"] = results_file
                    print(f"📄 Found results file: {results_file}")
                    break
            
            # Remove process reference
            exec_info.pop("process", None)
            
    except Exception as e:
        print(f"❌ Error monitoring execution {execution_id}: {e}")
        exec_info["status"] = "failed"
        exec_info["error"] = str(e)

@app.route("/", methods=["GET"])
async def web_ui(request):
    """Serve enhanced web interface with CSV editing"""
    html = """
<!DOCTYPE html>
<html>
<head>
    <title>Business-Friendly Workflow Manager</title>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <style>
        body { font-family: Arial, sans-serif; max-width: 1200px; margin: 40px auto; padding: 20px; }
        .section { background: #f5f5f5; padding: 20px; margin: 20px 0; border-radius: 8px; }
        button { background: #007cba; color: white; padding: 12px 24px; border: none; border-radius: 4px; cursor: pointer; margin: 5px; font-size: 14px; }
        button:hover { background: #005a8b; }
        button:disabled { background: #ccc; cursor: not-allowed; }
        .btn-success { background: #28a745; }
        .btn-success:hover { background: #218838; }
        .btn-warning { background: #ffc107; color: #212529; }
        .btn-warning:hover { background: #e0a800; }
        .btn-info { background: #17a2b8; }
        .btn-info:hover { background: #138496; }
        input[type="file"] { margin: 10px 0; padding: 8px; }
        .results { background: #e8f4f8; padding: 15px; margin: 10px 0; border-radius: 4px; }
        .error { background: #ffebee; color: #c62828; }
        .success { background: #e8f5e8; color: #2e7d32; }
        .warning { background: #fff3cd; color: #856404; }
        .info { background: #d1ecf1; color: #0c5460; }
        pre { background: #f0f0f0; padding: 10px; overflow-x: auto; border-radius: 4px; font-size: 12px; }
        .spinner { display: inline-block; width: 16px; height: 16px; border: 2px solid #f3f3f3; border-top: 2px solid #007cba; border-radius: 50%; animation: spin 1s linear infinite; }
        @keyframes spin { 0% { transform: rotate(0deg); } 100% { transform: rotate(360deg); } }
        .workflow-buttons { display: none; margin-top: 15px; }
        .workflow-buttons.show { display: block; }
        .instructions { background: #f8f9fa; padding: 15px; border-left: 4px solid #007cba; margin: 15px 0; }
        .feature-highlight { background: linear-gradient(135deg, #667eea 0%, #764ba2 100%); color: white; padding: 20px; border-radius: 8px; margin: 20px 0; }
    </style>
</head>
<body>
    <div class="feature-highlight">
        <h1>🚀 Business-Friendly Workflow Manager</h1>
        <p><strong>✨ Full Business Freedom:</strong> Create, edit, and manage workflows entirely through Excel/CSV!</p>
    </div>
    
    <!-- Quick Start -->
    <div class="section">
        <h3>🎯 Quick Start for Business Users</h3>
        <div class="instructions">
            <strong>New to workflows? Start here:</strong>
            <ol>
                <li>Download a template CSV to see the format</li>
                <li>Edit it in Excel with your business requirements</li>
                <li>Upload your edited CSV to create a workflow</li>
                <li>Execute and get results!</li>
            </ol>
        </div>
        <button class="btn-info" onclick="downloadTemplate()">📋 Download Template CSV</button>
        <button class="btn-info" onclick="showExamples()">📖 View Examples</button>
        <div id="exampleResult"></div>
    </div>
    
    <!-- Direct CSV Upload for Business Users -->
    <div class="section">
        <h3>📊 Business User: Create Workflow from CSV</h3>
        <div class="instructions">
            <strong>For Business Users:</strong> Upload your CSV file directly (no JSON needed!)
        </div>
        <input type="file" id="businessCsvFile" accept=".csv" />
        <button class="btn-success" onclick="uploadBusinessCSV()">📤 Upload Business CSV & Create Workflow</button>
        <div id="businessCsvResult"></div>
    </div>
    
    <!-- Technical Users: JSON Upload -->
    <div class="section">
        <h3>⚙️ Technical Users: Upload JSON Workflow</h3>
        <input type="file" id="workflowFile" accept=".json" />
        <button onclick="uploadWorkflow()">Upload JSON Workflow</button>
        
        <div class="workflow-buttons" id="workflowButtons">
            <button class="btn-success" onclick="convertToBusinessCSV()">📊 Convert to Business CSV</button>
            <button class="btn-warning" onclick="convertToTechnicalCSV()">⚙️ Convert to Technical CSV</button>
        </div>
        
        <div id="workflowResult"></div>
    </div>
    
    <!-- CSV Editing -->
    <div class="section">
        <h3>✏️ Edit Existing Workflow</h3>
        <p><strong>Instructions:</strong> Download CSV → Edit in Excel → Upload modified CSV</p>
        <input type="file" id="csvFile" accept=".csv" />
        <button onclick="uploadEditedCSV()">📝 Upload Edited CSV</button>
        <div id="csvResult"></div>
    </div>
    
    <!-- Data Upload (Optional) -->
    <div class="section">
        <h3>📁 Upload Data Files (Optional)</h3>
        <input type="file" id="dataFiles" multiple accept=".csv,.xlsx,.json,.txt" />
        <button onclick="uploadData()">📂 Upload Data Files</button>
        <div id="dataResult"></div>
    </div>
    
    <!-- Execution -->
    <div class="section">
        <h3>🚀 Execute Workflow</h3>
        <button id="executeBtn" onclick="executeWorkflow()" disabled>▶️ Run Workflow</button>
        <button id="executeDataBtn" onclick="executeWithData()" disabled>▶️ Run with Data</button>
        <div id="executionResult"></div>
    </div>
    
    <!-- Results -->
    <div class="section">
        <h3>📈 Results & Status</h3>
        <button id="statusBtn" onclick="checkStatus()">🔄 Check Status</button>
        <button id="downloadBtn" onclick="downloadResults()" disabled>📥 Download Results</button>
        <div id="statusResult"></div>
    </div>

    <script>
        let currentWorkflowId = null;
        let currentDataFiles = [];
        let currentExecutionId = null;
        let statusInterval = null;

        function showResult(elementId, message, type = 'success') {
            const el = document.getElementById(elementId);
            const className = type === 'error' ? 'error' : type === 'warning' ? 'warning' : type === 'info' ? 'info' : 'success';
            el.innerHTML = `<div class="results ${className}">${message}</div>`;
        }

        function setButtonState(buttonId, enabled, text = null) {
            const btn = document.getElementById(buttonId);
            if (btn) {
                btn.disabled = !enabled;
                if (text) btn.textContent = text;
            }
        }

        function enableExecutionButtons() {
            setButtonState('executeBtn', true);
            setButtonState('executeDataBtn', true);
        }

        function startStatusMonitoring() {
            if (statusInterval) clearInterval(statusInterval);
            statusInterval = setInterval(async () => {
                await checkStatus(false);
            }, 2000);
        }

        function stopStatusMonitoring() {
            if (statusInterval) {
                clearInterval(statusInterval);
                statusInterval = null;
            }
        }

        async function downloadTemplate() {
            try {
                const response = await fetch('/template-csv');
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = 'workflow_template.csv';
                    a.click();
                    window.URL.revokeObjectURL(url);
                    
                    showResult('exampleResult', 
                        `📋 Template downloaded! Edit in Excel and upload back.`, 'info');
                } else {
                    const result = await response.json();
                    showResult('exampleResult', `❌ Template download failed: ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('exampleResult', `❌ Template download failed: ${error.message}`, 'error');
            }
        }

        async function showExamples() {
            showResult('exampleResult', `
                <h4>📚 CSV Format Examples:</h4>
                <pre>
<strong>Basic Step Format:</strong>
Step,Agent_Name,What_It_Does,Tools_Used,Key_Parameters,Waits_For,Expected_Output
1,ResearchAgent,Find market trends,research:search,Search for: AI market 2025,NONE,Market analysis report

<strong>Advanced Parameters:</strong>
Key_Parameters: Search for: market data | Get 20 results | 95% confidence | Timeout: 30 seconds

<strong>Dependencies:</strong>
Waits_For: ResearchAgent + DataCollector (waits for both to complete)

<strong>Custom Fields:</strong>
Custom_Field_1: Department: Marketing
Custom_Field_2: Budget: $5000
                </pre>
            `, 'info');
        }

        async function uploadBusinessCSV() {
            const fileInput = document.getElementById('businessCsvFile');
            if (!fileInput.files[0]) {
                showResult('businessCsvResult', 'Please select a CSV file', 'error');
                return;
            }

            const formData = new FormData();
            formData.append('csv_file', fileInput.files[0]);

            try {
                const response = await fetch('/upload-business-csv', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentWorkflowId = result.workflow_id;
                    showResult('businessCsvResult', 
                        `✅ Workflow created from CSV! ID: ${result.workflow_id}<br>
                         📊 ${result.steps} steps processed<br>
                         🤖 Agents: ${result.agents.join(', ')}<br>
                         Ready to execute!`);
                    
                    enableExecutionButtons();
                } else {
                    showResult('businessCsvResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('businessCsvResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function uploadWorkflow() {
            const fileInput = document.getElementById('workflowFile');
            if (!fileInput.files[0]) {
                showResult('workflowResult', 'Please select a JSON workflow file', 'error');
                return;
            }

            const formData = new FormData();
            formData.append('workflow', fileInput.files[0]);

            try {
                const response = await fetch('/upload-workflow', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentWorkflowId = result.workflow_id;
                    document.getElementById('workflowButtons').classList.add('show');
                    showResult('workflowResult', 
                        `✅ Workflow uploaded: ${result.steps} steps, ${result.agents.length} agents`);
                    enableExecutionButtons();
                } else {
                    showResult('workflowResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('workflowResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function convertToBusinessCSV() {
            if (!currentWorkflowId) {
                showResult('workflowResult', 'No workflow to convert', 'error');
                return;
            }

            try {
                const response = await fetch(`/convert-to-business-csv/${currentWorkflowId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `business_workflow_${currentWorkflowId}.csv`;
                    a.click();
                    window.URL.revokeObjectURL(url);
                    
                    showResult('workflowResult', 
                        `📊 Business CSV downloaded! Edit in Excel and upload back.`, 'info');
                } else {
                    const result = await response.json();
                    showResult('workflowResult', `❌ Conversion failed: ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('workflowResult', `❌ CSV conversion failed: ${error.message}`, 'error');
            }
        }

        async function convertToTechnicalCSV() {
            if (!currentWorkflowId) {
                showResult('workflowResult', 'No workflow to convert', 'error');
                return;
            }

            try {
                const response = await fetch(`/convert-to-technical-csv/${currentWorkflowId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `technical_workflow_${currentWorkflowId}.csv`;
                    a.click();
                    window.URL.revokeObjectURL(url);
                    
                    showResult('workflowResult', 
                        `⚙️ Technical CSV downloaded!`, 'info');
                } else {
                    const result = await response.json();
                    showResult('workflowResult', `❌ Conversion failed: ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('workflowResult', `❌ CSV conversion failed: ${error.message}`, 'error');
            }
        }

        async function uploadEditedCSV() {
            const fileInput = document.getElementById('csvFile');
            if (!fileInput.files[0]) {
                showResult('csvResult', 'Please select a CSV file', 'error');
                return;
            }

            const formData = new FormData();
            formData.append('csv_file', fileInput.files[0]);

            try {
                const response = await fetch('/upload-edited-csv', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentWorkflowId = result.workflow_id;
                    showResult('csvResult', 
                        `✅ CSV processed! New workflow created with ${result.steps} steps. Ready to execute.`);
                    
                    enableExecutionButtons();
                } else {
                    showResult('csvResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('csvResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function uploadData() {
            const fileInput = document.getElementById('dataFiles');
            if (!fileInput.files.length) {
                showResult('dataResult', 'Please select data files', 'error');
                return;
            }

            const formData = new FormData();
            Array.from(fileInput.files).forEach((file, index) => {
                formData.append(`data_${index}`, file);
            });

            try {
                const response = await fetch('/upload-data', {
                    method: 'POST',
                    body: formData
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentDataFiles = result.uploaded_files;
                    showResult('dataResult', 
                        `✅ Uploaded ${result.uploaded_files.length} data files`);
                } else {
                    showResult('dataResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('dataResult', `❌ Upload failed: ${error.message}`, 'error');
            }
        }

        async function executeWorkflow() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', 'error');
                return;
            }

            setButtonState('executeBtn', false, '⏳ Starting...');
            setButtonState('executeDataBtn', false);

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        use_data: false
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution started: ${result.execution_id}<br><div class="spinner"></div> Running...`);
                    startStatusMonitoring();
                } else {
                    showResult('executionResult', `❌ ${result.error}`, 'error');
                    setButtonState('executeBtn', true, '▶️ Run Workflow');
                    setButtonState('executeDataBtn', true);
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, 'error');
                setButtonState('executeBtn', true, '▶️ Run Workflow');
                setButtonState('executeDataBtn', true);
            }
        }

        async function executeWithData() {
            if (!currentWorkflowId) {
                showResult('executionResult', 'Please upload a workflow first', 'error');
                return;
            }
            if (!currentDataFiles.length) {
                showResult('executionResult', 'Please upload data files first', 'error');
                return;
            }

            setButtonState('executeBtn', false);
            setButtonState('executeDataBtn', false, '⏳ Starting...');

            try {
                const response = await fetch('/execute', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ 
                        workflow_id: currentWorkflowId,
                        data_files: currentDataFiles,
                        use_data: true
                    })
                });
                const result = await response.json();
                
                if (response.ok) {
                    currentExecutionId = result.execution_id;
                    showResult('executionResult', 
                        `✅ Execution with data started: ${result.execution_id}<br><div class="spinner"></div> Running...`);
                    startStatusMonitoring();
                } else {
                    showResult('executionResult', `❌ ${result.error}`, 'error');
                    setButtonState('executeBtn', true);
                    setButtonState('executeDataBtn', true, '▶️ Run with Data');
                }
            } catch (error) {
                showResult('executionResult', `❌ Execution failed: ${error.message}`, 'error');
                setButtonState('executeBtn', true);
                setButtonState('executeDataBtn', true, '▶️ Run with Data');
            }
        }

        async function checkStatus(showMessage = true) {
            if (!currentExecutionId) {
                if (showMessage) showResult('statusResult', 'No execution in progress', 'error');
                return;
            }

            try {
                const response = await fetch(`/status/${currentExecutionId}`);
                const result = await response.json();
                
                if (response.ok) {
                    const statusMessage = `Status: ${result.status}<br>
                         Started: ${result.start_time}<br>
                         ${result.end_time ? `Ended: ${result.end_time}<br>` : ''}
                         ${result.results_file ? '✅ Results ready' : '⏳ Running...'}`;
                    
                    if (showMessage) showResult('statusResult', statusMessage);
                    
                    if (result.status === 'completed') {
                        stopStatusMonitoring();
                        showResult('executionResult', 
                            `✅ Execution completed: ${currentExecutionId}<br>📄 Results available for download`);
                        setButtonState('executeBtn', true, '▶️ Run Workflow');
                        setButtonState('executeDataBtn', true, '▶️ Run with Data');
                        setButtonState('downloadBtn', true);
                    } else if (result.status === 'failed') {
                        stopStatusMonitoring();
                        showResult('executionResult', 
                            `❌ Execution failed: ${currentExecutionId}<br>${result.error || 'Check logs for details'}`, 'error');
                        setButtonState('executeBtn', true, '▶️ Run Workflow');
                        setButtonState('executeDataBtn', true, '▶️ Run with Data');
                    }
                } else {
                    if (showMessage) showResult('statusResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                if (showMessage) showResult('statusResult', `❌ Status check failed: ${error.message}`, 'error');
            }
        }

        async function downloadResults() {
            if (!currentExecutionId) {
                showResult('statusResult', 'No execution to download', 'error');
                return;
            }

            try {
                const response = await fetch(`/download/${currentExecutionId}`);
                if (response.ok) {
                    const blob = await response.blob();
                    const url = window.URL.createObjectURL(blob);
                    const a = document.createElement('a');
                    a.href = url;
                    a.download = `results_${currentExecutionId}.json`;
                    a.click();
                    window.URL.revokeObjectURL(url);
                    showResult('statusResult', '✅ Results downloaded');
                } else {
                    const result = await response.json();
                    showResult('statusResult', `❌ ${result.error}`, 'error');
                }
            } catch (error) {
                showResult('statusResult', `❌ Download failed: ${error.message}`, 'error');
            }
        }

        // Clean up on page unload
        window.addEventListener('beforeunload', () => {
            stopStatusMonitoring();
        });
    </script>
</body>
</html>
    """
    return response.html(html)

@app.route("/template-csv", methods=["GET"])
async def download_template_csv(request):
    """Download template CSV for business users"""
    try:
        template_rows = create_template_csv()
        
        # Create CSV in memory
        output = io.StringIO()
        
        if template_rows:
            fieldnames = template_rows[0].keys()
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(template_rows)
            
            # Add instructions at the bottom
            writer.writerow({})
            writer.writerow({"Step": "INSTRUCTIONS:", "Agent_Name": "How to use this template"})
            writer.writerow({"Step": "1.", "Agent_Name": "Each row is a workflow step"})
            writer.writerow({"Step": "2.", "Agent_Name": "Modify Agent_Name, What_It_Does, Tools_Used as needed"})
            writer.writerow({"Step": "3.", "Agent_Name": "Set Key_Parameters using format: 'Search for: your query | Get 10 results'"})
            writer.writerow({"Step": "4.", "Agent_Name": "Use Waits_For to create dependencies: 'AgentName' or 'Agent1 + Agent2'"})
            writer.writerow({"Step": "5.", "Agent_Name": "Set Enabled to NO to skip steps"})
            writer.writerow({"Step": "6.", "Agent_Name": "Add custom fields as needed"})
            writer.writerow({"Step": "7.", "Agent_Name": "Save as CSV and upload back!"})
        
        csv_content = output.getvalue()
        output.close()
        
        return response.text(csv_content, headers={
            'Content-Type': 'text/csv',
            'Content-Disposition': 'attachment; filename="workflow_template.csv"'
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/upload-business-csv", methods=["POST"])
async def upload_business_csv(request):
    """Upload CSV directly from business users to create workflow"""
    try:
        if 'csv_file' not in request.files:
            return response.json({"error": "No CSV file provided"}, status=400)
        
        file = request.files['csv_file'][0]
        
        # Read and parse CSV
        csv_content = file.body.decode('utf-8')
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        csv_data = list(csv_reader)
        
        if not csv_data:
            return response.json({"error": "CSV file is empty"}, status=400)
        
        # Convert to workflow
        workflow_data = convert_business_csv_to_workflow(csv_data)
        
        if not workflow_data:
            return response.json({"error": "No valid workflow steps found in CSV"}, status=400)
        
        # Save workflow
        workflow_id = str(uuid.uuid4())
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        with open(workflow_path, 'w') as f:
            json.dump(workflow_data, f, indent=2)
        
        # Analyze workflow
        agents = [step.get("agent", "unknown") for step in workflow_data if step.get("agent")]
        
        return response.json({
            "workflow_id": workflow_id,
            "steps": len(workflow_data),
            "agents": list(set(agents)),
            "filename": file.name,
            "message": "Workflow created successfully from business CSV"
        })
        
    except Exception as e:
        return response.json({"error": f"Failed to process CSV: {str(e)}"}, status=500)

@app.route("/upload-workflow", methods=["POST"])
async def upload_workflow(request):
    """Upload JSON workflow"""
    try:
        if 'workflow' not in request.files:
            return response.json({"error": "No workflow file"}, status=400)
        
        file = request.files['workflow'][0]
        workflow_id = str(uuid.uuid4())
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        with open(workflow_path, 'wb') as f:
            f.write(file.body)
        
        # Analyze workflow
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        steps = workflow_data if isinstance(workflow_data, list) else workflow_data.get("steps", [])
        agents = [step.get("agent", "unknown") for step in steps if step.get("agent")]
        
        return response.json({
            "workflow_id": workflow_id,
            "steps": len(steps),
            "agents": list(set(agents))[:10],
            "filename": file.name
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/convert-to-business-csv/<workflow_id>", methods=["GET"])
async def convert_to_business_csv(request, workflow_id):
    """Convert JSON workflow to business-friendly CSV format"""
    try:
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        if not os.path.exists(workflow_path):
            return response.json({"error": "Workflow not found"}, status=404)
        
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        # Convert to business-friendly format
        business_rows, variables = convert_workflow_to_business_csv(workflow_data)
        
        # Create CSV in memory
        output = io.StringIO()
        
        if business_rows:
            fieldnames = business_rows[0].keys()
            writer = csv.DictWriter(output, fieldnames=fieldnames)
            writer.writeheader()
            writer.writerows(business_rows)
            
            # Add instructions at the bottom
            writer.writerow({})
            writer.writerow({"Step": "INSTRUCTIONS:", "Agent_Name": "Edit this CSV in Excel"})
            writer.writerow({"Step": "1.", "Agent_Name": "Modify any values you want to change"})
            writer.writerow({"Step": "2.", "Agent_Name": "Save as CSV format"})
            writer.writerow({"Step": "3.", "Agent_Name": "Upload the edited CSV back"})
            writer.writerow({"Step": "4.", "Agent_Name": "Execute your improved workflow!"})
        
        csv_content = output.getvalue()
        output.close()
        
        return response.text(csv_content, headers={
            'Content-Type': 'text/csv',
            'Content-Disposition': f'attachment; filename="business_workflow_{workflow_id}.csv"'
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/convert-to-technical-csv/<workflow_id>", methods=["GET"])
async def convert_to_technical_csv(request, workflow_id):
    """Convert JSON workflow to technical CSV format"""
    try:
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        if not os.path.exists(workflow_path):
            return response.json({"error": "Workflow not found"}, status=404)
        
        with open(workflow_path, 'r') as f:
            workflow_data = json.load(f)
        
        # Create technical CSV format
        output = io.StringIO()
        writer = csv.writer(output)
        
        # Header
        writer.writerow(["Type", "Agent", "Content", "Tools", "Parameters", "ReadFrom", "OutputFormat", "Custom_Fields"])
        
        # Data rows
        for item in workflow_data:
            if item.get("type") == "state":
                continue
                
            tools = json.dumps(item.get("tools", []))
            parameters = json.dumps(item.get("parameters", {}))
            read_from = json.dumps(item.get("readFrom", []))
            
            # Custom fields
            custom_fields = {}
            for key, value in item.items():
                if key not in ["agent", "content", "tools", "parameters", "readFrom", "output_format"]:
                    custom_fields[key] = value
            
            writer.writerow([
                "step",
                item.get("agent", ""),
                item.get("content", ""),
                tools,
                parameters,
                read_from,
                item.get("output_format", ""),
                json.dumps(custom_fields)
            ])
        
        csv_content = output.getvalue()
        output.close()
        
        return response.text(csv_content, headers={
            'Content-Type': 'text/csv',
            'Content-Disposition': f'attachment; filename="technical_workflow_{workflow_id}.csv"'
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/upload-edited-csv", methods=["POST"])
async def upload_edited_csv(request):
    """Upload edited CSV and convert back to workflow"""
    try:
        if 'csv_file' not in request.files:
            return response.json({"error": "No CSV file provided"}, status=400)
        
        file = request.files['csv_file'][0]
        
        # Read and parse CSV
        csv_content = file.body.decode('utf-8')
        csv_reader = csv.DictReader(io.StringIO(csv_content))
        csv_data = list(csv_reader)
        
        if not csv_data:
            return response.json({"error": "CSV file is empty"}, status=400)
        
        # Convert back to workflow
        workflow_data = convert_business_csv_to_workflow(csv_data)
        
        if not workflow_data:
            return response.json({"error": "No valid workflow steps found in CSV"}, status=400)
        
        # Save new workflow
        workflow_id = str(uuid.uuid4())
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        with open(workflow_path, 'w') as f:
            json.dump(workflow_data, f, indent=2)
        
        # Analyze workflow
        agents = [step.get("agent", "unknown") for step in workflow_data if step.get("agent")]
        
        return response.json({
            "workflow_id": workflow_id,
            "steps": len(workflow_data),
            "agents": list(set(agents)),
            "filename": file.name,
            "message": "CSV successfully converted to workflow"
        })
        
    except Exception as e:
        return response.json({"error": f"Failed to process CSV: {str(e)}"}, status=500)

@app.route("/upload-data", methods=["POST"])
async def upload_data(request):
    """Upload data files"""
    try:
        uploaded_files = []
        
        for key, file_list in request.files.items():
            if key.startswith('data_'):
                file = file_list[0]
                filename = f"{uuid.uuid4()}_{file.name}"
                filepath = os.path.join(UPLOAD_DIR, filename)
                
                with open(filepath, 'wb') as f:
                    f.write(file.body)
                
                uploaded_files.append({
                    "original_name": file.name,
                    "stored_name": filename,
                    "path": filepath,
                    "size": len(file.body)
                })
        
        return response.json({
            "uploaded_files": uploaded_files,
            "count": len(uploaded_files)
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/execute", methods=["POST"])
async def execute_workflow(request):
    """Execute workflow"""
    try:
        data = request.json
        workflow_id = data.get("workflow_id")
        use_data = data.get("use_data", False)
        data_files = data.get("data_files", [])
        
        if not workflow_id:
            return response.json({"error": "No workflow ID provided"}, status=400)
        
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        if not os.path.exists(workflow_path):
            return response.json({"error": "Workflow not found"}, status=404)
        
        # Create execution ID
        execution_id = str(uuid.uuid4())
        
        # Prepare command (this would be your actual workflow execution command)
        command = ["python", "workflow_executor.py", workflow_path]
        
        if use_data and data_files:
            for data_file in data_files:
                command.extend(["--data", data_file["path"]])
        
        # Start process
        process = subprocess.Popen(
            command,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            cwd=os.getcwd()
        )
        
        # Store execution info
        executions[execution_id] = {
            "workflow_id": workflow_id,
            "execution_id": execution_id,
            "status": "running",
            "start_time": datetime.now().isoformat(),
            "command": " ".join(command),
            "process": process,
            "data_files": data_files if use_data else []
        }
        
        # Start monitoring in background
        monitor_thread = threading.Thread(target=monitor_execution, args=(execution_id,))
        monitor_thread.daemon = True
        monitor_thread.start()
        
        return response.json({
            "execution_id": execution_id,
            "status": "started",
            "workflow_id": workflow_id
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/status/<execution_id>", methods=["GET"])
async def get_execution_status(request, execution_id):
    """Get execution status"""
    try:
        if execution_id not in executions:
            return response.json({"error": "Execution not found"}, status=404)
        
        exec_info = executions[execution_id]
        
        # Remove process from response (not JSON serializable)
        status_info = {k: v for k, v in exec_info.items() if k != "process"}
        
        return response.json(status_info)
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/download/<execution_id>", methods=["GET"])
async def download_results(request, execution_id):
    """Download execution results"""
    try:
        if execution_id not in executions:
            return response.json({"error": "Execution not found"}, status=404)
        
        exec_info = executions[execution_id]
        results_file = exec_info.get("results_file")
        
        if not results_file or not os.path.exists(results_file):
            return response.json({"error": "Results file not found"}, status=404)
        
        with open(results_file, 'rb') as f:
            file_content = f.read()
        
        return response.raw(file_content, headers={
            'Content-Type': 'application/json',
            'Content-Disposition': f'attachment; filename="results_{execution_id}.json"'
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

if __name__ == "__main__":
    print("🚀 Starting Business-Friendly Workflow Manager")
    print("📊 Business users can now create workflows entirely through CSV!")
    print("🌐 Web interface available at: http://localhost:8000")
    print("📋 Template CSV available for download")
    print("✨ Full freedom for business users to customize workflows")
    
    app.run(host="0.0.0.0", port=8000, debug=True)


==================================================
FILE: ollama_config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "qwen2.5:7b",
    "api_key": "",  # Ollama doesn't need API key
    "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    "timeout": 1200  # Increase timeout
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: openrouter_config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "deepseek/deepseek-chat:free",
#     "default_model": "openrouter/quasar-alpha",
    "api_key": "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",
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db" 
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: quick_fix_test.py
==================================================

#!/usr/bin/env python3
"""
Quick test of the simple browser adapter (no Playwright needed)
"""

def test_simple_browser():
    """Test the simple browser implementation"""
    print("🚀 Testing Simple Browser (No Playwright)")
    print("=" * 50)
    
    try:
        # Replace the old browser_adapter with simple one
        import os
        import sys
        from pathlib import Path
        
        # Get the framework directory
        framework_dir = Path(__file__).parent
        component_dir = framework_dir / "COMPONENT"
        
        # Backup old browser_adapter if it exists
        old_adapter = component_dir / "browser_adapter.py"
        backup_adapter = component_dir / "browser_adapter_playwright_backup.py"
        
        if old_adapter.exists() and not backup_adapter.exists():
            import shutil
            shutil.copy2(old_adapter, backup_adapter)
            print("✅ Backed up Playwright adapter")
        
        # Write the simple browser adapter
        simple_adapter_code = '''#!/usr/bin/env python3

import os
import sys
import json
import time
import logging
import requests
import re
from typing import Dict, Any, List, Optional
from datetime import datetime
from urllib.parse import urljoin, urlparse

# IMPORTANT: Add tool namespace for tool_manager discovery
TOOL_NAMESPACE = "browser"

# Set up logging
logger = logging.getLogger("browser_adapter")

# Try to import BeautifulSoup
try:
    from bs4 import BeautifulSoup
    BEAUTIFULSOUP_AVAILABLE = True
    logger.info("✅ BeautifulSoup successfully imported")
except ImportError:
    BEAUTIFULSOUP_AVAILABLE = False
    logger.warning("❌ BeautifulSoup not available. Install with 'pip install beautifulsoup4'")

class SimpleBrowserSession:
    """Simple browser session using requests + BeautifulSoup"""
    
    def __init__(self, session_id: str = "default"):
        self.session_id = session_id
        self.session = requests.Session()
        self.current_url = None
        self.current_soup = None
        self.history = []
        
        # Set realistic headers
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        })

class SimpleBrowserManager:
    """Browser manager using simple HTTP requests"""
    
    def __init__(self):
        self.sessions = {}
        self.default_timeout = 30
        
    def create_browser(self, browser_id: str = None, browser_type: str = "simple", 
                      headless: bool = True) -> Dict[str, Any]:
        """Create a new browser session"""
        if browser_id is None:
            browser_id = "default"
        
        try:
            session = SimpleBrowserSession(browser_id)
            self.sessions[browser_id] = session
            
            logger.info(f"✅ Browser created: {browser_id}")
            
            return {
                "status": "success", 
                "browser_id": browser_id, 
                "type": "simple_http",
                "message": f"Simple browser session created: {browser_id}"
            }
            
        except Exception as e:
            logger.error(f"❌ Error creating browser: {str(e)}")
            return {"error": f"Failed to create browser: {str(e)}"}
    
    def navigate(self, url: str, browser_id: str = None, wait_until: str = "load") -> Dict[str, Any]:
        """Navigate to a URL"""
        if browser_id is None:
            browser_id = "default"
            
        # Create browser if it doesn't exist
        if browser_id not in self.sessions:
            result = self.create_browser(browser_id)
            if "error" in result:
                return result
        
        session_obj = self.sessions.get(browser_id)
        if not session_obj:
            return {"error": f"No session found for browser {browser_id}"}
        
        # Fix URL if needed
        if not url.startswith(('http://', 'https://')):
            url = 'https://' + url
        
        try:
            start_time = time.time()
            response = session_obj.session.get(url, timeout=self.default_timeout)
            response.raise_for_status()
            end_time = time.time()
            
            # Store current state
            session_obj.current_url = response.url
            session_obj.history.append(response.url)
            
            # Parse with BeautifulSoup if available
            if BEAUTIFULSOUP_AVAILABLE:
                session_obj.current_soup = BeautifulSoup(response.content, 'html.parser')
                page_title = session_obj.current_soup.title.string.strip() if session_obj.current_soup.title else "No Title"
            else:
                session_obj.current_soup = None
                # Try to extract title from raw HTML
                title_match = re.search(r'<title[^>]*>([^<]+)</title>', response.text, re.IGNORECASE)
                page_title = title_match.group(1).strip() if title_match else "No Title"
            
            logger.info(f"✅ Navigated to: {response.url}")
            
            return {
                "status": "success",
                "url": response.url,
                "title": page_title,
                "status_code": response.status_code,
                "navigation_time_seconds": round(end_time - start_time, 2)
            }
            
        except requests.exceptions.RequestException as e:
            logger.error(f"❌ Navigation error: {str(e)}")
            return {"error": f"Failed to navigate to {url}: {str(e)}"}
        except Exception as e:
            logger.error(f"❌ Unexpected error: {str(e)}")
            return {"error": f"Unexpected error: {str(e)}"}
    
    def get_page_content(self, browser_id: str = None, content_type: str = "text") -> Dict[str, Any]:
        """Get page content"""
        if browser_id is None:
            browser_id = "default"
            
        session_obj = self.sessions.get(browser_id)
        if not session_obj:
            return {"error": f"No session found for browser {browser_id}"}
        
        if not session_obj.current_url:
            return {"error": "No page loaded. Navigate to a URL first."}
        
        try:
            # Get fresh content
            response = session_obj.session.get(session_obj.current_url, timeout=self.default_timeout)
            response.raise_for_status()
            
            if content_type.lower() == "html":
                content = response.text
            else:
                # Extract clean text
                if BEAUTIFULSOUP_AVAILABLE:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    
                    # Remove unwanted elements
                    for element in soup(["script", "style", "nav", "header", "footer", "aside"]):
                        element.decompose()
                    
                    # Try to get main content
                    main_content = (soup.find('main') or 
                                  soup.find('article') or 
                                  soup.find('div', {'id': 'content'}) or 
                                  soup.find('div', {'class': 'content'}) or 
                                  soup.body)
                    
                    if main_content:
                        content = main_content.get_text(separator=' ', strip=True)
                    else:
                        content = soup.get_text(separator=' ', strip=True)
                    
                    # Clean up text
                    content = re.sub(r'\\s+', ' ', content).strip()
                else:
                    # Fallback: try to extract text from HTML
                    content = re.sub(r'<[^>]+>', ' ', response.text)
                    content = re.sub(r'\\s+', ' ', content).strip()
            
            metadata = {
                "url": session_obj.current_url,
                "content_length": len(content),
                "content_type": content_type,
                "timestamp": datetime.now().isoformat()
            }
            
            return {
                "status": "success",
                "content": content,
                "metadata": metadata
            }
            
        except Exception as e:
            logger.error(f"❌ Content extraction error: {str(e)}")
            return {"error": f"Failed to get page content: {str(e)}"}
    
    def close_browser(self, browser_id: str = None) -> Dict[str, Any]:
        """Close browser session"""
        if browser_id is None:
            browser_id = "default"
        
        try:
            if browser_id in self.sessions:
                session_obj = self.sessions[browser_id]
                session_obj.session.close()
                del self.sessions[browser_id]
                
                logger.info(f"✅ Browser closed: {browser_id}")
                return {"status": "success", "message": f"Browser {browser_id} closed"}
            else:
                return {"error": f"Browser {browser_id} not found"}
                
        except Exception as e:
            logger.error(f"❌ Close error: {str(e)}")
            return {"error": f"Failed to close browser: {str(e)}"}

# Global browser manager
BROWSER_MANAGER = SimpleBrowserManager()

# Tool Registry Functions
def browser_create(browser_id: str = None, browser_type: str = "simple", headless: bool = True, **kwargs) -> Dict[str, Any]:
    """Create a new simple browser session"""
    return BROWSER_MANAGER.create_browser(browser_id, browser_type, headless)

def browser_navigate(url: str, browser_id: str = None, wait_until: str = "load", **kwargs) -> Dict[str, Any]:
    """Navigate to a URL"""
    return BROWSER_MANAGER.navigate(url, browser_id, wait_until)

def browser_get_content(browser_id: str = None, content_type: str = "text", **kwargs) -> Dict[str, Any]:
    """Get page content"""
    return BROWSER_MANAGER.get_page_content(browser_id, content_type)

def browser_close(browser_id: str = None, **kwargs) -> Dict[str, Any]:
    """Close browser"""
    return BROWSER_MANAGER.close_browser(browser_id)

def browser_stats(**kwargs) -> Dict[str, Any]:
    """Get browser statistics"""
    return {
        "active_sessions": len(BROWSER_MANAGER.sessions),
        "browser_type": "simple_http",
        "beautifulsoup_available": BEAUTIFULSOUP_AVAILABLE,
        "timestamp": datetime.now().isoformat()
    }

# Register tools
TOOL_REGISTRY = {
    "create": browser_create,
    "navigate": browser_navigate, 
    "get_content": browser_get_content,
    "close": browser_close,
    "stats": browser_stats
}

logger.info(f"✅ Simple browser tools registered: {list(TOOL_REGISTRY.keys())}")
'''
        
        # Write the simple adapter
        with open(old_adapter, 'w', encoding='utf-8') as f:
            f.write(simple_adapter_code)
        
        print("✅ Replaced browser adapter with simple version")
        
        # Now test it
        print("\n🧪 Testing simple browser functionality...")
        
        # Add paths
        sys.path.insert(0, str(component_dir))
        sys.path.insert(0, str(framework_dir))
        
        # Import and test
        from tool_manager import tool_manager
        
        # Rediscover tools to pick up the new adapter
        tool_manager.tools.clear()
        tool_manager.imported_modules.clear()
        tool_count = tool_manager.discover_tools()
        
        print(f"✅ Rediscovered {tool_count} tools")
        
        # Test browser creation
        print("\\n1. Testing browser creation...")
        result = tool_manager.execute_tool("browser:create", browser_id="test")
        if result.get("status") == "success":
            print("✅ Browser creation: SUCCESS")
        else:
            print(f"❌ Browser creation failed: {result.get('error')}")
            return False
        
        # Test navigation
        print("\\n2. Testing navigation...")
        result = tool_manager.execute_tool("browser:navigate", 
                                         url="https://httpbin.org/html", 
                                         browser_id="test")
        if result.get("status") == "success":
            print(f"✅ Navigation: SUCCESS - {result.get('title')}")
        else:
            print(f"❌ Navigation failed: {result.get('error')}")
            return False
        
        # Test content extraction
        print("\\n3. Testing content extraction...")
        result = tool_manager.execute_tool("browser:get_content", browser_id="test")
        if result.get("status") == "success":
            content_length = len(result.get("content", ""))
            print(f"✅ Content extraction: SUCCESS - {content_length} chars")
        else:
            print(f"❌ Content extraction failed: {result.get('error')}")
            return False
        
        # Test cleanup
        print("\\n4. Testing cleanup...")
        result = tool_manager.execute_tool("browser:close", browser_id="test")
        if result.get("status") == "success":
            print("✅ Browser cleanup: SUCCESS")
        else:
            print(f"❌ Cleanup failed: {result.get('error')}")
        
        print("\\n🎉 All simple browser tests PASSED!")
        print("\\n💡 Your framework now has working browser capabilities!")
        return True
        
    except Exception as e:
        print(f"❌ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_combined_functionality():
    """Test research + simple browser together"""
    print("\\n🔄 Testing Combined Research + Browser...")
    
    try:
        from tool_manager import tool_manager
        
        # 1. Research with simple search
        print("1. Performing research...")
        research_result = tool_manager.execute_tool(
            "research:search", 
            query="python requests tutorial", 
            num_results=2
        )
        
        if research_result.get("status") != "success":
            print(f"❌ Research failed: {research_result.get('error')}")
            return False
        
        print(f"✅ Found {research_result.get('num_results')} results")
        
        # 2. Get a URL from research and browse it
        if research_result.get("results"):
            first_url = research_result["results"][0].get("link")
            if first_url:
                print(f"2. Browsing first result: {first_url[:50]}...")
                
                # Create browser and navigate
                tool_manager.execute_tool("browser:create", browser_id="research")
                nav_result = tool_manager.execute_tool("browser:navigate", 
                                                     url=first_url, 
                                                     browser_id="research")
                
                if nav_result.get("status") == "success":
                    print(f"✅ Browsed to: {nav_result.get('title')}")
                    
                    # Get content
                    content_result = tool_manager.execute_tool("browser:get_content", 
                                                             browser_id="research")
                    
                    if content_result.get("status") == "success":
                        content_length = len(content_result.get("content", ""))
                        print(f"✅ Extracted {content_length} chars of content")
                        
                        # Cleanup
                        tool_manager.execute_tool("browser:close", browser_id="research")
                        
                        print("\\n🚀 COMBINED FUNCTIONALITY WORKING!")
                        print("Your framework can now:")
                        print("  ✅ Search the web")
                        print("  ✅ Fetch web content") 
                        print("  ✅ Browse websites")
                        print("  ✅ Extract page content")
                        print("  ✅ Work together seamlessly")
                        
                        return True
                    else:
                        print(f"❌ Content extraction failed: {content_result.get('error')}")
                else:
                    print(f"❌ Navigation failed: {nav_result.get('error')}")
        
        return False
        
    except Exception as e:
        print(f"❌ Combined test failed: {e}")
        return False

if __name__ == "__main__":
    print("🚀 QUICK FIX: Replacing Playwright with Simple Browser")
    print("=" * 60)
    
    success = test_simple_browser()
    
    if success:
        print("\\n" + "=" * 60)
        combined_success = test_combined_functionality()
        
        if combined_success:
            print("\\n🏆 FRAMEWORK FULLY FUNCTIONAL!")
            print("\\nQuick usage:")
            print("```python")
            print("from tool_manager import tool_manager")
            print("# Search")
            print("result = tool_manager.execute_tool('research:search', query='AI news')")
            print("# Browse") 
            print("tool_manager.execute_tool('browser:create')")
            print("tool_manager.execute_tool('browser:navigate', url='https://example.com')")
            print("content = tool_manager.execute_tool('browser:get_content')")
            print("```")
        else:
            print("\\n⚠️ Simple browser works, but combined functionality needs work")
    else:
        print("\\n❌ Simple browser replacement failed")
        print("\\nFallback: Your research tools still work!")
        print("Just use research:search and research:fetch_content")


==================================================
FILE: solution_example.py
==================================================

#!/usr/bin/env python3
"""
Direct Tool Usage - Simple Python Solution
No workflows, just import tools and solve problems directly!
"""

import sys
import os
from pathlib import Path

# Add COMPONENT directory to path
component_dir = Path(__file__).parent / "COMPONENT"
if component_dir.exists():
    sys.path.insert(0, str(component_dir))

# Import all tools directly
try:
    # Browser tools
    from browser_adapter import (
        browser_create, browser_navigate, browser_get_content,
        browser_find_elements, browser_click, browser_close
    )
    
    # Research tools  
    from research_adapter import (
        research_combined_search, research_analyze_content,
        research_generate_summary, research_fetch_content
    )
    
    # Citation tools
    from cite_adapter import cite_sources, cite_source
    
    # Vector DB tools
    from vector_db_adapter import (
        vector_db_search, vector_db_add, vector_db_batch_add
    )
    
    # ML tools
    from ml_adapter import (
        ml_train_model, ml_predict, ml_evaluate_model
    )
    
    # Planning tools
    from planning_adapter import (
        planning_create_plan, planning_chain_of_thought
    )
    
    # Memory tools
    from memory_adapter import (
        memory_create_system, memory_store_operation
    )
    
    print("✅ All tools imported successfully!")
    
except ImportError as e:
    print(f"⚠️ Some tools not available: {e}")

# SOLUTION 1: Options Trading Research
def research_options_trading_platforms():
    """Direct solution - research options trading platforms"""
    
    print("🔍 Researching options trading platforms...")
    
    # Step 1: Research with multiple queries
    queries = [
        "best options trading platforms 2024",
        "algorithmic options trading software",
        "options trading education platforms"
    ]
    
    all_results = []
    for query in queries:
        print(f"Searching: {query}")
        result = research_combined_search(query=query, num_results=5)
        if "error" not in result:
            all_results.extend(result.get("search_results", []))
    
    # Step 2: Analyze the content
    combined_content = " ".join([r.get("content", "") for r in all_results])
    analysis = research_analyze_content(content=combined_content, max_length=3000)
    
    # Step 3: Generate summary
    summary = research_generate_summary(
        results={"content": combined_content},
        query="options trading platforms comparison"
    )
    
    # Step 4: Create citations
    sources = [{"url": r.get("url"), "title": r.get("title")} for r in all_results if r.get("url")]
    citations = cite_sources(sources=sources, style="apa")
    
    return {
        "platforms_found": len(all_results),
        "analysis": analysis,
        "summary": summary,
        "citations": citations,
        "raw_results": all_results
    }

# SOLUTION 2: Automated Web Data Collection
def collect_financial_data_with_browser():
    """Direct solution - collect financial data using browser automation"""
    
    print("🌐 Collecting financial data with browser...")
    
    # Step 1: Create browser
    browser_result = browser_create(browser_id="finance", headless=True)
    if "error" in browser_result:
        return {"error": "Failed to create browser"}
    
    financial_data = []
    
    # Step 2: Visit multiple financial sites
    sites = [
        "https://finance.yahoo.com/quote/SPY/options",
        "https://www.cboe.com/tradeable_products/sp_500/spx_options/",
        "https://www.nasdaq.com/market-activity/options"
    ]
    
    for site in sites:
        print(f"Visiting: {site}")
        
        # Navigate
        nav = browser_navigate(url=site, browser_id="finance")
        if "error" in nav:
            continue
            
        # Get content
        content = browser_get_content(browser_id="finance", content_type="text")
        if "error" not in content:
            # Analyze the content
            analysis = research_analyze_content(content=content["content"][:2000])
            
            financial_data.append({
                "site": site,
                "title": nav.get("title", ""),
                "content_length": len(content["content"]),
                "analysis": analysis
            })
    
    # Step 3: Cleanup
    browser_close(browser_id="finance")
    
    return {
        "sites_visited": len(financial_data),
        "data_collected": financial_data,
        "total_content": sum(d["content_length"] for d in financial_data)
    }

# SOLUTION 3: ML-Powered Content Analysis
def analyze_content_with_ml():
    """Direct solution - use ML to analyze collected content"""
    
    print("🤖 Analyzing content with machine learning...")
    
    # Step 1: Collect research data
    research = research_combined_search(
        query="machine learning trading strategies", 
        num_results=10
    )
    
    if "error" in research:
        return {"error": "Research failed"}
    
    # Step 2: Prepare data for ML
    texts = []
    labels = []
    
    for result in research["search_results"]:
        content = result.get("content", "")
        if content:
            texts.append(content[:500])  # Truncate for demo
            # Simple labeling based on keywords
            if any(word in content.lower() for word in ["education", "learning", "course"]):
                labels.append("educational")
            elif any(word in content.lower() for word in ["algorithm", "quant", "technical"]):
                labels.append("technical")
            else:
                labels.append("general")
    
    # Step 3: Store in vector database for semantic search
    if texts:
        vector_result = vector_db_batch_add(
            collection="ml_analysis",
            texts=texts,
            metadatas=[{"label": label} for label in labels]
        )
        
        # Step 4: Semantic search
        if "error" not in vector_result:
            search_result = vector_db_search(
                collection="ml_analysis",
                query="educational trading strategies",
                top_k=5
            )
            
            return {
                "texts_processed": len(texts),
                "vector_storage": vector_result,
                "semantic_search": search_result,
                "label_distribution": {label: labels.count(label) for label in set(labels)}
            }
    
    return {"error": "No content to process"}

# SOLUTION 4: Complete Intelligence Pipeline
def complete_intelligence_pipeline(topic="options trading education"):
    """Complete solution combining all tools"""
    
    print(f"🎯 Running complete intelligence pipeline for: {topic}")
    
    results = {}
    
    # Phase 1: Research
    print("Phase 1: Research")
    research = research_combined_search(query=topic, num_results=15)
    results["research"] = research
    
    # Phase 2: Web collection
    print("Phase 2: Web data collection")
    if "error" not in research:
        # Get top URLs
        urls = [r["url"] for r in research["search_results"][:3] if r.get("url")]
        
        # Create browser and visit sites
        browser_create(browser_id="pipeline")
        web_data = []
        
        for url in urls:
            nav = browser_navigate(url=url, browser_id="pipeline")
            if "error" not in nav:
                content = browser_get_content(browser_id="pipeline")
                if "error" not in content:
                    web_data.append({
                        "url": url,
                        "title": nav["title"],
                        "content": content["content"][:1000]
                    })
        
        browser_close(browser_id="pipeline")
        results["web_collection"] = web_data
    
    # Phase 3: Analysis
    print("Phase 3: Content analysis")
    all_content = ""
    if "web_collection" in results:
        all_content = " ".join([d["content"] for d in results["web_collection"]])
    
    if all_content:
        analysis = research_analyze_content(content=all_content)
        summary = research_generate_summary(
            results={"content": all_content},
            query=topic
        )
        results["analysis"] = analysis
        results["summary"] = summary
    
    # Phase 4: Knowledge storage
    print("Phase 4: Knowledge storage")
    if all_content:
        vector_db_add(
            collection="intelligence_pipeline",
            text=all_content,
            metadata={"topic": topic, "timestamp": str(Path(__file__).stat().st_mtime)}
        )
    
    # Phase 5: Planning
    print("Phase 5: Strategic planning")
    plan = planning_create_plan(
        name=f"{topic}_strategy",
        goal=f"Develop comprehensive understanding of {topic}",
        subtasks=[
            "Market analysis",
            "Technology assessment", 
            "Educational framework design",
            "Implementation roadmap"
        ]
    )
    results["strategic_plan"] = plan
    
    return results

# Main execution
if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="Direct Tool Usage Solutions")
    parser.add_argument("--solution", 
                       choices=["research", "browser", "ml", "complete"],
                       default="complete",
                       help="Which solution to run")
    parser.add_argument("--topic", default="options trading education",
                       help="Topic for research (for complete solution)")
    
    args = parser.parse_args()
    
    print("🚀 Running direct tool solution...")
    
    if args.solution == "research":
        result = research_options_trading_platforms()
    elif args.solution == "browser":
        result = collect_financial_data_with_browser()
    elif args.solution == "ml":
        result = analyze_content_with_ml()
    else:
        result = complete_intelligence_pipeline(args.topic)
    
    # Save results
    import json
    output_file = f"{args.solution}_solution_results.json"
    with open(output_file, 'w') as f:
        json.dump(result, f, indent=2, default=str)
    
    print(f"✅ Solution completed!")
    print(f"📁 Results saved to: {output_file}")
    
    # Print summary
    if isinstance(result, dict) and "error" not in result:
        print(f"📊 Success! Processed data and generated insights.")
    else:
        print(f"❌ Some issues occurred: {result}")


==================================================
FILE: synch_crypto_workflow.py
==================================================

#!/usr/bin/env python3
"""
Simple Cryptocurrency Analysis using Direct Tools
No cognitive workflow dependencies - pure tool usage
"""

from auto_import_tools import *
import json
import time

def analyze_crypto_simple():
    """Simple crypto analysis using available tools directly"""
    
    print("🚀 Starting Simple Cryptocurrency Analysis...")
    
    results = {
        "analysis_type": "cryptocurrency_market_research",
        "timestamp": time.time(),
        "steps": []
    }
    
    # Step 1: Research cryptocurrency trends
    print("\n📊 Step 1: Researching cryptocurrency market trends...")
    try:
        crypto_research = research_combined_search(
            query="cryptocurrency market trends 2024 analysis", 
            num_results=8
        )
        
        if "error" not in crypto_research:
            search_results = crypto_research.get("search_results", [])
            print(f"✅ Found {len(search_results)} research sources")
            
            results["steps"].append({
                "step": "market_research",
                "status": "success", 
                "sources_found": len(search_results),
                "data": crypto_research
            })
            
            # Show some results
            for i, result in enumerate(search_results[:3]):
                print(f"   📰 {i+1}. {result.get('title', 'No title')[:80]}...")
        else:
            print(f"❌ Research failed: {crypto_research['error']}")
            results["steps"].append({
                "step": "market_research",
                "status": "error",
                "error": crypto_research["error"]
            })
            
    except Exception as e:
        print(f"❌ Research error: {e}")
        results["steps"].append({
            "step": "market_research", 
            "status": "error",
            "error": str(e)
        })
    
    # Step 2: Analyze specific cryptocurrencies
    print("\n🪙 Step 2: Analyzing major cryptocurrencies...")
    crypto_symbols = ["Bitcoin", "Ethereum", "Solana", "Cardano"]
    
    for crypto in crypto_symbols:
        try:
            crypto_analysis = research_combined_search(
                query=f"{crypto} price analysis investment 2024",
                num_results=3
            )
            
            if "error" not in crypto_analysis:
                sources = len(crypto_analysis.get("search_results", []))
                print(f"   ✅ {crypto}: {sources} sources analyzed")
                
                results["steps"].append({
                    "step": f"analyze_{crypto.lower()}",
                    "status": "success",
                    "crypto": crypto,
                    "sources": sources,
                    "data": crypto_analysis
                })
            else:
                print(f"   ❌ {crypto}: analysis failed")
                results["steps"].append({
                    "step": f"analyze_{crypto.lower()}",
                    "status": "error", 
                    "crypto": crypto,
                    "error": crypto_analysis["error"]
                })
                
        except Exception as e:
            print(f"   ❌ {crypto}: error - {e}")
            results["steps"].append({
                "step": f"analyze_{crypto.lower()}",
                "status": "error",
                "crypto": crypto, 
                "error": str(e)
            })
    
    # Step 3: Analyze trading strategies
    print("\n📈 Step 3: Researching trading strategies...")
    try:
        strategy_research = research_combined_search(
            query="cryptocurrency trading strategies risk management 2024",
            num_results=5
        )
        
        if "error" not in strategy_research:
            sources = len(strategy_research.get("search_results", []))
            print(f"✅ Trading strategies: {sources} sources found")
            
            results["steps"].append({
                "step": "trading_strategies",
                "status": "success",
                "sources": sources, 
                "data": strategy_research
            })
        else:
            print(f"❌ Strategy research failed: {strategy_research['error']}")
            results["steps"].append({
                "step": "trading_strategies",
                "status": "error",
                "error": strategy_research["error"]
            })
            
    except Exception as e:
        print(f"❌ Strategy research error: {e}")
        results["steps"].append({
            "step": "trading_strategies",
            "status": "error", 
            "error": str(e)
        })
    
    # Step 4: Use ML tools for analysis (if available)
    print("\n🤖 Step 4: ML-based analysis...")
    try:
        # Check if we have data to analyze
        successful_steps = [s for s in results["steps"] if s["status"] == "success"]
        
        if len(successful_steps) >= 2:
            print("✅ Sufficient data collected for ML analysis")
            
            # Try to use ML tools
            try:
                # Get available ML algorithms
                ml_algorithms = ml_get_available_algorithms()
                print(f"   🔧 Available ML algorithms: {len(ml_algorithms.get('algorithms', []))}")
                
                results["steps"].append({
                    "step": "ml_analysis_prep",
                    "status": "success",
                    "available_algorithms": ml_algorithms
                })
                
            except Exception as ml_e:
                print(f"   ⚠️ ML tools not fully available: {ml_e}")
                results["steps"].append({
                    "step": "ml_analysis_prep",
                    "status": "partial",
                    "note": "ML tools available but data preparation needed"
                })
        else:
            print("⚠️ Insufficient data for ML analysis")
            results["steps"].append({
                "step": "ml_analysis_prep",
                "status": "skipped",
                "reason": "insufficient_data"
            })
            
    except Exception as e:
        print(f"❌ ML analysis error: {e}")
        results["steps"].append({
            "step": "ml_analysis_prep",
            "status": "error",
            "error": str(e)
        })
    
    # Step 5: Generate summary analysis
    print("\n📋 Step 5: Generating analysis summary...")
    try:
        # Collect all successful research data
        all_sources = []
        total_sources = 0
        
        for step in results["steps"]:
            if step["status"] == "success" and "data" in step:
                search_results = step["data"].get("search_results", [])
                all_sources.extend(search_results)
                total_sources += len(search_results)
        
        # Generate summary using research tools
        if all_sources:
            combined_content = " ".join([
                result.get("content", result.get("snippet", ""))
                for result in all_sources[:10]  # Limit to first 10 sources
            ])
            
            if combined_content:
                summary_analysis = research_analyze_content(
                    content=combined_content,
                    max_length=2000
                )
                
                if "error" not in summary_analysis:
                    print("✅ Summary analysis generated")
                    results["steps"].append({
                        "step": "summary_analysis",
                        "status": "success",
                        "total_sources_analyzed": total_sources,
                        "analysis": summary_analysis
                    })
                else:
                    print(f"❌ Summary analysis failed: {summary_analysis['error']}")
            else:
                print("⚠️ No content available for summary")
        else:
            print("⚠️ No sources available for summary")
            
    except Exception as e:
        print(f"❌ Summary generation error: {e}")
        results["steps"].append({
            "step": "summary_analysis",
            "status": "error",
            "error": str(e)
        })
    
    # Calculate success metrics
    successful_steps = [s for s in results["steps"] if s["status"] == "success"]
    total_steps = len(results["steps"])
    success_rate = (len(successful_steps) / total_steps * 100) if total_steps > 0 else 0
    
    results["summary"] = {
        "total_steps": total_steps,
        "successful_steps": len(successful_steps),
        "success_rate": success_rate,
        "analysis_completed": success_rate >= 60  # 60% success threshold
    }
    
    return results

def create_simple_recommendations(analysis_results):
    """Create simple investment recommendations based on analysis"""
    
    print("\n💡 Generating Investment Recommendations...")
    
    successful_steps = [s for s in analysis_results["steps"] if s["status"] == "success"]
    
    recommendations = {
        "overall_sentiment": "neutral",
        "risk_level": "medium",
        "recommended_actions": [],
        "risk_factors": [],
        "opportunities": []
    }
    
    # Analyze based on successful research
    if len(successful_steps) >= 3:
        recommendations["overall_sentiment"] = "positive"
        recommendations["recommended_actions"].append("Conduct deeper fundamental analysis")
        recommendations["opportunities"].append("Market research indicates active trading opportunities")
        
        # Check if we analyzed multiple cryptocurrencies
        crypto_analyses = [s for s in successful_steps if "crypto" in s.get("step", "")]
        if len(crypto_analyses) >= 2:
            recommendations["recommended_actions"].append("Diversify across analyzed cryptocurrencies")
            recommendations["opportunities"].append("Multiple cryptocurrencies show research activity")
    
    elif len(successful_steps) >= 1:
        recommendations["overall_sentiment"] = "cautious"
        recommendations["risk_level"] = "high"
        recommendations["recommended_actions"].append("Gather more market data before investing")
        recommendations["risk_factors"].append("Limited research data available")
    
    else:
        recommendations["overall_sentiment"] = "bearish"
        recommendations["risk_level"] = "very high"
        recommendations["recommended_actions"].append("Avoid investment until better data available")
        recommendations["risk_factors"].append("Insufficient research data for informed decision")
    
    # Add general recommendations
    recommendations["recommended_actions"].extend([
        "Monitor market volatility closely",
        "Set clear stop-loss limits",
        "Only invest what you can afford to lose"
    ])
    
    recommendations["risk_factors"].extend([
        "Cryptocurrency markets are highly volatile",
        "Regulatory changes can impact prices significantly",
        "Market sentiment can change rapidly"
    ])
    
    return recommendations

def main():
    """Main execution function"""
    print("🚀 Simple Cryptocurrency Analysis Tool")
    print("=" * 60)
    
    try:
        # Run the analysis
        analysis_results = analyze_crypto_simple()
        
        # Generate recommendations
        recommendations = create_simple_recommendations(analysis_results)
        
        # Combine results
        final_results = {
            "analysis": analysis_results,
            "recommendations": recommendations,
            "generated_at": time.time()
        }
        
        # Display summary
        print(f"\n📊 Analysis Complete!")
        summary = analysis_results["summary"]
        print(f"   ✅ Success Rate: {summary['success_rate']:.1f}%")
        print(f"   📈 Steps Completed: {summary['successful_steps']}/{summary['total_steps']}")
        print(f"   🎯 Analysis Status: {'✅ Complete' if summary['analysis_completed'] else '⚠️ Partial'}")
        
        print(f"\n💡 Investment Recommendations:")
        print(f"   📊 Sentiment: {recommendations['overall_sentiment'].upper()}")
        print(f"   ⚠️ Risk Level: {recommendations['risk_level'].upper()}")
        print(f"   📋 Actions: {len(recommendations['recommended_actions'])} recommendations")
        
        # Save results
        output_file = "simple_crypto_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(final_results, f, indent=2, default=str)
        
        print(f"\n📁 Complete results saved to: {output_file}")
        
        # Display top recommendations
        print(f"\n🎯 Top Recommendations:")
        for i, action in enumerate(recommendations['recommended_actions'][:3], 1):
            print(f"   {i}. {action}")
        
        return final_results
        
    except Exception as e:
        print(f"\n💥 Fatal error: {e}")
        import traceback
        traceback.print_exc()
        return None

if __name__ == "__main__":
    main()


==================================================
FILE: tool_manager.py
==================================================

#!/usr/bin/env python3

import os
import sys
import json
import inspect
import importlib.util
import glob
from typing import Dict, Any, List, Optional, Callable, Set
from functools import wraps
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("tool_manager")

class ToolManager:
    """Enhanced tool manager with automatic function discovery"""
    
    def __init__(self):
        self.tools = {}  # Tool registry
        self.imported_modules = set()  # Track imported modules
        self.namespace_prefixes = {}  # Map module to namespace prefix
        self.excluded_files = {
            'agent_runner.py', 'tool_manager.py', 'utils.py', 'config.py', 'call_api.py',
            '__init__.py', 'cli.py', 'async_executor.py', 'async_framework_main.py',
            'workflow_executor.py', 'workflow_state.py', 'enhanced_agent_runner.py'
        }
        self.excluded_functions = {
            'main', '__init__', 'setup', 'teardown', 'test_', 'debug_'
        }
        
        # Priority loading patterns - these get loaded first
        self.priority_patterns = [
            '*_adapter.py',  # All adapter files
            '*_manager.py',  # All manager files
            'memory_*.py',   # Memory-related files
            'cognitive_*.py' # Cognitive-related files
        ]
    
    def discover_tools(self, directories: List[str] = None) -> int:
        """Automatically discover all Python modules and their functions in given directories"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        if directories is None:
            directories = [current_dir]
            
            # Add COMPONENT directory if it exists
            component_dir = os.path.join(current_dir, "COMPONENT")
            if os.path.exists(component_dir):
                directories.append(component_dir)
                logger.info(f"Added COMPONENT directory: {component_dir}")
            
            # Also include any other directories within current_dir
            for item in os.listdir(current_dir):
                item_path = os.path.join(current_dir, item)
                if (os.path.isdir(item_path) and 
                    not item.startswith('.') and 
                    item not in ['__pycache__', 'COMPONENT', 'async_outputs', 'agent_outputs']):
                    directories.append(item_path)
        
        total_tools = 0
        logger.info(f"Scanning directories: {directories}")
        
        # **FIXED: Load priority files first**
        all_python_files = []
        priority_files = []
        
        # Find all Python files and categorize them
        for directory in directories:
            if not os.path.exists(directory):
                logger.warning(f"Directory does not exist: {directory}")
                continue
                
            python_files = glob.glob(os.path.join(directory, "*.py"))
            
            for py_file in python_files:
                filename = os.path.basename(py_file)
                
                # Skip excluded files
                if filename in self.excluded_files:
                    logger.debug(f"Skipping excluded file: {filename}")
                    continue
                
                # Check if it's a priority file
                is_priority = False
                for pattern in self.priority_patterns:
                    if self._matches_pattern(filename, pattern):
                        priority_files.append(py_file)
                        is_priority = True
                        break
                
                if not is_priority:
                    all_python_files.append(py_file)
        
        logger.info(f"Found {len(priority_files)} priority files and {len(all_python_files)} regular files")
        
        # Load priority files first
        for py_file in priority_files:
            total_tools += self._load_single_file(py_file)
        
        # Then load regular files
        for py_file in all_python_files:
            total_tools += self._load_single_file(py_file)
        
        logger.info(f"🔧 Total tools discovered: {total_tools}")
        return total_tools
    
    def _matches_pattern(self, filename: str, pattern: str) -> bool:
        """Check if filename matches a glob pattern"""
        import fnmatch
        return fnmatch.fnmatch(filename, pattern)
    
    def _load_single_file(self, py_file: str) -> int:
        """Load a single Python file and register its tools"""
        filename = os.path.basename(py_file)
        module_name = filename[:-3]  # Remove .py
        
        # Skip if already imported
        if module_name in self.imported_modules:
            logger.debug(f"Module already imported: {module_name}")
            return 0
        
        tools_count = 0
        
        try:
            # **FIXED: More robust module loading with better error handling**
            module = self._load_module_safely(py_file, module_name)
            if module is None:
                return 0
            
            self.imported_modules.add(module_name)
            
            # Determine namespace prefix
            if hasattr(module, 'TOOL_NAMESPACE'):
                prefix = getattr(module, 'TOOL_NAMESPACE')
            else:
                # By default, use the module name
                prefix = module_name
                
                # Special case: if it ends with _adapter, remove that part
                if prefix.endswith("_adapter"):
                    prefix = prefix[:-8]  # Remove "_adapter"
            
            self.namespace_prefixes[module_name] = prefix
            
            # **PRIORITY 1: Register TOOL_REGISTRY tools first**
            registry_tools = 0
            if hasattr(module, 'TOOL_REGISTRY'):
                tool_registry = getattr(module, 'TOOL_REGISTRY')
                if isinstance(tool_registry, dict):
                    for tool_id, tool_handler in tool_registry.items():
                        # Handle full tool IDs properly
                        if ':' in tool_id:
                            full_tool_id = tool_id  # Already has namespace
                        else:
                            full_tool_id = f"{prefix}:{tool_id}"
                        
                        # **FIXED: Validate tool handler before registration**
                        if callable(tool_handler):
                            self.register_tool(full_tool_id, tool_handler)
                            registry_tools += 1
                            tools_count += 1
                        else:
                            logger.warning(f"Tool handler for {tool_id} is not callable")
                    
                    logger.info(f"✅ Registered {registry_tools} tools from TOOL_REGISTRY in {module_name}")
            
            # **PRIORITY 2: Auto-discover functions that should be registered as tools**
            auto_tools = self._discover_module_tools(module, prefix)
            tools_count += len(auto_tools)
            
            logger.info(f"✅ Imported module {module_name} with {registry_tools} registry tools + {len(auto_tools)} auto-discovered functions")
            
        except Exception as e:
            logger.error(f"❌ Error importing {module_name} from {py_file}: {e}")
            import traceback
            logger.debug(f"Full traceback: {traceback.format_exc()}")
        
        return tools_count
    
    def _load_module_safely(self, py_file: str, module_name: str):
        """Safely load a module with proper error handling"""
        try:
            # **FIXED: Add the module directory to sys.path BEFORE creating spec**
            module_dir = os.path.dirname(py_file)
            path_added = False
            if module_dir not in sys.path:
                sys.path.insert(0, module_dir)
                path_added = True
            
            try:
                # Create spec
                spec = importlib.util.spec_from_file_location(module_name, py_file)
                if spec is None or spec.loader is None:
                    logger.warning(f"Could not create spec for {py_file}")
                    return None
                
                # Create module
                module = importlib.util.module_from_spec(spec)
                
                # **FIXED: Add module to sys.modules before execution**
                sys.modules[module_name] = module
                
                # Execute module
                spec.loader.exec_module(module)
                logger.debug(f"Successfully loaded module: {module_name}")
                return module
                
            except Exception as e:
                logger.error(f"Error executing module {module_name}: {e}")
                # Remove from sys.modules if execution failed
                if module_name in sys.modules:
                    del sys.modules[module_name]
                return None
                
            finally:
                # Remove from sys.path if we added it
                if path_added and module_dir in sys.path:
                    sys.path.remove(module_dir)
                    
        except Exception as e:
            logger.error(f"Error loading module {module_name}: {e}")
            return None
    
    def _discover_module_tools(self, module, prefix: str) -> List[str]:
        """Discover and register tools from a module"""
        discovered_tools = []
        
        # Get all functions from the module
        for name, obj in inspect.getmembers(module):
            # Skip private functions, special methods, and non-functions
            if name.startswith('_') or not inspect.isfunction(obj):
                continue
            
            # Skip functions that start with excluded prefixes
            if any(name.startswith(excluded) for excluded in self.excluded_functions):
                continue
                
            # Skip functions that are already in TOOL_REGISTRY (avoid duplicates)
            if hasattr(module, 'TOOL_REGISTRY'):
                tool_registry = getattr(module, 'TOOL_REGISTRY')
                if isinstance(tool_registry, dict) and any(handler == obj for handler in tool_registry.values()):
                    continue
            
            # Check if function has a docstring (we only want documented functions)
            if obj.__doc__ and obj.__doc__.strip():
                # Create a tool ID based on the prefix and function name
                tool_id = f"{prefix}:{name}"
                self.register_tool(tool_id, obj)
                discovered_tools.append(tool_id)
                logger.debug(f"Auto-registered tool: {tool_id}")
        
        return discovered_tools
    
    def register_tool(self, tool_id: str, handler: Callable) -> None:
        """Register a function as a tool with flexible parameter handling"""
        @wraps(handler)
        def flexible_handler(**kwargs):
            try:
                # Get function signature
                sig = inspect.signature(handler)
                
                # If function takes **kwargs, pass everything
                if any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values()):
                    return handler(**kwargs)
                
                # Otherwise, filter kwargs to match function signature
                filtered_kwargs = {}
                for param_name, param in sig.parameters.items():
                    if param_name in kwargs:
                        filtered_kwargs[param_name] = kwargs[param_name]
                    elif param.default == param.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
                        # Required parameter missing
                        logger.warning(f"Required parameter '{param_name}' missing for tool {tool_id}")
                        return {"error": f"Required parameter '{param_name}' missing"}
                
                return handler(**filtered_kwargs)
                
            except Exception as e:
                logger.error(f"Tool execution failed for {tool_id}: {str(e)}")
                return {"error": f"Tool execution failed: {str(e)}"}
        
        self.tools[tool_id] = flexible_handler
        logger.debug(f"Registered tool: {tool_id}")
    
    def execute_tool(self, tool_id: str, **kwargs) -> Any:
        """Execute a registered tool"""
        if tool_id not in self.tools:
            # Try to find it by prefix and auto-load
            if ':' in tool_id:
                prefix = tool_id.split(':', 1)[0]
                
                # Look for modules that might contain this tool
                for module_name, module_prefix in self.namespace_prefixes.items():
                    if module_prefix == prefix and module_name not in self.imported_modules:
                        # Try to import module
                        self._try_load_module(module_name, prefix)
                        break
        
        if tool_id not in self.tools:
            available_tools = self.get_available_tools_by_prefix(tool_id.split(':', 1)[0] if ':' in tool_id else '')
            error_msg = f"Unknown tool: {tool_id}"
            if available_tools:
                error_msg += f". Available tools with similar prefix: {', '.join(available_tools[:5])}"
            return {"error": error_msg}
        
        try:
            logger.info(f"Executing tool {tool_id} with params: {list(kwargs.keys())}")
            result = self.tools[tool_id](**kwargs)
            logger.debug(f"Tool {tool_id} executed successfully")
            return result
        except Exception as e:
            logger.error(f"Error executing tool {tool_id}: {str(e)}")
            return {"error": str(e)}
    
    def _try_load_module(self, module_name: str, prefix: str):
        """Try to load a module that might contain tools"""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # Check multiple possible locations
            possible_paths = [
                os.path.join(current_dir, f"{module_name}.py"),
                os.path.join(current_dir, "COMPONENT", f"{module_name}.py"),
                os.path.join(current_dir, "tools", f"{module_name}.py"),
                os.path.join(current_dir, "adapters", f"{module_name}.py"),
            ]
            
            for module_path in possible_paths:
                if os.path.exists(module_path):
                    self._load_single_file(module_path)
                    logger.info(f"Dynamically loaded module: {module_name}")
                    break
                        
        except Exception as e:
            logger.warning(f"Failed to dynamically load module {module_name}: {e}")
    
    def get_all_tools(self) -> List[str]:
        """Get a list of all registered tool IDs"""
        return sorted(list(self.tools.keys()))
    
    def get_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get tools by namespace prefix"""
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(f"{prefix}:")]
    
    def get_available_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get available tools that match a prefix"""
        if not prefix:
            return []
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(prefix)]
    
    def is_tool_available(self, tool_id: str) -> bool:
        """Check if a tool is available"""
        return tool_id in self.tools
    
    def get_tool_info(self, tool_id: str) -> Dict[str, Any]:
        """Get information about a specific tool"""
        if tool_id not in self.tools:
            return {"error": f"Tool not found: {tool_id}"}
        
        try:
            # Get the original function from the wrapper
            original_func = self.tools[tool_id].__wrapped__ if hasattr(self.tools[tool_id], '__wrapped__') else self.tools[tool_id]
            
            # Get function signature and docstring
            sig = inspect.signature(original_func)
            
            return {
                "tool_id": tool_id,
                "name": original_func.__name__,
                "module": original_func.__module__ if hasattr(original_func, '__module__') else 'unknown',
                "docstring": original_func.__doc__ or "No documentation available",
                "parameters": {
                    name: {
                        "type": str(param.annotation) if param.annotation != param.empty else "Any",
                        "default": str(param.default) if param.default != param.empty else "Required",
                        "kind": str(param.kind)
                    }
                    for name, param in sig.parameters.items()
                }
            }
        except Exception as e:
            return {"error": f"Could not get tool info: {str(e)}"}
    
    def list_tools_by_module(self) -> Dict[str, List[str]]:
        """List tools organized by module/namespace"""
        tools_by_module = {}
        for tool_id in self.tools.keys():
            if ':' in tool_id:
                prefix, _ = tool_id.split(':', 1)
                if prefix not in tools_by_module:
                    tools_by_module[prefix] = []
                tools_by_module[prefix].append(tool_id)
            else:
                if 'global' not in tools_by_module:
                    tools_by_module['global'] = []
                tools_by_module['global'].append(tool_id)
        
        return tools_by_module
    
    def get_stats(self) -> Dict[str, Any]:
        """Get statistics about the tool manager"""
        tools_by_module = self.list_tools_by_module()
        
        return {
            "total_tools": len(self.tools),
            "total_modules": len(self.imported_modules),
            "tools_by_module": {k: len(v) for k, v in tools_by_module.items()},
            "namespaces": list(self.namespace_prefixes.values())
        }
    
    def force_reload_adapters(self) -> Dict[str, Any]:
        """Force reload all adapter modules"""
        try:
            # Clear existing state
            self.tools.clear()
            self.imported_modules.clear()
            self.namespace_prefixes.clear()
            
            # Re-discover all tools
            total_tools = self.discover_tools()
            
            return {
                "success": True,
                "total_tools": total_tools,
                "stats": self.get_stats()
            }
        except Exception as e:
            return {"error": f"Failed to force reload adapters: {str(e)}"}
    
    def debug_tool_loading(self) -> Dict[str, Any]:
        """Debug information about tool loading"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        
        # Check for adapter files
        adapter_files = []
        for pattern in self.priority_patterns:
            adapter_files.extend(glob.glob(os.path.join(current_dir, pattern)))
        
        return {
            "current_directory": current_dir,
            "adapter_files_found": [os.path.basename(f) for f in adapter_files],
            "imported_modules": list(self.imported_modules),
            "namespace_prefixes": self.namespace_prefixes,
            "total_tools": len(self.tools),
            "tools_by_namespace": self.list_tools_by_module()
        }

# Create a global tool manager instance
tool_manager = ToolManager()

# Additional utility functions that can be used as tools themselves
def list_all_tools(**kwargs) -> Dict[str, Any]:
    """List all available tools"""
    return {
        "tools": tool_manager.get_all_tools(),
        "stats": tool_manager.get_stats(),
        "by_module": tool_manager.list_tools_by_module()
    }

def get_tool_info(**kwargs) -> Dict[str, Any]:
    """Get information about a specific tool"""
    tool_id = kwargs.get('tool_id', '')
    if not tool_id:
        return {"error": "tool_id parameter is required"}
    
    return tool_manager.get_tool_info(tool_id)

def reload_tools(**kwargs) -> Dict[str, Any]:
    """Reload and rediscover all tools"""
    return tool_manager.force_reload_adapters()

def debug_tool_loading(**kwargs) -> Dict[str, Any]:
    """Debug tool loading process"""
    return tool_manager.debug_tool_loading()

# Register these utility tools
tool_manager.register_tool("tools:list_all", list_all_tools)
tool_manager.register_tool("tools:get_info", get_tool_info)
tool_manager.register_tool("tools:reload", reload_tools)
tool_manager.register_tool("tools:debug", debug_tool_loading)

# **FIXED: Initialize tool discovery on import**
if __name__ == "__main__":
    # Run tool discovery
    discovered_count = tool_manager.discover_tools()
    print(f"🔧 Tool Manager initialized with {discovered_count} tools")
    
    # Print debug info
    debug_info = tool_manager.debug_tool_loading()
    print(f"📁 Found adapter files: {debug_info['adapter_files_found']}")
    print(f"🎯 Namespaces: {debug_info['namespace_prefixes']}")
else:
    # Auto-discover tools when imported
    try:
        discovered_count = tool_manager.discover_tools()
        print(f"🔧 Tool Manager auto-initialized with {discovered_count} tools")
    except Exception as e:
        print(f"❌ Tool Manager initialization failed: {e}")


==================================================
FILE: utils.py
==================================================

#!/usr/bin/env python3

import os
import json
import re
import datetime
from typing import Dict, Any, List, Optional, Set

def log_api(agent_name, prompt, response):
    """Simple function to log API calls and responses to a file."""
    logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file = os.path.join(logs_dir, f"{agent_name}_api.log")
    
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n==== API CALL AT {timestamp} ====\n")
        f.write(f"PROMPT:\n{prompt}")
        f.write(f"\n\n==== API RESPONSE ====\n")
        f.write(f"{response}")
        f.write("\n\n")

def extract_json_from_text(text: str) -> Dict[str, Any]:
    """Extract JSON from text, handling various formats"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # Try to find JSON within code blocks
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try to find anything that looks like a JSON object
    object_pattern = r'({[\s\S]*?})'
    match = re.search(object_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    return {"error": "Could not extract valid JSON from response", "text": text[:500]}

def extract_tool_calls(response_content: str) -> List[Dict[str, Any]]:
    """Extract all tool calls from a response"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    tool_calls = []
    
    matches = re.finditer(tool_usage_pattern, response_content, re.DOTALL)
    for match in matches:
        tool_name = match.group(1).strip()
        params_text = "{" + match.group(2) + "}"
        try:
            params = json.loads(params_text)
            tool_calls.append({
                "tool_name": tool_name,
                "params": params,
                "full_text": match.group(0)
            })
        except json.JSONDecodeError:
            continue
    
    return tool_calls

def process_single_tool_call(response_content):
    """Process a response that may contain a single tool call"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    match = re.search(tool_usage_pattern, response_content, re.DOTALL)
    
    if not match:
        return None, response_content
    
    tool_name = match.group(1).strip()
    params_text = "{" + match.group(2) + "}"
    
    try:
        params = json.loads(params_text)
        return {
            "tool_name": tool_name,
            "params": params
        }, response_content
    except json.JSONDecodeError:
        return None, response_content

def is_hashable(obj):
    """Check if an object can be used as a dictionary key"""
    try:
        hash(obj)
        return True
    except TypeError:
        return False

def get_config():
    """Get configuration or create default config"""
    try:
        from config import CONFIG
        return CONFIG
    except ImportError:
        try:
            from openrouter_config import CONFIG
            return CONFIG
        except ImportError:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            CONFIG = {
                "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "default_model": "deepseek/deepseek-chat:free",
                "output_dir": os.path.join(current_dir, "async_outputs")
            }
            os.makedirs(CONFIG["output_dir"], exist_ok=True)
            return CONFIG


==================================================
FILE: workflow_runner_v1.py
==================================================

#!/usr/bin/env python3
import json
import sys
from pathlib import Path
from tool_manager import tool_manager

class WorkflowRunner:
    def __init__(self):
        self.state = {}
        self.results = {}
        
    def run(self, workflow_file):
        with open(workflow_file) as f:
            workflow = json.load(f)
        
        for step in workflow:
            if step.get("type") == "state":
                self._handle_state(step)
            else:
                self._execute_step(step)
        
        return self.results
    
    def _handle_state(self, step):
        if step["operation"] == "set_variable":
            self.state[step["variable_name"]] = step["value"]
    
    def _execute_step(self, step):
        agent_id = step["agent"]
        
        # Get data from previous agents
        input_data = {}
        for source in step.get("readFrom", []):
            if source in self.results:
                input_data[source] = self.results[source]
        
        # Execute tools
        tool_results = {}
        for tool_spec in step.get("tools", []):
            if isinstance(tool_spec, str):
                tool_id = tool_spec
                params = step.get("parameters", {})
            else:
                tool_id = tool_spec["tool"]
                params = tool_spec.get("parameters", {})
            
            # Replace template variables
            params = self._replace_templates(params)
            
            result = tool_manager.execute_tool(tool_id, **params)
            tool_results[tool_id] = result
        
        self.results[agent_id] = {
            "input_data": input_data,
            "tool_results": tool_results,
            "metadata": {
                "content": step.get("content"),
                "output_format": step.get("output_format")
            }
        }
    
    def _replace_templates(self, obj):
        if isinstance(obj, str):
            for var, val in self.state.items():
                obj = obj.replace(f"{{{{{var}}}}}", str(val))
            return obj
        elif isinstance(obj, dict):
            return {k: self._replace_templates(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._replace_templates(item) for item in obj]
        return obj

def main():
    if len(sys.argv) != 2:
        print("Usage: python workflow_runner.py <workflow.json>")
        sys.exit(1)
    
    runner = WorkflowRunner()
    results = runner.run(sys.argv[1])
    
    output_file = Path(sys.argv[1]).stem + "_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"Results saved to {output_file}")

if __name__ == "__main__":
    main()


==================================================
FILE: workflow_runner_v2.py
==================================================

#!/usr/bin/env python3
import json
import sys
import argparse
from pathlib import Path
from tool_manager import tool_manager
#Allows data inputs 
class WorkflowRunner:
    def __init__(self, data_files=None):
        self.state = {}
        self.results = {}
        self.data_files = data_files or []
        
    def run(self, workflow_file):
        with open(workflow_file) as f:
            workflow = json.load(f)
        
        # Add data files to state
        for i, data_file in enumerate(self.data_files):
            self.state[f"data_file_{i+1}"] = data_file
        
        for step in workflow:
            if step.get("type") == "state":
                self._handle_state(step)
            else:
                self._execute_step(step)
        
        return self.results
    
    def _handle_state(self, step):
        if step["operation"] == "set_variable":
            self.state[step["variable_name"]] = step["value"]
    
    def _execute_step(self, step):
        agent_id = step["agent"]
        
        # Get data from previous agents
        input_data = {}
        for source in step.get("readFrom", []):
            if source in self.results:
                input_data[source] = self.results[source]
        
        # Execute tools
        tool_results = {}
        for tool_spec in step.get("tools", []):
            if isinstance(tool_spec, str):
                tool_id = tool_spec
                params = step.get("parameters", {})
            else:
                tool_id = tool_spec["tool"]
                params = tool_spec.get("parameters", {})
            
            # Replace template variables
            params = self._replace_templates(params)
            
            result = tool_manager.execute_tool(tool_id, **params)
            tool_results[tool_id] = result
        
        self.results[agent_id] = {
            "input_data": input_data,
            "tool_results": tool_results,
            "metadata": {
                "content": step.get("content"),
                "output_format": step.get("output_format")
            }
        }
    
    def _replace_templates(self, obj):
        if isinstance(obj, str):
            for var, val in self.state.items():
                obj = obj.replace(f"{{{{{var}}}}}", str(val))
            return obj
        elif isinstance(obj, dict):
            return {k: self._replace_templates(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._replace_templates(item) for item in obj]
        return obj

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--workflow", required=True, help="JSON workflow file")
    parser.add_argument("--data", nargs="*", default=[], help="Up to 3 CSV data files")
    
    args = parser.parse_args()
    
    if len(args.data) > 3:
        print("Error: Maximum 3 data files allowed")
        sys.exit(1)
    
    runner = WorkflowRunner(args.data)
    results = runner.run(args.workflow)
    
    output_file = Path(args.workflow).stem + "_results.json"
    with open(output_file, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"Results saved to {output_file}")

if __name__ == "__main__":
    main()


==================================================
SUMMARY: Processed 21 Python files
Output saved to: all_python_files.txt
