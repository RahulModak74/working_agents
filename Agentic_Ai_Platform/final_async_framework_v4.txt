Python Files Content Aggregation
Generated on 2025-07-16 at 18:05:33
==================================================

==================================================
FILE: async_dynamic_agents.py
==================================================

#!/usr/bin/env python3

import asyncio
import json
from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger("async_dynamic_agents")

class AsyncDynamicAgentExecutor:
    """Async executor with dynamic agent support"""
    
    def __init__(self, base_executor):
        self.base_executor = base_executor
        self.dynamic_results = {}
    
    async def execute_dynamic_workflow(self, workflow: List[Dict[str, Any]], 
                                     data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with dynamic agent support"""
        
        results = {}
        
        for step_index, step in enumerate(workflow):
            step_type = step.get("type", "agent")
            
            if step_type == "dynamic":
                # Handle dynamic agent
                dynamic_result = await self._execute_dynamic_agent(step, results, data_file)
                
                agent_name = step.get("agent")
                if agent_name:
                    results[agent_name] = dynamic_result
                
                # Check if dynamic agent selected an action
                action_key = await self._extract_action_from_result(dynamic_result)
                
                if action_key:
                    action_name = f"{agent_name}_action"
                    results[action_name] = action_key
                    logger.info(f"üîç Dynamic agent selected action: {action_key}")
                    
                    # Execute the selected action
                    await self._execute_dynamic_action(step, action_key, results, data_file)
            
            else:
                # Handle regular agent
                if hasattr(self.base_executor, '_execute_step_async'):
                    result = await self.base_executor._execute_step_async(step, data_file)
                else:
                    # Fallback to regular execution
                    result = await self._execute_regular_agent(step, results, data_file)
                
                agent_name = step.get("agent")
                if agent_name:
                    results[agent_name] = result
        
        return results
    
    async def _execute_dynamic_agent(self, step: Dict[str, Any], 
                                   current_results: Dict[str, Any], 
                                   data_file: str = None) -> Dict[str, Any]:
        """Execute dynamic agent that can choose its action"""
        
        agent_name = step.get("agent")
        initial_prompt = step.get("initial_prompt", "")
        output_format = step.get("output_format", {})
        required_tools = step.get("tools", [])
        
        # Collect references for dynamic agent
        references = self._collect_references(step.get("readFrom", []), current_results)
        
        # Create enhanced prompt for dynamic decision making
        enhanced_prompt = initial_prompt
        
        # Add available actions information
        actions = step.get("actions", {})
        if actions:
            enhanced_prompt += "\n\n### Available Actions:\n"
            for action_key, action_info in actions.items():
                action_desc = action_info.get("description", action_key)
                enhanced_prompt += f"- **{action_key}**: {action_desc}\n"
            
            enhanced_prompt += "\nBased on the context and available information, select the most appropriate action."
        
        # Add reference information
        if references:
            enhanced_prompt += "\n\n### Reference Information:\n"
            for ref_name, ref_content in references.items():
                enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                if isinstance(ref_content, dict):
                    enhanced_prompt += json.dumps(ref_content, indent=2)
                else:
                    enhanced_prompt += str(ref_content)
        
        # Execute the dynamic agent
        result = await self._call_agent_async(
            agent_name=agent_name,
            prompt=enhanced_prompt,
            output_format=output_format,
            required_tools=required_tools,
            file_path=data_file if step.get("file") else None
        )
        
        return result
    
    async def _execute_dynamic_action(self, dynamic_step: Dict[str, Any], 
                                    action_key: str, 
                                    current_results: Dict[str, Any],
                                    data_file: str = None):
        """Execute the action selected by dynamic agent"""
        
        actions = dynamic_step.get("actions", {})
        
        if action_key not in actions:
            logger.warning(f"Selected action '{action_key}' not found in available actions")
            return
        
        action = actions[action_key]
        next_agent_name = action.get("agent")
        
        if not next_agent_name:
            logger.warning(f"Action '{action_key}' has no agent specified")
            return
        
        action_content = action.get("content", "")
        action_output_format = action.get("output_format")
        action_tools = action.get("tools", [])
        
        # Collect references for the action
        action_refs = self._collect_references(action.get("readFrom", []), current_results)
        
        # Execute the action agent
        action_result = await self._call_agent_async(
            agent_name=next_agent_name,
            prompt=action_content,
            output_format=action_output_format,
            required_tools=action_tools,
            file_path=data_file if action.get("file") else None,
            references=action_refs
        )
        
        # Store action result
        current_results[next_agent_name] = action_result
        logger.info(f"‚úÖ Dynamic action '{action_key}' executed by {next_agent_name}")
    
    async def _extract_action_from_result(self, result: Dict[str, Any]) -> Optional[str]:
        """Extract selected action from dynamic agent result"""
        
        if not isinstance(result, dict):
            return None
        
        # Try various possible action keys
        for key in ["response_action", "action", "selected_focus", "selected_action", "choice"]:
            if key in result:
                action_value = result[key]
                if isinstance(action_value, str):
                    return action_value
                elif isinstance(action_value, dict) and "action" in action_value:
                    return action_value["action"]
        
        # Try to parse from content
        content = result.get("content", "")
        if isinstance(content, str):
            # Look for action indicators in content
            import re
            action_patterns = [
                r"selected action:\s*([a-zA-Z_][a-zA-Z0-9_]*)",
                r"action:\s*([a-zA-Z_][a-zA-Z0-9_]*)",
                r"choose:\s*([a-zA-Z_][a-zA-Z0-9_]*)"
            ]
            
            for pattern in action_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    return match.group(1)
        
        return None
    
    async def _call_agent_async(self, agent_name: str, prompt: str, 
                              output_format: Optional[Dict[str, Any]] = None,
                              required_tools: List[str] = None,
                              file_path: Optional[str] = None,
                              references: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Call agent asynchronously with full feature support"""
        
        # Build enhanced prompt
        enhanced_prompt = prompt
        
        # Add reference information
        if references:
            enhanced_prompt += "\n\n### Reference Information:\n"
            for ref_name, ref_content in references.items():
                enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                if isinstance(ref_content, dict):
                    enhanced_prompt += json.dumps(ref_content, indent=2)
                else:
                    enhanced_prompt += str(ref_content)
        
        # Add file content if provided
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                    if len(file_content) > 10000:
                        file_content = file_content[:10000]
                    enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
            except Exception as e:
                logger.warning(f"Could not read file {file_path}: {e}")
        
        # Add tool information
        if required_tools:
            enhanced_prompt += f"\n\nYou have access to these tools: {', '.join(required_tools)}"
        
        # Add format instructions
        if output_format:
            output_type = output_format.get("type", "text")
            schema = output_format.get("schema")
            
            if output_type == "json" and schema:
                enhanced_prompt += "\n\n### Response Format Instructions:\n"
                enhanced_prompt += "You MUST respond with a valid JSON object exactly matching this schema:\n"
                enhanced_prompt += f"```json\n{json.dumps(schema, indent=2)}\n```\n"
                enhanced_prompt += "\nReturning properly formatted JSON is CRITICAL."
        
        # Build conversation
        system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
        conversation = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": enhanced_prompt}
        ]
        
        # Use base executor's API call method
        if hasattr(self.base_executor, '_call_api_async'):
            response = await self.base_executor._call_api_async(conversation)
        else:
            # Fallback - simulate async call
            await asyncio.sleep(0.1)
            response = f"Simulated response from {agent_name}"
        
        # Process response based on output format
        if output_format and output_format.get("type") == "json":
            return self._extract_json_from_response(response)
        else:
            return {"content": response}
    
    async def _execute_regular_agent(self, step: Dict[str, Any], 
                                   current_results: Dict[str, Any],
                                   data_file: str = None) -> Dict[str, Any]:
        """Execute regular (non-dynamic) agent"""
        
        agent_name = step.get("agent")
        content = step.get("content", "")
        output_format = step.get("output_format", {})
        required_tools = step.get("tools", [])
        
        # Collect references
        references = self._collect_references(step.get("readFrom", []), current_results)
        
        # Execute agent
        return await self._call_agent_async(
            agent_name=agent_name,
            prompt=content,
            output_format=output_format,
            required_tools=required_tools,
            file_path=data_file if step.get("file") else None,
            references=references
        )
    
    def _collect_references(self, read_from: List[str], current_results: Dict[str, Any]) -> Dict[str, Any]:
        """Collect references from current results"""
        references = {}
        
        for ref in read_from:
            if ref == "*":
                # Include all current results
                references.update(current_results)
            elif isinstance(ref, str) and ref in current_results:
                references[ref] = current_results[ref]
        
        return references
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """Extract JSON from response with fallback"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    return {"error": "Could not parse JSON", "raw_response": response}
            else:
                return {"error": "No JSON found", "raw_response": response}

# Integration class that combines all async features
class FullAsyncWorkflowExecutor:
    """Complete async workflow executor with all features"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50):
        self.config = config
        self.max_concurrent = max_concurrent
        
        # Initialize sub-executors
        from async_executor import AsyncWorkflowExecutor
        self.base_executor = AsyncWorkflowExecutor(config, max_concurrent)
        
        # Add tool integration if available
        try:
            from async_tool_integration import AsyncToolIntegratedExecutor
            self.tool_executor = AsyncToolIntegratedExecutor(config, max_concurrent)
        except ImportError as e:
            logger.warning(f"Tool integration not available: {e}")
            self.tool_executor = None
        
        # Add flow control if available
        try:
            from tool_manager import tool_manager
            from async_flow_control import AsyncFlowControlExecutor
            self.flow_executor = AsyncFlowControlExecutor(tool_manager, max_concurrent)
        except ImportError as e:
            logger.warning(f"Flow control not available: {e}")
            self.flow_executor = None
        
        # Add dynamic agent support
        self.dynamic_executor = AsyncDynamicAgentExecutor(self.base_executor)
    
    async def execute_complete_workflow(self, workflow: List[Dict[str, Any]], 
                                      data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with all async features enabled"""
        
        logger.info("üöÄ Starting complete async workflow execution")
        
        # Analyze workflow to determine features needed
        features_needed = self._analyze_workflow_features(workflow)
        logger.info(f"Detected features: {', '.join(features_needed)}")
        
        try:
            # Choose appropriate executor based on features
            if "dynamic_agents" in features_needed:
                if "flow_control" in features_needed and self.flow_executor:
                    # Complex workflow with both dynamic agents and flow control
                    return await self._execute_complex_workflow(workflow, data_file)
                else:
                    # Dynamic agents only
                    return await self.dynamic_executor.execute_dynamic_workflow(workflow, data_file)
            
            elif "flow_control" in features_needed and self.flow_executor:
                # Flow control workflow
                return await self.flow_executor.execute_workflow_with_flow_control(workflow, data_file)
            
            elif "tools" in features_needed and self.tool_executor:
                # Tool-enabled workflow
                return await self.tool_executor.execute_workflow_with_tools(workflow, data_file)
            
            else:
                # Basic async workflow
                async with self.base_executor as executor:
                    return await executor.execute_workflow_async(workflow, data_file)
        
        except Exception as e:
            logger.error(f"‚ùå Complete async workflow execution failed: {e}")
            return {"error": str(e), "success": False}
    
    async def _execute_complex_workflow(self, workflow: List[Dict[str, Any]], 
                                      data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with multiple advanced features"""
        
        # This would combine dynamic agents, flow control, and tools
        # For now, prioritize dynamic agents with fallback to flow control
        
        try:
            return await self.dynamic_executor.execute_dynamic_workflow(workflow, data_file)
        except Exception as e:
            logger.warning(f"Dynamic execution failed, falling back to flow control: {e}")
            if self.flow_executor:
                return await self.flow_executor.execute_workflow_with_flow_control(workflow, data_file)
            else:
                raise e
    
    def _analyze_workflow_features(self, workflow: List[Dict[str, Any]]) -> List[str]:
        """Analyze workflow to determine what features are needed"""
        
        features = []
        
        for step in workflow:
            # Check for dynamic agents
            if step.get("type") == "dynamic":
                features.append("dynamic_agents")
            
            # Check for flow control
            if step.get("type") in ["loop", "conditional", "parallel", "state"]:
                features.append("flow_control")
            
            # Check for tools
            if step.get("tools"):
                features.append("tools")
        
        return list(set(features))  # Remove duplicates

# Utility function for easy usage
async def execute_advanced_workflow(workflow_file: str, config: Dict[str, Any], 
                                  data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Execute workflow with all advanced async features"""
    
    # Load workflow
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow = json.load(f)
    
    if isinstance(workflow, dict) and "steps" in workflow:
        workflow = workflow["steps"]
    
    # Execute with full feature support
    executor = FullAsyncWorkflowExecutor(config, max_concurrent)
    return await executor.execute_complete_workflow(workflow, data_file)


==================================================
FILE: async_executor.py
==================================================

#!/usr/bin/env python3

import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from collections import defaultdict, deque
import logging

logger = logging.getLogger("async_executor")

@dataclass
class AgentTask:
    """Represents a single agent execution task"""
    agent_name: str
    prompt: str
    file_path: Optional[str] = None
    output_format: Optional[Dict[str, Any]] = None
    references: Optional[Dict[str, Any]] = None
    required_tools: List[str] = None
    dependencies: Set[str] = None
    priority: int = 0
    step_index: int = 0  # Add step tracking

class AsyncWorkflowExecutor:
    """High-performance async workflow executor with enhanced logging"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50, 
                 max_connections: int = 100, rate_limit: float = 10.0):
        self.config = config
        self.max_concurrent = max_concurrent
        self.max_connections = max_connections
        self.rate_limit = rate_limit  # requests per second
        
        # Rate limiting
        self.request_times = deque()
        self.rate_limiter = asyncio.Semaphore(max_concurrent)
        
        # Results storage
        self.results = {}
        self.completed_agents = set()
        self.failed_agents = set()
        
        # Enhanced tracking
        self.workflow_start_time = None
        self.step_timings = {}
        self.current_batch = 0
        self.total_batches = 0
        
        # Connection management - initialize as None
        self.session = None
        self.connector = None
        
        # Session management state
        self._session_initialized = False
        self._session_lock = asyncio.Lock()
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self._close_session()
    
    async def _ensure_session(self):
        """Ensure HTTP session is properly initialized"""
        async with self._session_lock:
            if not self._session_initialized:
                try:
                    self.connector = aiohttp.TCPConnector(
                        limit=self.max_connections,
                        limit_per_host=20,
                        keepalive_timeout=30,
                        enable_cleanup_closed=True
                    )
                    
                    timeout = aiohttp.ClientTimeout(total=120, connect=10)
                    self.session = aiohttp.ClientSession(
                        connector=self.connector,
                        timeout=timeout,
                        headers={'Content-Type': 'application/json'}
                    )
                    self._session_initialized = True
                    logger.info("‚úÖ HTTP session initialized successfully")
                except Exception as e:
                    logger.error(f"‚ùå Failed to initialize HTTP session: {e}")
                    raise
    
    async def _close_session(self):
        """Properly close HTTP session"""
        async with self._session_lock:
            if self.session:
                try:
                    await self.session.close()
                    logger.debug("Session closed")
                except Exception as e:
                    logger.warning(f"Error closing session: {e}")
                finally:
                    self.session = None
            
            if self.connector:
                try:
                    await self.connector.close()
                    logger.debug("Connector closed")
                except Exception as e:
                    logger.warning(f"Error closing connector: {e}")
                finally:
                    self.connector = None
            
            self._session_initialized = False
    
    async def _rate_limit(self):
        """Implement rate limiting"""
        async with self.rate_limiter:
            now = time.time()
            
            # Remove old requests outside the window
            while self.request_times and now - self.request_times[0] > 1.0:
                self.request_times.popleft()
            
            # Check if we need to wait
            if len(self.request_times) >= self.rate_limit:
                sleep_time = 1.0 - (now - self.request_times[0])
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
            
            self.request_times.append(now)
    
    async def _call_api_async(self, conversation: List[Dict], retries: int = 3, agent_name: str = "unknown") -> str:
        """Async API call with retries and rate limiting - enhanced with agent context"""
        # Ensure session is available
        await self._ensure_session()
        
        if not self.session:
            logger.error(f"‚ùå Session not available for API call - Agent: {agent_name}")
            return "Error: Session not initialized"
        
        await self._rate_limit()
        
        payload = {
            "model": self.config["default_model"],
            "messages": conversation
        }
        
        headers = {}
        if self.config.get("api_key"):
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
        
        for attempt in range(retries):
            try:
                async with self.session.post(
                    self.config["endpoint"],
                    json=payload,
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = (data.get('content', '') or 
                                 data.get('choices', [{}])[0].get('message', {}).get('content', ''))
                        return content
                    elif response.status == 429:  # Rate limited
                        wait_time = 2 ** attempt
                        logger.warning(f"‚è±Ô∏è Agent '{agent_name}' rate limited, waiting {wait_time}s (attempt {attempt + 1}/{retries})")
                        await asyncio.sleep(wait_time)
                    else:
                        error_text = await response.text()
                        logger.error(f"‚ùå Agent '{agent_name}' API error {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"‚è±Ô∏è Agent '{agent_name}' API timeout (attempt {attempt + 1}/{retries})")
            except Exception as e:
                logger.error(f"‚ùå Agent '{agent_name}' API error (attempt {attempt + 1}/{retries}): {e}")
            
            if attempt < retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        logger.error(f"‚ùå Agent '{agent_name}' failed after {retries} attempts")
        return f"Error: API request failed for {agent_name} after {retries} attempts"
    
    async def _execute_agent_async(self, task: AgentTask) -> Dict[str, Any]:
        """Execute a single agent asynchronously with enhanced logging"""
        step_start_time = time.time()
        
        # Log agent start with context
        logger.info(f"üöÄ Starting Agent: '{task.agent_name}' (Step {task.step_index + 1})")
        if task.dependencies:
            logger.debug(f"   üìã Dependencies: {', '.join(task.dependencies)}")
        if task.required_tools:
            logger.debug(f"   üîß Tools: {', '.join(task.required_tools)}")
        
        try:
            # Build the prompt
            enhanced_prompt = task.prompt
            
            # Add reference information
            if task.references:
                enhanced_prompt += "\n\n### Reference Information:\n"
                for ref_name, ref_content in task.references.items():
                    enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                    if isinstance(ref_content, dict):
                        enhanced_prompt += json.dumps(ref_content, indent=2)
                    else:
                        enhanced_prompt += str(ref_content)
                
                logger.debug(f"   üìä Using {len(task.references)} reference(s): {', '.join(task.references.keys())}")
            
            # Add file content if provided
            if task.file_path:
                try:
                    with open(task.file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                        original_size = len(file_content)
                        if len(file_content) > 10000:
                            file_content = file_content[:10000]
                        enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
                        
                    logger.debug(f"   üìÇ File loaded: {task.file_path} ({original_size} chars, {original_size//1024}KB)")
                except Exception as e:
                    logger.warning(f"   ‚ö†Ô∏è Could not read file {task.file_path}: {e}")
            
            # Add format instructions
            if task.output_format:
                output_type = task.output_format.get("type", "text")
                schema = task.output_format.get("schema")
                
                if output_type == "json" and schema:
                    enhanced_prompt += "\n\n### Response Format Instructions:\n"
                    enhanced_prompt += "You MUST respond with a valid JSON object exactly matching this schema:\n"
                    enhanced_prompt += f"```json\n{json.dumps(schema, indent=2)}\n```\n"
                    enhanced_prompt += "\nReturning properly formatted JSON is CRITICAL."
                
                logger.debug(f"   üìù Output format: {output_type}")
            
            # Build conversation
            system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
            conversation = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Make API call with agent context
            logger.debug(f"   üåê Making API call for '{task.agent_name}'...")
            response = await self._call_api_async(conversation, agent_name=task.agent_name)
            
            # Process response based on output format
            if task.output_format and task.output_format.get("type") == "json":
                try:
                    result = json.loads(response)
                    logger.debug(f"   ‚úÖ JSON parsing successful for '{task.agent_name}'")
                except json.JSONDecodeError:
                    # Try to extract JSON from response
                    import re
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        try:
                            result = json.loads(json_match.group())
                            logger.debug(f"   ‚úÖ JSON extracted from response for '{task.agent_name}'")
                        except:
                            result = {"error": "Could not parse JSON", "raw_response": response}
                            logger.warning(f"   ‚ö†Ô∏è JSON extraction failed for '{task.agent_name}'")
                    else:
                        result = {"error": "No JSON found", "raw_response": response}
                        logger.warning(f"   ‚ö†Ô∏è No JSON found in response for '{task.agent_name}'")
            else:
                result = {"content": response}
            
            # Log completion with timing
            step_duration = time.time() - step_start_time
            self.step_timings[task.agent_name] = step_duration
            
            logger.info(f"‚úÖ Completed Agent: '{task.agent_name}' in {step_duration:.1f}s")
            return result
            
        except Exception as e:
            step_duration = time.time() - step_start_time
            error_result = {"error": f"Agent execution failed: {str(e)}"}
            logger.error(f"‚ùå Failed Agent: '{task.agent_name}' after {step_duration:.1f}s - {e}")
            return error_result
    
    def _build_dependency_graph(self, workflow: List[Dict]) -> Dict[str, Set[str]]:
        """Build dependency graph from workflow"""
        dependencies = {}
        
        for step in workflow:
            agent_name = step.get("agent", "")
            deps = set()
            
            # Check readFrom dependencies
            read_from = step.get("readFrom", [])
            for ref in read_from:
                if isinstance(ref, str) and ref != "*":
                    deps.add(ref)
                elif ref == "*":
                    # Depends on all previous agents
                    for prev_step in workflow:
                        if prev_step.get("agent") == agent_name:
                            break
                        prev_agent = prev_step.get("agent")
                        if prev_agent:
                            deps.add(prev_agent)
            
            dependencies[agent_name] = deps
        
        return dependencies
    
    def _create_execution_batches(self, workflow: List[Dict]) -> List[List[AgentTask]]:
        """Create batches of agents that can be executed in parallel"""
        dependencies = self._build_dependency_graph(workflow)
        
        # Create tasks with step indices
        tasks = {}
        for step_index, step in enumerate(workflow):
            agent_name = step.get("agent", "")
            if agent_name:
                task = AgentTask(
                    agent_name=agent_name,
                    prompt=step.get("content", ""),
                    file_path=step.get("file"),
                    output_format=step.get("output_format"),
                    required_tools=step.get("tools", []),
                    dependencies=dependencies.get(agent_name, set()),
                    step_index=step_index  # Add step tracking
                )
                tasks[agent_name] = task
        
        # Create batches using topological sort
        batches = []
        completed = set()
        remaining = set(tasks.keys())
        
        while remaining:
            # Find tasks with no pending dependencies
            ready = []
            for agent_name in remaining:
                task = tasks[agent_name]
                if task.dependencies.issubset(completed):
                    ready.append(task)
            
            if not ready:
                # Circular dependency or error - add remaining tasks
                logger.warning("‚ö†Ô∏è Possible circular dependency detected")
                ready = [tasks[name] for name in remaining]
            
            # Create batch (limit size for memory management)
            batch_size = min(len(ready), self.max_concurrent)
            batch = ready[:batch_size]
            batches.append(batch)
            
            # Update completed and remaining
            for task in batch:
                completed.add(task.agent_name)
                remaining.discard(task.agent_name)
        
        return batches
    
    async def execute_workflow_async(self, workflow: List[Dict], data_file: str = None) -> Dict[str, Any]:
        """Execute workflow asynchronously with enhanced logging"""
        self.workflow_start_time = time.time()
        
        logger.info(f"üöÄ Starting async workflow execution with {len(workflow)} agents")
        
        # Ensure session is ready
        await self._ensure_session()
        
        # Create execution batches
        batches = self._create_execution_batches(workflow)
        self.total_batches = len(batches)
        
        logger.info(f"üì¶ Created {len(batches)} execution batches")
        
        # Log batch overview
        for i, batch in enumerate(batches):
            agent_names = [task.agent_name for task in batch]
            logger.info(f"   Batch {i + 1}: {len(batch)} agents - {', '.join(agent_names[:3])}{'...' if len(batch) > 3 else ''}")
        
        results = {}
        
        try:
            for batch_idx, batch in enumerate(batches):
                self.current_batch = batch_idx + 1
                batch_start_time = time.time()
                
                agent_names = [task.agent_name for task in batch]
                logger.info(f"üì¶ BATCH {batch_idx + 1}/{len(batches)}: Executing {len(batch)} agents")
                logger.info(f"   üéØ Agents: {', '.join(agent_names)}")
                
                # Update references for tasks in this batch
                for task in batch:
                    if task.required_tools:
                        # Add references from completed agents
                        refs = {}
                        for dep in task.dependencies:
                            if dep in results:
                                refs[dep] = results[dep]
                        task.references = refs
                
                # Execute batch concurrently
                batch_tasks = [self._execute_agent_async(task) for task in batch]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # Store results and log individual completions
                successful_agents = []
                failed_agents = []
                
                for task, result in zip(batch, batch_results):
                    if isinstance(result, Exception):
                        results[task.agent_name] = {"error": str(result)}
                        self.failed_agents.add(task.agent_name)
                        failed_agents.append(task.agent_name)
                    else:
                        results[task.agent_name] = result
                        self.completed_agents.add(task.agent_name)
                        successful_agents.append(task.agent_name)
                
                # Log batch completion
                batch_duration = time.time() - batch_start_time
                completed_count = len(self.completed_agents)
                failed_count = len(self.failed_agents)
                total_count = len(workflow)
                
                logger.info(f"‚úÖ BATCH {batch_idx + 1} COMPLETED in {batch_duration:.1f}s")
                logger.info(f"   ‚úÖ Successful: {len(successful_agents)} - {', '.join(successful_agents)}")
                if failed_agents:
                    logger.warning(f"   ‚ùå Failed: {len(failed_agents)} - {', '.join(failed_agents)}")
                
                # Overall progress update
                progress_percent = (completed_count / total_count) * 100
                total_elapsed = time.time() - self.workflow_start_time
                
                logger.info(f"üìä OVERALL PROGRESS: {completed_count}/{total_count} ({progress_percent:.1f}%) completed, {failed_count} failed")
                logger.info(f"‚è±Ô∏è Total elapsed: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)")
                
                # Estimate remaining time
                if completed_count > 0:
                    avg_time_per_agent = total_elapsed / completed_count
                    remaining_agents = total_count - completed_count
                    estimated_remaining = avg_time_per_agent * remaining_agents
                    logger.info(f"üîÆ Estimated remaining time: {estimated_remaining:.1f}s ({estimated_remaining/60:.1f}min)")
                
                # Small delay between batches to prevent overwhelming
                if batch_idx < len(batches) - 1:
                    await asyncio.sleep(0.1)
        
        except Exception as e:
            logger.error(f"‚ùå Batch execution failed: {e}")
            return {"error": str(e), "results": results}
        
        # Final summary
        total_execution_time = time.time() - self.workflow_start_time
        logger.info("üéâ Async workflow execution completed")
        logger.info(f"üìä Final Results: {len(self.completed_agents)} completed, {len(self.failed_agents)} failed")
        logger.info(f"‚è±Ô∏è Total execution time: {total_execution_time:.1f}s ({total_execution_time/60:.1f}min)")
        
        # Log slowest agents
        if self.step_timings:
            slowest_agents = sorted(self.step_timings.items(), key=lambda x: x[1], reverse=True)[:3]
            logger.info(f"üêå Slowest agents: {', '.join([f'{name}({time:.1f}s)' for name, time in slowest_agents])}")
        
        return {
            "results": results,
            "completed_count": len(self.completed_agents),
            "failed_count": len(self.failed_agents),
            "total_count": len(workflow),
            "execution_time": total_execution_time,
            "step_timings": self.step_timings
        }

# Utility function for easy usage
async def execute_large_workflow(workflow_file: str, config: Dict[str, Any], 
                                data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Execute a large workflow asynchronously"""
    
    # Load workflow
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow = json.load(f)
    
    if isinstance(workflow, dict) and "steps" in workflow:
        workflow = workflow["steps"]
    
    async with AsyncWorkflowExecutor(config, max_concurrent=max_concurrent) as executor:
        return await executor.execute_workflow_async(workflow, data_file)


==================================================
FILE: async_flow_control.py
==================================================

#!/usr/bin/env python3

import asyncio
import json
import copy
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import logging

# Import workflow state components (assuming they exist)
try:
    from workflow_state import WorkflowContext, WorkflowState, ConditionEvaluator
except ImportError:
    # Minimal implementations if not available
    class WorkflowState:
        def __init__(self):
            self.variables = {}
            self.scopes = []
            self.execution_history = []
            self.checkpoints = {}
            self.current_scope = "global"
            self.iteration_counters = {}
        
        def set_variable(self, name: str, value: Any, scope: str = None):
            self.variables[name] = value
        
        def get_variable(self, name: str, scope: str = None) -> Any:
            return self.variables.get(name)
        
        def get_state_summary(self) -> Dict[str, Any]:
            return {"variables": self.variables}
    
    class ConditionEvaluator:
        def __init__(self, state): 
            self.state = state
        def evaluate(self, condition: str) -> bool:
            return True  # Simplified
    
    class WorkflowContext:
        def __init__(self):
            self.state = WorkflowState()
            self.condition_evaluator = ConditionEvaluator(self.state)
            self.results = {}

logger = logging.getLogger("async_flow_control")

class AsyncFlowControlExecutor:
    """Async executor with advanced flow control capabilities"""
    
    def __init__(self, tool_manager, max_concurrent: int = 50):
        self.tool_manager = tool_manager
        self.max_concurrent = max_concurrent
        self.context = WorkflowContext()
    
    async def execute_workflow_with_flow_control(self, workflow: List[Dict[str, Any]], 
                                               data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with full flow control support"""
        
        try:
            logger.info("Starting async workflow execution with flow control")
            
            # Initialize workflow variables if present
            if isinstance(workflow, dict) and "variables" in workflow:
                for var_name, var_value in workflow["variables"].items():
                    self.context.state.set_variable(var_name, var_value)
                workflow = workflow.get("steps", [])
            
            # Execute all steps with flow control
            for step_index, step in enumerate(workflow):
                logger.info(f"Executing step {step_index + 1}/{len(workflow)}")
                
                try:
                    result = await self._execute_step_async(step, data_file)
                    
                    # Store result if agent name is present
                    if "agent" in step:
                        agent_name = step["agent"]
                        self.context.results[agent_name] = result
                        logger.info(f"‚úÖ Step {step_index + 1} ({agent_name}) completed")
                    
                except Exception as e:
                    error_msg = f"‚ùå Step {step_index + 1} failed: {str(e)}"
                    logger.error(error_msg)
                    
                    # Handle error based on policy
                    error_policy = step.get("on_error", "stop")
                    if error_policy == "stop":
                        raise Exception(error_msg)
                    elif error_policy == "continue":
                        logger.warning("Continuing execution despite error")
                        continue
            
            # Return final results
            final_results = dict(self.context.results)
            final_results["workflow_state"] = self.context.state.get_state_summary()
            
            logger.info("‚úÖ Async flow control workflow execution completed")
            return final_results
            
        except Exception as e:
            logger.error(f"‚ùå Async flow control workflow execution failed: {str(e)}")
            return {"error": str(e), "partial_results": dict(self.context.results)}
    
    async def _execute_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute a single workflow step with async flow control"""
        
        # Resolve template variables in step
        resolved_step = self._resolve_step_variables(step)
        
        # Get step type
        step_type = resolved_step.get("type", "agent")
        
        # Route to appropriate async handler
        if step_type == "agent":
            return await self._execute_agent_step_async(resolved_step, data_file)
        elif step_type == "loop":
            return await self._execute_loop_step_async(resolved_step, data_file)
        elif step_type == "conditional":
            return await self._execute_conditional_step_async(resolved_step, data_file)
        elif step_type == "parallel":
            return await self._execute_parallel_step_async(resolved_step, data_file)
        elif step_type == "state":
            return await self._execute_state_step_async(resolved_step)
        else:
            raise ValueError(f"Unknown step type: {step_type}")
    
    async def _execute_agent_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute agent step asynchronously"""
        # This would integrate with your AsyncToolIntegratedExecutor
        # For now, simplified implementation
        agent_name = step.get("agent")
        content = step.get("content", "")
        
        # Simulate async agent execution
        await asyncio.sleep(0.1)  # Simulate work
        
        return {"content": f"Result from {agent_name}: {content[:50]}..."}
    
    async def _execute_loop_step_async(self, step: Dict[str, Any], data_file: str = None) -> List[Any]:
        """Execute async loop with various loop types"""
        
        loop_type = step.get("loop_type", "while")
        max_iterations = step.get("max_iterations", 100)
        steps = step.get("steps", [])
        
        if not steps:
            logger.warning("Loop step has no substeps")
            return []
        
        results = []
        iteration = 0
        
        # Create loop scope
        loop_scope_name = f"loop_{datetime.now().timestamp()}"
        self.context.state.set_variable("current_loop", loop_scope_name)
        
        try:
            if loop_type == "while":
                condition = step.get("condition", "true")
                
                while iteration < max_iterations:
                    # Update iteration counter
                    self.context.state.set_variable("iteration", iteration)
                    
                    # Check condition
                    if not self._evaluate_condition(condition):
                        logger.info(f"Loop condition failed at iteration {iteration}")
                        break
                    
                    # Execute substeps concurrently if possible
                    iteration_results = []
                    if step.get("parallel_substeps", False):
                        # Execute substeps in parallel
                        tasks = [self._execute_step_async(substep, data_file) for substep in steps]
                        iteration_results = await asyncio.gather(*tasks, return_exceptions=True)
                    else:
                        # Execute substeps sequentially
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                    
                    results.append(iteration_results)
                    iteration += 1
                    
                    logger.debug(f"Completed async while loop iteration {iteration}")
            
            elif loop_type == "for_count":
                count = step.get("count", 1)
                
                # Execute iterations with controlled concurrency
                semaphore = asyncio.Semaphore(min(self.max_concurrent, count))
                
                async def execute_iteration(i):
                    async with semaphore:
                        self.context.state.set_variable("iteration", i)
                        self.context.state.set_variable("index", i)
                        
                        iteration_results = []
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                        
                        return iteration_results
                
                # Execute all iterations concurrently with semaphore control
                tasks = [execute_iteration(i) for i in range(min(count, max_iterations))]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                logger.debug(f"Completed async for_count loop with {len(results)} iterations")
            
            elif loop_type == "for_each":
                collection = step.get("collection", [])
                item_variable = step.get("item_variable", "current_item")
                
                # Resolve collection if it's a variable reference
                if isinstance(collection, str):
                    collection = self.context.state.get_variable(collection) or []
                
                # Execute with controlled concurrency
                semaphore = asyncio.Semaphore(min(self.max_concurrent, len(collection)))
                
                async def execute_for_item(i, item):
                    async with semaphore:
                        self.context.state.set_variable("iteration", i)
                        self.context.state.set_variable("index", i)
                        self.context.state.set_variable(item_variable, item)
                        
                        iteration_results = []
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                        
                        return iteration_results
                
                # Execute all items concurrently with semaphore control
                tasks = [execute_for_item(i, item) for i, item in enumerate(collection[:max_iterations])]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                logger.debug(f"Completed async for_each loop with {len(results)} items")
            
            else:
                raise ValueError(f"Unknown loop type: {loop_type}")
        
        finally:
            # Cleanup loop scope
            pass
        
        logger.info(f"Async loop completed with {len(results)} iterations")
        return results
    
    async def _execute_conditional_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute async conditional step"""
        
        condition = step.get("condition", "true")
        true_steps = step.get("true_steps", step.get("then_steps", []))
        false_steps = step.get("false_steps", step.get("else_steps", []))
        
        # Evaluate condition
        if self._evaluate_condition(condition):
            logger.info(f"Condition '{condition}' evaluated to True")
            
            # Execute true steps (potentially in parallel)
            if step.get("parallel_execution", False):
                tasks = [self._execute_step_async(substep, data_file) for substep in true_steps]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                results = []
                for substep in true_steps:
                    result = await self._execute_step_async(substep, data_file)
                    results.append(result)
            
            return results
        else:
            logger.info(f"Condition '{condition}' evaluated to False")
            
            # Execute false steps
            if step.get("parallel_execution", False):
                tasks = [self._execute_step_async(substep, data_file) for substep in false_steps]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                results = []
                for substep in false_steps:
                    result = await self._execute_step_async(substep, data_file)
                    results.append(result)
            
            return results
    
    async def _execute_parallel_step_async(self, step: Dict[str, Any], data_file: str = None) -> List[Any]:
        """Execute parallel step with controlled concurrency"""
        
        parallel_steps = step.get("steps", step.get("agents", []))
        max_parallel = step.get("max_parallel", self.max_concurrent)
        
        if not parallel_steps:
            logger.warning("Parallel step has no substeps")
            return []
        
        # Use semaphore to control concurrency
        semaphore = asyncio.Semaphore(max_parallel)
        
        async def execute_with_semaphore(substep):
            async with semaphore:
                return await self._execute_step_async(substep, data_file)
        
        # Execute all substeps with controlled concurrency
        tasks = [execute_with_semaphore(substep) for substep in parallel_steps]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Parallel substep {i} failed: {result}")
                processed_results.append({"error": str(result)})
            else:
                processed_results.append(result)
        
        logger.info(f"Async parallel execution completed with {len(parallel_steps)} substeps")
        return processed_results
    
    async def _execute_state_step_async(self, step: Dict[str, Any]) -> Any:
        """Execute async state management step"""
        
        operation = step.get("operation")
        if not operation:
            raise ValueError("State step must have 'operation' field")
        
        # Most state operations are synchronous, so just wrap them
        if operation == "set_variable":
            var_name = step.get("variable_name")
            var_value = step.get("value")
            if var_name:
                self.context.state.set_variable(var_name, var_value)
                return {"success": True, "variable": var_name, "value": var_value}
        
        elif operation == "get_variable":
            var_name = step.get("variable_name")
            if var_name:
                value = self.context.state.get_variable(var_name)
                return {"variable": var_name, "value": value}
        
        elif operation == "delay":
            delay_seconds = step.get("seconds", 1.0)
            await asyncio.sleep(delay_seconds)
            return {"success": True, "delayed_seconds": delay_seconds}
        
        # Add other state operations as needed
        return {"success": True, "operation": operation}
    
    def _resolve_step_variables(self, step: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve template variables in step"""
        # Simplified implementation - could use workflow_state's resolve_step_variables
        return step
    
    def _evaluate_condition(self, condition: str) -> bool:
        """Evaluate condition"""
        # Use the condition evaluator if available
        if hasattr(self.context, 'condition_evaluator'):
            return self.context.condition_evaluator.evaluate(condition)
        
        # Simplified evaluation
        if condition.lower() in ['true', '1', 'yes']:
            return True
        elif condition.lower() in ['false', '0', 'no']:
            return False
        
        # Try to evaluate variable
        if hasattr(self.context.state, 'get_variable'):
            value = self.context.state.get_variable(condition)
            return bool(value)
        
        return True  # Default to true for safety



==================================================
FILE: async_tool_integration.py
==================================================

#!/usr/bin/env python3

import asyncio
from typing import Dict, Any, List
from async_executor import AsyncWorkflowExecutor, AgentTask
from tool_manager import tool_manager
from utils import extract_tool_calls
import json
import logging

logger = logging.getLogger("async_tool_executor")

class AsyncToolIntegratedExecutor(AsyncWorkflowExecutor):
    """Enhanced async executor with tool management integration"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50, 
                 max_connections: int = 100, rate_limit: float = 10.0):
        super().__init__(config, max_concurrent, max_connections, rate_limit)
        
        # Auto-discover tools
        self.num_tools = tool_manager.discover_tools()
        logger.info(f"üîß Auto-discovered {self.num_tools} tools for async execution")
    
    async def _execute_agent_with_tools(self, task: AgentTask) -> Dict[str, Any]:
        """Execute agent with tool support"""
        try:
            # Build enhanced prompt with tool information
            enhanced_prompt = task.prompt
            
            # Add tool information if required_tools is provided
            if task.required_tools:
                available_tools = []
                unavailable_tools = []
                
                for tool_id in task.required_tools:
                    if tool_manager.is_tool_available(tool_id):
                        available_tools.append(tool_id)
                    else:
                        unavailable_tools.append(tool_id)
                
                if available_tools:
                    enhanced_prompt += f"\n\nYou have access to these tools: {', '.join(available_tools)}"
                    enhanced_prompt += """
To use a tool, format your response like this:

I need to use the tool: $TOOL_NAME
Parameters:
{
  "param1": "value1",
  "param2": "value2"
}

Wait for the tool result before continuing.
"""
                
                if unavailable_tools:
                    enhanced_prompt += f"\n\nNote: The following tools are not available: {', '.join(unavailable_tools)}"
            
            # Add reference information
            if task.references:
                enhanced_prompt += "\n\n### Reference Information:\n"
                for ref_name, ref_content in task.references.items():
                    enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                    if isinstance(ref_content, dict):
                        enhanced_prompt += json.dumps(ref_content, indent=2)
                    else:
                        enhanced_prompt += str(ref_content)
            
            # Add file content if provided
            if task.file_path:
                try:
                    with open(task.file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                        if len(file_content) > 10000:
                            file_content = file_content[:10000]
                        enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
                except Exception as e:
                    logger.warning(f"Could not read file {task.file_path}: {e}")
            
            # Build system message
            system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
            
            # Add domain-specific additions to system message
            agent_name = task.agent_name.lower()
            if "security" in agent_name or "threat" in agent_name or "cyber" in enhanced_prompt.lower():
                system_message = "You are a cybersecurity analysis assistant. " + system_message
            elif "journey" in agent_name or "customer" in agent_name or "segment" in enhanced_prompt.lower():
                system_message = "You are a customer journey analysis assistant. " + system_message
            elif "finance" in agent_name or "investment" in agent_name or "portfolio" in enhanced_prompt.lower():
                system_message = "You are a financial analysis assistant. " + system_message
            
            # Build conversation
            conversation = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Make initial API call
            response_content = await self._call_api_async(conversation)
            
            # Handle tool calls
            if "I need to use the tool:" in response_content:
                logger.info(f"Detected tool call attempts in response for {task.agent_name}")
                
                # Process all tool calls in the response
                all_tool_calls = extract_tool_calls(response_content)
                
                if all_tool_calls:
                    # Process tool calls (limit to 5 for safety)
                    for idx, tool_call in enumerate(all_tool_calls[:5]):
                        tool_name = tool_call["tool_name"]
                        params = tool_call["params"]
                        
                        logger.info(f"üì° Processing tool call {idx+1}: {tool_name}")
                        
                        # Execute tool (this is synchronous, but fast)
                        tool_result = tool_manager.execute_tool(tool_name, **params)
                        tool_result_str = json.dumps(tool_result, indent=2)
                        
                        # Replace tool call with result
                        tool_call_text = tool_call["full_text"]
                        response_content = response_content.replace(
                            tool_call_text,
                            f"Tool result for {tool_name}:\n```json\n{tool_result_str}\n```"
                        )
                    
                    # Get final response with tool results
                    final_prompt = f"Here is the result of executing your tool calls:\n\n{response_content}\n\nBased on these results, please provide your final response."
                    
                    conversation.append({"role": "assistant", "content": response_content})
                    conversation.append({"role": "user", "content": final_prompt})
                    
                    # Get final response
                    response_content = await self._call_api_async(conversation)
            
            # Process response based on output format
            if task.output_format and task.output_format.get("type") == "json":
                result = self._extract_json_from_response(response_content)
            else:
                result = {"content": response_content}
            
            logger.info(f"‚úÖ Agent {task.agent_name} completed successfully")
            return result
            
        except Exception as e:
            error_result = {"error": f"Agent execution failed: {str(e)}"}
            logger.error(f"‚ùå Agent {task.agent_name} failed: {e}")
            return error_result
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """Extract JSON from response with fallback"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    return {"error": "Could not parse JSON", "raw_response": response}
            else:
                return {"error": "No JSON found", "raw_response": response}
    
    async def execute_workflow_with_tools(self, workflow: List[Dict], data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with full tool integration"""
        logger.info(f"Starting async workflow execution with tools: {len(workflow)} agents")
        
        # Create execution batches (same as parent)
        batches = self._create_execution_batches(workflow)
        logger.info(f"Created {len(batches)} execution batches with tool support")
        
        results = {}
        
        for batch_idx, batch in enumerate(batches):
            logger.info(f"Executing batch {batch_idx + 1}/{len(batches)} with {len(batch)} agents")
            
            # Update references for tasks in this batch
            for task in batch:
                # Add references from completed agents
                refs = {}
                for dep in task.dependencies:
                    if dep in results:
                        refs[dep] = results[dep]
                task.references = refs
            
            # Execute batch with tool support
            batch_tasks = [self._execute_agent_with_tools(task) for task in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Store results
            for task, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    results[task.agent_name] = {"error": str(result)}
                    self.failed_agents.add(task.agent_name)
                else:
                    results[task.agent_name] = result
                    self.completed_agents.add(task.agent_name)
            
            # Progress update
            completed_count = len(self.completed_agents)
            failed_count = len(self.failed_agents)
            total_count = len(workflow)
            
            logger.info(f"Progress: {completed_count}/{total_count} completed, {failed_count} failed")
            
            # Small delay between batches
            if batch_idx < len(batches) - 1:
                await asyncio.sleep(0.1)
        
        logger.info("‚úÖ Async workflow with tools execution completed")
        
        return {
            "results": results,
            "completed_count": len(self.completed_agents),
            "failed_count": len(self.failed_agents),
            "total_count": len(workflow),
            "tools_discovered": self.num_tools
        }


==================================================
FILE: config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "qwen2.5:7b",
    "api_key": "",  # Ollama doesn't need API key
    "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    "timeout": 1200  # Increase timeout
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: cross_platform_runner.py
==================================================

#!/usr/bin/env python3
"""
Cross-Platform Workflow Runner for Microsoft Intelligence Analysis
Compatible with both Windows and Linux - All dependencies included
"""

import asyncio
import aiohttp
import json
import time
import os
import sys
import logging
import re
import importlib.util
import inspect
import glob
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional, Set, Callable
from dataclasses import dataclass
from collections import defaultdict, deque
from functools import wraps

# Windows compatibility
if sys.platform == "win32":
    asyncio.set_event_loop_policy(asyncio.WindowsProactorEventLoopPolicy())

# Setup detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow_execution.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("workflow_runner")

# Configuration Management
def get_config():
    """Get configuration with multiple fallbacks"""
    config_files = ['config.py', 'openrouter_config.py', 'ollama_config.py']
    
    for config_file in config_files:
        if Path(config_file).exists():
            try:
                spec = importlib.util.spec_from_file_location("config", config_file)
                if spec and spec.loader:
                    config_module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(config_module)
                    return config_module.CONFIG
            except Exception as e:
                logger.debug(f"Failed to load {config_file}: {e}")
    
    # Default config
    return {
        "output_dir": str(Path.cwd() / "agent_outputs"),
        "default_model": "deepseek/deepseek-chat:free",
        "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
        "endpoint": "https://openrouter.ai/api/v1/chat/completions",
        "timeout": 300
    }

# Utility Functions
def extract_json_from_text(text: str) -> Dict[str, Any]:
    """Extract JSON from text response"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    object_pattern = r'({[\s\S]*?})'
    match = re.search(object_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    return {"error": "Could not extract valid JSON", "text": text[:500]}

def extract_tool_calls(response_content: str) -> List[Dict[str, Any]]:
    """Extract tool calls from response"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    tool_calls = []
    
    matches = re.finditer(tool_usage_pattern, response_content, re.DOTALL)
    for match in matches:
        tool_name = match.group(1).strip()
        params_text = "{" + match.group(2) + "}"
        try:
            params = json.loads(params_text)
            tool_calls.append({
                "tool_name": tool_name,
                "params": params,
                "full_text": match.group(0)
            })
        except json.JSONDecodeError:
            continue
    
    return tool_calls

# Tool Manager Implementation
class ToolManager:
    def __init__(self):
        self.tools = {}
        self.imported_modules = set()
        self.excluded_files = {
            'agent_runner.py', 'tool_manager.py', 'utils.py', 'config.py', 'call_api.py',
            '__init__.py', 'cli.py', 'async_executor.py', 'async_framework_main.py',
            'workflow_executor.py', 'workflow_state.py', 'enhanced_agent_runner.py'
        }
        self.excluded_functions = {
            'main', '__init__', 'setup', 'teardown', 'test_', 'debug_'
        }
    
    def discover_tools(self, directories: List[str] = None) -> int:
        """Auto-discover tools from Python modules"""
        current_dir = Path(__file__).parent
        if directories is None:
            directories = [current_dir]
            
            component_dir = current_dir / "COMPONENT"
            if component_dir.exists():
                directories.append(component_dir)
            
            for item in current_dir.iterdir():
                if (item.is_dir() and 
                    not item.name.startswith('.') and 
                    item.name not in ['__pycache__', 'COMPONENT', 'agent_outputs']):
                    directories.append(item)
        
        total_tools = 0
        logger.info(f"Scanning directories: {[str(d) for d in directories]}")
        
        for directory in directories:
            directory = Path(directory)
            if not directory.exists():
                continue
                
            python_files = list(directory.glob("*.py"))
            logger.info(f"Found {len(python_files)} Python files in {directory}")
            
            for py_file in python_files:
                if py_file.name in self.excluded_files:
                    continue
                
                module_name = py_file.stem
                if module_name in self.imported_modules:
                    continue
                
                try:
                    spec = importlib.util.spec_from_file_location(module_name, py_file)
                    if spec is None or spec.loader is None:
                        continue
                        
                    module = importlib.util.module_from_spec(spec)
                    
                    # Add directory to path temporarily
                    str_dir = str(py_file.parent)
                    if str_dir not in sys.path:
                        sys.path.insert(0, str_dir)
                        added_to_path = True
                    else:
                        added_to_path = False
                    
                    try:
                        spec.loader.exec_module(module)
                        self.imported_modules.add(module_name)
                        
                        prefix = getattr(module, 'TOOL_NAMESPACE', module_name)
                        if prefix.endswith("_adapter"):
                            prefix = prefix[:-8]
                        
                        # Register from TOOL_REGISTRY
                        registry_tools = 0
                        if hasattr(module, 'TOOL_REGISTRY'):
                            tool_registry = getattr(module, 'TOOL_REGISTRY')
                            if isinstance(tool_registry, dict):
                                for tool_id, tool_handler in tool_registry.items():
                                    full_tool_id = f"{prefix}:{tool_id}" if ':' not in tool_id else tool_id
                                    self.register_tool(full_tool_id, tool_handler)
                                    registry_tools += 1
                                    total_tools += 1
                        
                        # Auto-discover functions
                        auto_tools = self._discover_module_tools(module, prefix)
                        total_tools += len(auto_tools)
                        
                        logger.info(f"Imported {module_name}: {registry_tools} registry + {len(auto_tools)} auto tools")
                        
                    finally:
                        if added_to_path and str_dir in sys.path:
                            sys.path.remove(str_dir)
                    
                except Exception as e:
                    logger.error(f"Error importing {module_name}: {e}")
                    continue
        
        logger.info(f"üîß Total tools discovered: {total_tools}")
        return total_tools
    
    def _discover_module_tools(self, module, prefix: str) -> List[str]:
        """Discover tools from module functions"""
        discovered_tools = []
        
        for name, obj in inspect.getmembers(module):
            if name.startswith('_') or not inspect.isfunction(obj):
                continue
            
            if any(name.startswith(excluded) for excluded in self.excluded_functions):
                continue
                
            if hasattr(module, 'TOOL_REGISTRY'):
                tool_registry = getattr(module, 'TOOL_REGISTRY')
                if isinstance(tool_registry, dict) and any(handler == obj for handler in tool_registry.values()):
                    continue
            
            if obj.__doc__ and obj.__doc__.strip():
                tool_id = f"{prefix}:{name}"
                self.register_tool(tool_id, obj)
                discovered_tools.append(tool_id)
        
        return discovered_tools
    
    def register_tool(self, tool_id: str, handler: Callable) -> None:
        """Register a tool with flexible parameter handling"""
        @wraps(handler)
        def flexible_handler(**kwargs):
            try:
                sig = inspect.signature(handler)
                
                if any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values()):
                    return handler(**kwargs)
                
                filtered_kwargs = {}
                for param_name, param in sig.parameters.items():
                    if param_name in kwargs:
                        filtered_kwargs[param_name] = kwargs[param_name]
                    elif param.default == param.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
                        return {"error": f"Required parameter '{param_name}' missing"}
                
                return handler(**filtered_kwargs)
                
            except Exception as e:
                return {"error": f"Tool execution failed: {str(e)}"}
        
        self.tools[tool_id] = flexible_handler
    
    def execute_tool(self, tool_id: str, **kwargs) -> Any:
        """Execute a registered tool"""
        if tool_id not in self.tools:
            return {"error": f"Unknown tool: {tool_id}"}
        
        try:
            logger.info(f"Executing tool {tool_id}")
            result = self.tools[tool_id](**kwargs)
            return result
        except Exception as e:
            return {"error": str(e)}
    
    def get_all_tools(self) -> List[str]:
        """Get all registered tool IDs"""
        return sorted(list(self.tools.keys()))
    
    def is_tool_available(self, tool_id: str) -> bool:
        """Check if tool is available"""
        return tool_id in self.tools

# Global tool manager instance
tool_manager = ToolManager()

# Agent Task Definition
@dataclass
class AgentTask:
    agent_name: str
    prompt: str
    file_path: Optional[str] = None
    output_format: Optional[Dict[str, Any]] = None
    references: Optional[Dict[str, Any]] = None
    required_tools: List[str] = None
    dependencies: Set[str] = None
    step_index: int = 0

# Async Workflow Executor
class AsyncToolIntegratedExecutor:
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 8):
        self.config = config
        self.max_concurrent = max_concurrent
        self.rate_limiter = asyncio.Semaphore(max_concurrent)
        self.session = None
        self.connector = None
        self.results = {}
        self.completed_agents = set()
        self.failed_agents = set()
        self.request_times = deque()
        self.rate_limit = 2.0
        
    async def __aenter__(self):
        """Initialize HTTP session"""
        self.connector = aiohttp.TCPConnector(
            limit=self.max_concurrent * 2,
            limit_per_host=10,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        
        timeout = aiohttp.ClientTimeout(total=300, connect=30)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={'Content-Type': 'application/json'}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Clean up HTTP session"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    async def _rate_limit(self):
        """Rate limiting implementation"""
        async with self.rate_limiter:
            now = time.time()
            self.request_times = deque([t for t in self.request_times if now - t < 1.0])
            
            if len(self.request_times) >= self.rate_limit:
                sleep_time = 1.0 - (now - self.request_times[0])
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
            
            self.request_times.append(now)
    
    async def _call_api_async(self, conversation: List[Dict], agent_name: str = "unknown") -> str:
        """Async API call with retries"""
        await self._rate_limit()
        
        payload = {
            "model": self.config["default_model"],
            "messages": conversation,
            "temperature": 0.7,
            "max_tokens": 2000
        }
        
        headers = {}
        if self.config.get("api_key"):
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
        
        for attempt in range(3):
            try:
                async with self.session.post(
                    self.config["endpoint"],
                    json=payload,
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = (data.get('content', '') or 
                                 data.get('choices', [{}])[0].get('message', {}).get('content', ''))
                        return content
                    elif response.status == 429:
                        wait_time = 2 ** attempt
                        logger.warning(f"Rate limited for {agent_name}, waiting {wait_time}s")
                        await asyncio.sleep(wait_time)
                    else:
                        error_text = await response.text()
                        logger.error(f"API error {response.status} for {agent_name}: {error_text}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"Timeout for {agent_name} (attempt {attempt + 1})")
            except Exception as e:
                logger.error(f"API error for {agent_name}: {e}")
            
            if attempt < 2:
                await asyncio.sleep(2 ** attempt)
        
        return f"Error: API request failed for {agent_name}"
    
    async def _execute_agent_with_tools(self, task: AgentTask) -> Dict[str, Any]:
        """Execute agent with tool support"""
        try:
            enhanced_prompt = task.prompt
            
            # Add tool information
            if task.required_tools:
                available_tools = []
                for tool_id in task.required_tools:
                    if tool_manager.is_tool_available(tool_id):
                        available_tools.append(tool_id)
                
                if available_tools:
                    enhanced_prompt += f"\n\nAvailable tools: {', '.join(available_tools)}"
                    enhanced_prompt += """
To use a tool, format your response like this:

I need to use the tool: TOOL_NAME
Parameters:
{
  "param1": "value1",
  "param2": "value2"
}

Wait for the tool result before continuing.
"""
            
            # Add references
            if task.references:
                enhanced_prompt += "\n\n### Reference Information:\n"
                for ref_name, ref_content in task.references.items():
                    enhanced_prompt += f"\n#### {ref_name}:\n"
                    if isinstance(ref_content, dict):
                        enhanced_prompt += json.dumps(ref_content, indent=2)
                    else:
                        enhanced_prompt += str(ref_content)
            
            # Add file content
            if task.file_path and Path(task.file_path).exists():
                try:
                    file_content = Path(task.file_path).read_text(encoding='utf-8')
                    if len(file_content) > 10000:
                        file_content = file_content[:10000]
                    enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
                except Exception as e:
                    logger.warning(f"Could not read file {task.file_path}: {e}")
            
            # Add format instructions
            if task.output_format and task.output_format.get("type") == "json":
                schema = task.output_format.get("schema")
                if schema:
                    enhanced_prompt += "\n\n### Response Format:\n"
                    enhanced_prompt += "Return valid JSON matching this schema:\n"
                    enhanced_prompt += f"```json\n{json.dumps(schema, indent=2)}\n```"
            
            # Build conversation
            conversation = [
                {"role": "system", "content": "You are a specialized assistant. Follow instructions precisely."},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Make API call
            response_content = await self._call_api_async(conversation, task.agent_name)
            
            # Handle tool calls
            if "I need to use the tool:" in response_content:
                tool_calls = extract_tool_calls(response_content)
                
                if tool_calls:
                    for tool_call in tool_calls[:3]:  # Limit to 3 tools
                        tool_name = tool_call["tool_name"]
                        params = tool_call["params"]
                        
                        logger.info(f"Executing tool: {tool_name}")
                        tool_result = tool_manager.execute_tool(tool_name, **params)
                        tool_result_str = json.dumps(tool_result, indent=2)
                        
                        # Replace tool call with result
                        response_content = response_content.replace(
                            tool_call["full_text"],
                            f"Tool result for {tool_name}:\n```json\n{tool_result_str}\n```"
                        )
                    
                    # Get final response
                    final_prompt = f"Based on the tool results:\n\n{response_content}\n\nProvide your final analysis."
                    conversation.append({"role": "assistant", "content": response_content})
                    conversation.append({"role": "user", "content": final_prompt})
                    
                    response_content = await self._call_api_async(conversation, task.agent_name)
            
            # Process response
            if task.output_format and task.output_format.get("type") == "json":
                result = extract_json_from_text(response_content)
            else:
                result = {"content": response_content}
            
            logger.info(f"‚úÖ Agent {task.agent_name} completed")
            return result
            
        except Exception as e:
            error_result = {"error": f"Agent execution failed: {str(e)}"}
            logger.error(f"‚ùå Agent {task.agent_name} failed: {e}")
            return error_result
    
    def create_task_from_step(self, step: Dict[str, Any], step_index: int) -> AgentTask:
        """Create AgentTask from workflow step"""
        return AgentTask(
            agent_name=step.get("agent", ""),
            prompt=step.get("content", ""),
            file_path=step.get("file"),
            output_format=step.get("output_format"),
            required_tools=step.get("tools", []),
            step_index=step_index
        )
    
    def _build_dependency_graph(self, workflow: List[Dict]) -> Dict[str, Set[str]]:
        """Build dependency graph"""
        dependencies = {}
        
        for step in workflow:
            agent_name = step.get("agent", "")
            deps = set()
            
            read_from = step.get("readFrom", [])
            for ref in read_from:
                if isinstance(ref, str) and ref != "*":
                    deps.add(ref)
                elif ref == "*":
                    for prev_step in workflow:
                        if prev_step.get("agent") == agent_name:
                            break
                        prev_agent = prev_step.get("agent")
                        if prev_agent:
                            deps.add(prev_agent)
            
            dependencies[agent_name] = deps
        
        return dependencies
    
    def _create_execution_batches(self, workflow: List[Dict]) -> List[List[AgentTask]]:
        """Create execution batches based on dependencies"""
        dependencies = self._build_dependency_graph(workflow)
        
        tasks = {}
        for step_index, step in enumerate(workflow):
            agent_name = step.get("agent", "")
            if agent_name:
                task = self.create_task_from_step(step, step_index)
                task.dependencies = dependencies.get(agent_name, set())
                tasks[agent_name] = task
        
        batches = []
        completed = set()
        remaining = set(tasks.keys())
        
        while remaining:
            ready = []
            for agent_name in remaining:
                task = tasks[agent_name]
                if task.dependencies.issubset(completed):
                    ready.append(task)
            
            if not ready:
                ready = [tasks[name] for name in list(remaining)[:self.max_concurrent]]
            
            batch = ready[:self.max_concurrent]
            batches.append(batch)
            
            for task in batch:
                completed.add(task.agent_name)
                remaining.discard(task.agent_name)
        
        return batches
    
    async def execute_workflow_with_tools(self, workflow: List[Dict], data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with tool integration"""
        logger.info(f"Starting workflow execution: {len(workflow)} agents")
        
        batches = self._create_execution_batches(workflow)
        logger.info(f"Created {len(batches)} execution batches")
        
        for batch_idx, batch in enumerate(batches):
            batch_start_time = time.time()
            agent_names = [task.agent_name for task in batch]
            logger.info(f"Batch {batch_idx + 1}/{len(batches)}: {agent_names}")
            
            # Update references
            for task in batch:
                refs = {}
                for dep in task.dependencies:
                    if dep in self.results:
                        refs[dep] = self.results[dep]
                task.references = refs
            
            # Execute batch
            batch_tasks = [self._execute_agent_with_tools(task) for task in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Store results
            for task, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    self.results[task.agent_name] = {"error": str(result)}
                    self.failed_agents.add(task.agent_name)
                else:
                    self.results[task.agent_name] = result
                    self.completed_agents.add(task.agent_name)
            
            batch_duration = time.time() - batch_start_time
            logger.info(f"Batch {batch_idx + 1} completed in {batch_duration:.1f}s")
        
        return {
            "results": self.results,
            "completed_count": len(self.completed_agents),
            "failed_count": len(self.failed_agents),
            "total_count": len(workflow)
        }

# Workflow Definitions
def create_simplified_microsoft_workflow():
    """Create simplified Microsoft workflow"""
    workflow = {
        "workflow_name": "Microsoft_Intelligence_Simplified",
        "description": "Simplified Microsoft competitive analysis with real agents",
        "steps": [
            {
                "agent": "microsoft_stock_analyzer",
                "content": "Research Microsoft's current stock performance, market cap, and recent earnings. Find latest financial metrics and analyst ratings for MSFT stock.",
                "tools": ["research:combined_search"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "stock_price": "string",
                        "market_cap": "string",
                        "recent_earnings": "object",
                        "analyst_ratings": "array"
                    }
                }
            },
            {
                "agent": "azure_market_researcher", 
                "content": "Research Microsoft Azure's current market share in cloud computing. Compare with Amazon AWS and Google Cloud Platform market positions.",
                "tools": ["research:combined_search", "cite:sources"],
                "readFrom": ["microsoft_stock_analyzer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "azure_market_share": "string",
                        "aws_comparison": "object",
                        "gcp_comparison": "object",
                        "growth_trends": "array"
                    }
                }
            },
            {
                "agent": "office365_competitive_analyst",
                "content": "Analyze Microsoft Office 365 competitive position against Google Workspace. Research user adoption rates and feature comparisons.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["microsoft_stock_analyzer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "office365_users": "string",
                        "google_workspace_threat": "object",
                        "feature_advantages": "array",
                        "market_trends": "array"
                    }
                }
            },
            {
                "agent": "microsoft_ai_strategy_analyst",
                "content": "Research Microsoft's AI strategy including Copilot, OpenAI partnership, and competition with Google's AI offerings.",
                "tools": ["research:combined_search", "research:generate_summary"],
                "readFrom": ["azure_market_researcher"],
                "output_format": {
                    "type": "json", 
                    "schema": {
                        "copilot_adoption": "string",
                        "openai_partnership": "object",
                        "google_ai_competition": "object",
                        "ai_revenue_impact": "string"
                    }
                }
            },
            {
                "agent": "google_competitive_threat_analyzer",
                "content": "Analyze Google as Microsoft's primary competitor. Research Google Cloud, Workspace, and AI initiatives that threaten Microsoft's market position.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["azure_market_researcher", "office365_competitive_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "google_cloud_growth": "object",
                        "workspace_vs_office365": "object",
                        "google_ai_threat": "object",
                        "strategic_moves": "array"
                    }
                }
            },
            {
                "agent": "amazon_aws_threat_analyzer",
                "content": "Analyze Amazon AWS as Microsoft Azure's biggest competitor. Research AWS market dominance and competitive strategies.",
                "tools": ["research:combined_search", "cite:sources"],
                "readFrom": ["azure_market_researcher"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "aws_market_dominance": "object",
                        "azure_vs_aws_features": "object",
                        "pricing_competition": "object",
                        "enterprise_adoption": "array"
                    }
                }
            },
            {
                "agent": "microsoft_financial_trend_analyzer",
                "content": "Analyze Microsoft's financial trends, revenue growth by segment, and future projections based on competitive landscape.",
                "tools": ["research:combined_search", "research:generate_summary"],
                "readFrom": ["microsoft_stock_analyzer", "azure_market_researcher", "office365_competitive_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "revenue_by_segment": "object",
                        "growth_projections": "object",
                        "competitive_risks": "array",
                        "investment_thesis": "string"
                    }
                }
            },
            {
                "agent": "tech_industry_trends_analyst",
                "content": "Research broader technology industry trends affecting Microsoft: cloud adoption, AI transformation, remote work, cybersecurity.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["microsoft_ai_strategy_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "cloud_adoption_trends": "object",
                        "ai_transformation_impact": "object",
                        "remote_work_influence": "object",
                        "cybersecurity_opportunities": "array"
                    }
                }
            },
            {
                "agent": "competitive_landscape_synthesizer",
                "content": "Synthesize all competitive intelligence into Microsoft's overall strategic position and recommendations.",
                "tools": ["research:generate_summary", "cite:format_citations"],
                "readFrom": ["google_competitive_threat_analyzer", "amazon_aws_threat_analyzer", "microsoft_ai_strategy_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "competitive_position": "string",
                        "key_strengths": "array",
                        "major_threats": "array",
                        "strategic_recommendations": "array"
                    }
                }
            },
            {
                "agent": "microsoft_investment_advisor",
                "content": "Create investment recommendation for Microsoft stock based on competitive analysis and market trends.",
                "tools": ["research:analyze_content", "cite:sources"],
                "readFrom": ["microsoft_financial_trend_analyzer", "competitive_landscape_synthesizer", "tech_industry_trends_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "investment_recommendation": "string",
                        "price_target": "string",
                        "key_catalysts": "array",
                        "risk_factors": "array",
                        "time_horizon": "string"
                    }
                }
            },
            {
                "agent": "executive_report_generator",
                "content": "Generate comprehensive executive report summarizing Microsoft's competitive position, opportunities, and strategic recommendations.",
                "tools": ["research:generate_summary", "cite:format_citations"],
                "readFrom": ["*"],
                "output_format": {
                    "type": "markdown",
                    "sections": [
                        "Executive Summary",
                        "Market Position Analysis", 
                        "Competitive Threats",
                        "Growth Opportunities",
                        "Financial Outlook",
                        "Strategic Recommendations"
                    ]
                }
            },
            {
                "agent": "workflow_analytics_monitor",
                "content": "Analyze workflow execution performance and data quality metrics.",
                "tools": ["research:analyze_content"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "execution_stats": "object",
                        "data_quality": "object",
                        "agent_performance": "array",
                        "recommendations": "array"
                    }
                }
            }
        ]
    }
    
    return workflow

# Main Execution Functions
async def execute_microsoft_workflow_fixed():
    """Execute Microsoft workflow with proper framework integration"""
    
    print("üöÄ Microsoft Intelligence Workflow - Cross-Platform Version")
    print("=" * 60)
    
    workflow = create_simplified_microsoft_workflow()
    
    print(f"üìä Workflow Overview:")
    print(f"   Total Agents: {len(workflow['steps'])}")
    print(f"   Workflow: {workflow['workflow_name']}")
    print()
    
    # Save workflow to file
    workflow_file = "microsoft_simplified.json"
    with open(workflow_file, 'w', encoding='utf-8') as f:
        json.dump(workflow, f, indent=2)
    
    print(f"‚úÖ Workflow saved to: {workflow_file}")
    
    try:
        # Discover tools
        print("üîß Discovering available tools...")
        num_tools = tool_manager.discover_tools()
        print(f"   Discovered {num_tools} tools")
        
        # Show available research tools
        research_tools = [tool for tool in tool_manager.get_all_tools() if tool.startswith('research:')]
        print(f"   Available research tools: {len(research_tools)}")
        for tool in research_tools[:5]:
            print(f"     - {tool}")
        
        # Get configuration
        config = get_config()
        print(f"   API Endpoint: {config.get('endpoint', 'Not configured')}")
        print(f"   Model: {config.get('default_model', 'Not configured')}")
        
        # Check API key
        if not config.get('api_key'):
            print("‚ö†Ô∏è  Warning: No API key configured. Some tools may not work.")
        
        print("\nüöÄ Starting workflow execution...")
        start_time = time.time()
        
        # Execute workflow
        async with AsyncToolIntegratedExecutor(config, max_concurrent=8) as executor:
            results = await executor.execute_workflow_with_tools(workflow['steps'])
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Display results
        print(f"\n{'='*60}")
        print("üéâ WORKFLOW EXECUTION COMPLETED")
        print(f"{'='*60}")
        
        if isinstance(results, dict) and "results" in results:
            exec_results = results["results"]
            total_agents = len(workflow['steps'])
            completed_agents = results.get("completed_count", 0)
            failed_agents = results.get("failed_count", 0)
            success_rate = (completed_agents / total_agents) * 100 if total_agents > 0 else 0
            
            print(f"üìà Execution Statistics:")
            print(f"   Total Agents: {total_agents}")
            print(f"   Completed Successfully: {completed_agents}")
            print(f"   Failed: {failed_agents}")
            print(f"   Success Rate: {success_rate:.1f}%")
            print(f"   Execution Time: {execution_time:.1f} seconds")
            print(f"   Average Time per Agent: {execution_time/total_agents:.2f} seconds")
            
            # Show successful agents
            print(f"\n‚úÖ Successful Agents:")
            for agent_name, result in exec_results.items():
                if not isinstance(result, dict) or "error" not in result:
                    print(f"   ‚úì {agent_name}")
                    if isinstance(result, dict) and "content" in result:
                        content = str(result["content"])
                        preview = content[:100] + "..." if len(content) > 100 else content
                        print(f"     Preview: {preview}")
            
            # Show failed agents
            print(f"\n‚ùå Failed Agents:")
            failed_count = 0
            for agent_name, result in exec_results.items():
                if isinstance(result, dict) and "error" in result:
                    failed_count += 1
                    error_msg = result.get('error', 'Unknown error')
                    print(f"   ‚ùå {agent_name}")
                    print(f"     Error: {error_msg}")
            
            if failed_count == 0:
                print("   üéâ No failed agents!")
            
            # Save results
            timestamp = int(start_time)
            results_file = f"microsoft_results_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüìÅ Detailed results saved to: {results_file}")
            
            # Performance analysis
            if execution_time > 0:
                agents_per_second = completed_agents / execution_time
                print(f"\nüöÄ Performance Metrics:")
                print(f"   Agents per Second: {agents_per_second:.2f}")
                print(f"   Total Tools Discovered: {num_tools}")
                print(f"   Framework: AsyncToolIntegratedExecutor")
            
            # Data quality check
            research_agents = [name for name in exec_results.keys() if 'research' in name or 'analyst' in name]
            print(f"\nüìä Data Quality:")
            print(f"   Research Agents: {len(research_agents)}")
            print(f"   Synthesis Agents: {len([n for n in exec_results.keys() if 'synthesizer' in n or 'generator' in n])}")
            
            return results
            
        else:
            print(f"‚ùå Unexpected result format: {type(results)}")
            return results
            
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("üí° Make sure all framework files are available")
        return None
        
    except Exception as e:
        print(f"‚ùå Execution failed: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_individual_agent():
    """Test a single agent to verify tool integration"""
    
    print("\nüß™ Testing Individual Agent...")
    
    try:
        config = get_config()
        
        # Test single agent
        test_agent = {
            "agent": "test_microsoft_researcher",
            "content": "Research Microsoft's current stock price and recent news.",
            "tools": ["research:combined_search"],
            "output_format": {
                "type": "json",
                "schema": {
                    "stock_info": "object",
                    "recent_news": "array"
                }
            }
        }
        
        print(f"Testing agent: {test_agent['agent']}")
        
        async with AsyncToolIntegratedExecutor(config, max_concurrent=1) as executor:
            task = executor.create_task_from_step(test_agent, 0)
            result = await executor._execute_agent_with_tools(task)
        
        print(f"‚úÖ Test agent result: {type(result)}")
        if isinstance(result, dict):
            if "error" in result:
                print(f"‚ùå Test failed: {result['error']}")
            else:
                print(f"‚úÖ Test successful!")
                print(f"   Result keys: {list(result.keys())}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Individual agent test failed: {e}")
        return None

def run_workflow_from_file(workflow_path: str, data_file: str = None, max_concurrent: int = 5):
    """Run workflow from JSON file"""
    if not Path(workflow_path).exists():
        print(f"‚ùå Workflow file not found: {workflow_path}")
        return None
    
    print(f"üöÄ Loading workflow: {workflow_path}")
    
    try:
        with open(workflow_path, 'r', encoding='utf-8') as f:
            workflow_data = json.load(f)
        
        if isinstance(workflow_data, dict) and "steps" in workflow_data:
            workflow = workflow_data["steps"]
        else:
            workflow = workflow_data
        
        print(f"üìä Loaded {len(workflow)} agents from {workflow_path}")
        
        async def execute_loaded_workflow():
            try:
                # Discover tools
                print("üîß Discovering tools...")
                num_tools = tool_manager.discover_tools()
                print(f"   Discovered {num_tools} tools")
                
                # Get config
                config = get_config()
                
                # Execute workflow
                async with AsyncToolIntegratedExecutor(config, max_concurrent=max_concurrent) as executor:
                    results = await executor.execute_workflow_with_tools(workflow, data_file)
                
                return results
                
            except Exception as e:
                print(f"‚ùå Workflow execution failed: {e}")
                return None
        
        return asyncio.run(execute_loaded_workflow())
        
    except Exception as e:
        print(f"‚ùå Failed to load workflow: {e}")
        return None

def main():
    """Main execution function with CLI support"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Cross-Platform Workflow Runner")
    parser.add_argument("--workflow", help="Workflow JSON file to execute")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=5, help="Max concurrent agents")
    parser.add_argument("--microsoft", action="store_true", help="Run built-in Microsoft workflow")
    parser.add_argument("--test", action="store_true", help="Run test agent only")
    
    args = parser.parse_args()
    
    print("üè¢ Cross-Platform Workflow Runner")
    print(f"Platform: {sys.platform}")
    print("=" * 60)
    
    if args.test:
        # Test individual agent
        test_result = asyncio.run(test_individual_agent())
        if test_result and isinstance(test_result, dict) and "error" not in test_result:
            print("‚úÖ Test passed!")
        else:
            print("‚ùå Test failed!")
        return
    
    if args.workflow:
        # Run custom workflow
        print(f"Running custom workflow: {args.workflow}")
        results = run_workflow_from_file(args.workflow, args.data, args.concurrent)
        
        if results:
            print(f"\n‚úÖ Workflow completed!")
            print(f"Completed: {results.get('completed_count', 0)}")
            print(f"Failed: {results.get('failed_count', 0)}")
        else:
            print("‚ùå Workflow failed!")
        
        return
    
    if args.microsoft:
        # Run built-in Microsoft workflow
        print("Running built-in Microsoft workflow...")
        results = asyncio.run(execute_microsoft_workflow_fixed())
        
        if results:
            print("\nüéØ Microsoft Workflow Analysis Complete!")
        else:
            print("‚ùå Microsoft workflow failed!")
        
        return
    
    # Default: Test agent first, then ask what to do
    print("No specific workflow specified. Running test...")
    test_result = asyncio.run(test_individual_agent())
    
    if test_result and isinstance(test_result, dict) and "error" not in test_result:
        print("\n‚úÖ Test passed! System is working.")
        print("\nOptions:")
        print("1. Run Microsoft workflow: python script.py --microsoft")
        print("2. Run custom workflow: python script.py --workflow yourfile.json")
        print("3. Set concurrency: python script.py --workflow yourfile.json --concurrent 3")
    else:
        print("\n‚ùå Test failed! Check configuration:")
        print("1. Verify API key is set in config.py or environment")
        print("2. Check network connectivity")
        print("3. Ensure tool files are present")

if __name__ == "__main__":
    main()


==================================================
FILE: frontend_complete_asyc_framework.py
==================================================

#!/usr/bin/env python3

from sanic import Sanic, response
from sanic.request import Request
import json
import os
import asyncio
import uuid
from datetime import datetime
import traceback
import logging
from typing import Dict, Any, Optional
from pathlib import Path
import sys
import time
from collections import defaultdict

# Add current directory to Python path
current_dir = os.path.dirname(os.path.abspath(__file__))
if current_dir not in sys.path:
    sys.path.insert(0, current_dir)

# Rate limiting configuration
RATE_LIMIT_REQUESTS = 10  # requests per minute
RATE_LIMIT_WINDOW = 60    # seconds
request_history = defaultdict(list)

def check_rate_limit(identifier="default"):
    """Check if request is within rate limit"""
    now = time.time()
    
    # Clean old requests
    request_history[identifier] = [
        req_time for req_time in request_history[identifier] 
        if now - req_time < RATE_LIMIT_WINDOW
    ]
    
    # Check if under limit
    if len(request_history[identifier]) >= RATE_LIMIT_REQUESTS:
        return False
    
    # Add current request
    request_history[identifier].append(now)
    return True

# Import your complete async framework
try:
    from complete_async_framework import CompleteAsyncFrameworkManager, run_complete_async_workflow
    COMPLETE_FRAMEWORK_AVAILABLE = True
    print("‚úÖ Complete Async Framework loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Complete async framework not found: {e}")
    COMPLETE_FRAMEWORK_AVAILABLE = False

try:
    from tool_manager import tool_manager
    print("‚úÖ Tool manager loaded")
except ImportError as e:
    print(f"‚ö†Ô∏è Tool manager not found: {e}")
    tool_manager = None

try:
    from utils import get_config
    print("‚úÖ Utils loaded")
except ImportError as e:
    print(f"‚ö†Ô∏è Utils not found: {e}")
    def get_config():
        return {
            "output_dir": "./agent_outputs",
            "default_model": "qwen2.5:7b",
            "api_key": "",
            "endpoint": "http://localhost:11434/v1/chat/completions",
            "timeout": 1200
        }

# Fallback imports
try:
    from async_framework_main import run_async_workflow
    ASYNC_FRAMEWORK_AVAILABLE = True
except ImportError:
    ASYNC_FRAMEWORK_AVAILABLE = False

try:
    from workflow_fix import run_workflow_with_real_calls
    WORKFLOW_FIX_AVAILABLE = True
except ImportError:
    WORKFLOW_FIX_AVAILABLE = False

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("workflow_executor")

app = Sanic("WorkflowExecutor")

# Configuration
UPLOAD_DIR = "./uploads"
WORKFLOWS_DIR = "./workflows"
RESULTS_DIR = "./results"
TEMPLATES_DIR = "./templates"

# Ensure directories exist
for dir_path in [UPLOAD_DIR, WORKFLOWS_DIR, RESULTS_DIR, TEMPLATES_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# In-memory execution tracking
executions = {}

# Get configuration
try:
    config = get_config()
    logger.info(f"‚úÖ Configuration loaded: {config.get('endpoint', 'unknown')}")
except Exception as e:
    logger.error(f"‚ùå Failed to load configuration: {e}")
    config = {
        "output_dir": "./agent_outputs",
        "default_model": "qwen2.5:7b",
        "api_key": "",
        "endpoint": "http://localhost:11434/v1/chat/completions",
        "timeout": 1200
    }

# Update config with slower rate limits
config.update({
    "rate_limit_per_second": 0.5,  # Very slow - 1 request per 2 seconds
    "timeout": 300,  # 5 minutes timeout
    "max_retries": 2,  # Reduce retries
    "retry_delay": 10  # 10 second delay between retries
})

logger.info(f"üêå Rate limiting enabled: {config.get('rate_limit_per_second', 1)} requests/second")

def analyze_workflow_features(steps: list) -> list:
    """Analyze workflow to determine required features"""
    if not steps:
        return []
    
    features = []
    for step in steps:
        if not isinstance(step, dict):
            continue
        if step.get("type") == "dynamic":
            features.append("dynamic_agents")
        if step.get("type") in ["loop", "conditional", "parallel", "state"]:
            features.append("flow_control")
        if step.get("tools"):
            features.append("tools")
    
    return list(set(features))

# API Routes
@app.route("/health", methods=["GET"])
async def health_check(request: Request):
    """Enhanced health check endpoint"""
    
    # Check framework availability
    frameworks_available = {
        "complete_async_framework": COMPLETE_FRAMEWORK_AVAILABLE,
        "async_framework_main": ASYNC_FRAMEWORK_AVAILABLE,
        "workflow_fix": WORKFLOW_FIX_AVAILABLE
    }
    
    # Check tool manager
    tool_count = 0
    if tool_manager:
        try:
            tool_count = tool_manager.discover_tools()
        except Exception as e:
            logger.warning(f"Tool discovery failed: {e}")
    
    # Test API endpoint if possible
    api_status = "unknown"
    try:
        import requests
        headers = {"Content-Type": "application/json"}
        if config.get("api_key"):
            headers["Authorization"] = f"Bearer {config['api_key']}"
        
        test_payload = {
            "model": config["default_model"],
            "messages": [{"role": "user", "content": "test"}],
            "max_tokens": 1
        }
        
        response_test = requests.post(config["endpoint"], json=test_payload, headers=headers, timeout=5)
        api_status = "accessible" if response_test.status_code in [200, 401, 403] else f"error_{response_test.status_code}"
    except Exception as e:
        api_status = f"error: {str(e)[:50]}"
    
    return response.json({
        "status": "healthy",
        "service": "Complete Async Workflow Executor",
        "timestamp": datetime.now().isoformat(),
        "framework_status": frameworks_available,
        "config": {
            "model": config["default_model"],
            "endpoint": config["endpoint"],
            "api_status": api_status
        },
        "tools_discovered": tool_count,
        "directories": {
            "uploads": len(os.listdir(UPLOAD_DIR)),
            "workflows": len(os.listdir(WORKFLOWS_DIR)),
            "results": len(os.listdir(RESULTS_DIR))
        }
    })

@app.route("/test", methods=["GET"])
async def test_route(request: Request):
    """Simple test route"""
    return response.json({
        "status": "working", 
        "message": "Server is running",
        "frameworks": {
            "complete_framework": COMPLETE_FRAMEWORK_AVAILABLE,
            "async_framework": ASYNC_FRAMEWORK_AVAILABLE,
            "workflow_fix": WORKFLOW_FIX_AVAILABLE
        }
    })

@app.route("/upload-workflow", methods=["POST"])
async def upload_workflow(request: Request):
    """Upload a JSON workflow file"""
    try:
        logger.info("üì§ Upload workflow request received")
        
        if not request.files:
            logger.error("No files in request")
            return response.json({"error": "No files in request"}, status=400)
        
        logger.info(f"Files received: {list(request.files.keys())}")
        
        if 'workflow' not in request.files:
            logger.error("No workflow file provided")
            return response.json({"error": "No workflow file provided"}, status=400)
        
        workflow_file = request.files['workflow'][0]
        workflow_id = str(uuid.uuid4())
        
        logger.info(f"Processing workflow: {workflow_file.name}, Size: {len(workflow_file.body)} bytes")
        
        # Save workflow file
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        with open(workflow_path, 'wb') as f:
            f.write(workflow_file.body)
        
        # Validate and analyze JSON
        try:
            with open(workflow_path, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
        except json.JSONDecodeError as e:
            os.remove(workflow_path)
            logger.error(f"Invalid JSON: {e}")
            return response.json({"error": f"Invalid JSON: {str(e)}"}, status=400)
        
        # Extract workflow steps
        if isinstance(workflow_data, list):
            steps = workflow_data
        elif isinstance(workflow_data, dict):
            steps = workflow_data.get("steps", workflow_data.get("workflow", []))
        else:
            steps = []
        
        # Analyze workflow features
        features = analyze_workflow_features(steps)
        
        # Get step count and agents
        step_count = len(steps)
        agent_names = [step.get("agent", "unknown") for step in steps if step.get("agent")]
        
        logger.info(f"‚úÖ Workflow analyzed: {step_count} steps, {len(agent_names)} agents, features: {features}")
        
        return response.json({
            "workflow_id": workflow_id,
            "workflow_path": workflow_path,
            "status": "uploaded",
            "steps": step_count,
            "agents": agent_names[:10],  # First 10 agents
            "features": features,
            "filename": workflow_file.name,
            "message": "Workflow uploaded and analyzed successfully"
        })
        
    except Exception as e:
        logger.error(f"‚ùå Upload error: {str(e)}")
        traceback.print_exc()
        return response.json({"error": str(e)}, status=500)

@app.route("/upload-data", methods=["POST"])
async def upload_data(request: Request):
    """Upload data files for workflow execution"""
    try:
        if not request.files:
            return response.json({"error": "No files provided"}, status=400)
        
        uploaded_files = []
        
        for field_name, file_list in request.files.items():
            for file_obj in file_list:
                file_id = str(uuid.uuid4())
                safe_filename = f"{file_id}_{file_obj.name}"
                file_path = os.path.join(UPLOAD_DIR, safe_filename)
                
                with open(file_path, 'wb') as f:
                    f.write(file_obj.body)
                
                uploaded_files.append({
                    "field_name": field_name,
                    "original_name": file_obj.name,
                    "file_id": file_id,
                    "file_path": file_path,
                    "size": len(file_obj.body)
                })
                
                logger.info(f"üìÇ Data file uploaded: {file_obj.name} -> {file_path}")
        
        return response.json({
            "uploaded_files": uploaded_files,
            "message": f"Uploaded {len(uploaded_files)} files successfully"
        })
        
    except Exception as e:
        logger.error(f"‚ùå Data upload error: {str(e)}")
        return response.json({"error": str(e)}, status=500)

@app.route("/execute-workflow", methods=["POST"])
async def execute_workflow(request: Request):
    """Execute a workflow using the complete async framework"""
    try:
        data = request.json
        workflow_id = data.get('workflow_id')
        data_file_path = data.get('data_file_path')
        max_concurrent = data.get('max_concurrent', 3)  # Default to 3
        features = data.get('features', None)
        verbose = data.get('verbose', False)
        
        if not workflow_id:
            return response.json({"error": "workflow_id required"}, status=400)
        
        workflow_path = os.path.join(WORKFLOWS_DIR, f"{workflow_id}.json")
        
        if not os.path.exists(workflow_path):
            return response.json({"error": f"Workflow not found: {workflow_id}"}, status=404)
        
        # Validate data file if provided
        if data_file_path and not os.path.exists(data_file_path):
            return response.json({"error": f"Data file not found: {data_file_path}"}, status=404)
        
        # Create execution record
        execution_id = str(uuid.uuid4())
        executions[execution_id] = {
            "status": "started",
            "workflow_id": workflow_id,
            "workflow_path": workflow_path,
            "start_time": datetime.now().isoformat(),
            "data_file": data_file_path,
            "max_concurrent": max_concurrent,
            "features": features,
            "verbose": verbose
        }
        
        logger.info(f"üöÄ Starting execution {execution_id} with {max_concurrent} concurrent agents")
        
        # Start execution in background
        asyncio.create_task(run_complete_workflow_async(
            execution_id, workflow_path, data_file_path, max_concurrent, features, verbose
        ))
        
        return response.json({
            "execution_id": execution_id,
            "status": "started",
            "workflow_path": workflow_path,
            "data_file": data_file_path,
            "max_concurrent": max_concurrent,
            "framework": "complete_async_framework" if COMPLETE_FRAMEWORK_AVAILABLE else "fallback",
            "message": "Workflow execution started with complete async framework"
        })
        
    except Exception as e:
        logger.error(f"‚ùå Execute error: {str(e)}")
        return response.json({"error": str(e)}, status=500)

@app.route("/execution-status/<execution_id>", methods=["GET"])
async def get_execution_status(request: Request, execution_id: str):
    """Get execution status and results"""
    if execution_id not in executions:
        return response.json({"error": "Execution not found"}, status=404)
    
    execution_info = executions[execution_id].copy()
    
    # Check if results file exists
    results_file = os.path.join(RESULTS_DIR, f"{execution_id}_results.json")
    if os.path.exists(results_file):
        try:
            with open(results_file, 'r', encoding='utf-8') as f:
                results = json.load(f)
            
            # Extract summary information
            if isinstance(results, dict):
                if "results" in results:
                    exec_results = results["results"]
                    execution_info["results_summary"] = {
                        "total_agents": results.get("total_count", len(exec_results)),
                        "completed": results.get("completed_count", len(exec_results)),
                        "failed": results.get("failed_count", 0),
                        "execution_time": results.get("execution_time", 0),
                        "agents_per_second": results.get("completed_count", 0) / max(results.get("execution_time", 1), 1)
                    }
                else:
                    execution_info["results_summary"] = {
                        "total_agents": len([k for k in results.keys() if k != 'workflow_state']),
                        "completed": len([k for k in results.keys() if k != 'workflow_state']),
                        "failed": 0,
                        "execution_time": 0
                    }
                
                execution_info["results"] = results
                
        except Exception as e:
            execution_info["results_error"] = str(e)
    
    return response.json(execution_info)

@app.route("/list-workflows", methods=["GET"])
async def list_workflows(request: Request):
    """List all uploaded workflows with analysis"""
    try:
        workflows = []
        for filename in os.listdir(WORKFLOWS_DIR):
            if filename.endswith('.json'):
                workflow_id = filename[:-5]
                workflow_path = os.path.join(WORKFLOWS_DIR, filename)
                
                try:
                    with open(workflow_path, 'r', encoding='utf-8') as f:
                        workflow_data = json.load(f)
                    
                    if isinstance(workflow_data, list):
                        steps = workflow_data
                    elif isinstance(workflow_data, dict):
                        steps = workflow_data.get("steps", workflow_data.get("workflow", []))
                    else:
                        steps = []
                    
                    features = analyze_workflow_features(steps)
                    agent_names = [step.get("agent", "unknown") for step in steps if step.get("agent")]
                    
                    workflows.append({
                        "workflow_id": workflow_id,
                        "steps": len(steps),
                        "agents": agent_names[:5],
                        "features": features,
                        "created": datetime.fromtimestamp(os.path.getctime(workflow_path)).isoformat(),
                        "size_kb": round(os.path.getsize(workflow_path) / 1024, 1)
                    })
                    
                except Exception as e:
                    workflows.append({
                        "workflow_id": workflow_id,
                        "error": str(e)
                    })
        
        return response.json({"workflows": workflows})
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/list-executions", methods=["GET"])
async def list_executions(request: Request):
    """List all workflow executions"""
    try:
        enhanced_executions = {}
        for exec_id, exec_info in executions.items():
            enhanced_info = exec_info.copy()
            
            results_file = os.path.join(RESULTS_DIR, f"{exec_id}_results.json")
            if os.path.exists(results_file):
                try:
                    with open(results_file, 'r', encoding='utf-8') as f:
                        results = json.load(f)
                    
                    if isinstance(results, dict) and "results" in results:
                        enhanced_info["completion_summary"] = {
                            "completed": results.get("completed_count", 0),
                            "failed": results.get("failed_count", 0),
                            "total": results.get("total_count", 0)
                        }
                except:
                    pass
            
            enhanced_executions[exec_id] = enhanced_info
        
        return response.json({"executions": enhanced_executions})
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

@app.route("/framework-info", methods=["GET"])
async def framework_info(request: Request):
    """Get information about available frameworks and tools"""
    try:
        frameworks = {
            "complete_async_framework": {
                "available": COMPLETE_FRAMEWORK_AVAILABLE,
                "description": "Complete async framework with all features"
            },
            "async_framework_main": {
                "available": ASYNC_FRAMEWORK_AVAILABLE,
                "description": "Main async framework"
            },
            "workflow_fix": {
                "available": WORKFLOW_FIX_AVAILABLE,
                "description": "Workflow fix for real API calls"
            }
        }
        
        tools_info = {"available": False, "count": 0, "tools": []}
        if tool_manager:
            try:
                tool_count = tool_manager.discover_tools()
                tools_info = {
                    "available": True,
                    "count": tool_count,
                    "tools": tool_manager.get_all_tools()[:20],
                    "stats": tool_manager.get_stats()
                }
            except Exception as e:
                tools_info["error"] = str(e)
        
        return response.json({
            "frameworks": frameworks,
            "tools": tools_info,
            "config": {
                "model": config["default_model"],
                "endpoint": config["endpoint"],
                "output_dir": config.get("output_dir", "./agent_outputs")
            }
        })
        
    except Exception as e:
        return response.json({"error": str(e)}, status=500)

async def run_complete_workflow_async(execution_id: str, workflow_path: str, 
                                    data_file_path: str = None, max_concurrent: int = 3,
                                    features: list = None, verbose: bool = False):
    """Execute workflow asynchronously using complete framework"""
    try:
        logger.info(f"üöÄ [{execution_id}] Starting complete async workflow execution")
        executions[execution_id]["status"] = "running"
        
        # Rate limit check
        if not check_rate_limit(execution_id):
            logger.warning(f"‚ö†Ô∏è [{execution_id}] Rate limit exceeded, waiting...")
            await asyncio.sleep(30)  # Wait 30 seconds
        
        # Choose the best available framework
        if COMPLETE_FRAMEWORK_AVAILABLE:
            logger.info(f"üîß [{execution_id}] Using CompleteAsyncFrameworkManager")
            
            # Create framework manager with slower settings
            framework_manager = CompleteAsyncFrameworkManager(config, verbose)
            results = await framework_manager.execute_workflow(
                workflow_file=workflow_path,
                data_file=data_file_path,
                max_concurrent=min(max_concurrent, 3),  # Limit to 3
                features=features
            )
            
        elif ASYNC_FRAMEWORK_AVAILABLE:
            logger.info(f"üîß [{execution_id}] Using async_framework_main")
            results = await run_async_workflow(
                workflow_file=workflow_path,
                config=config,
                data_file=data_file_path,
                max_concurrent=min(max_concurrent, 3),  # Limit to 3
                features=features
            )
        elif WORKFLOW_FIX_AVAILABLE:
            logger.info(f"üîß [{execution_id}] Using workflow_fix")
            results = run_workflow_with_real_calls(
                workflow_file=workflow_path,
                data_file=data_file_path,
                max_concurrent=min(max_concurrent, 3)  # Limit to 3
            )
        else:
            raise Exception("No suitable framework available")
        
        # Save results
        results_file = os.path.join(RESULTS_DIR, f"{execution_id}_results.json")
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, default=str)
        
        # Update execution status
        execution_time = (datetime.now() - datetime.fromisoformat(executions[execution_id]["start_time"])).total_seconds()
        
        executions[execution_id].update({
            "status": "completed",
            "end_time": datetime.now().isoformat(),
            "results_file": results_file,
            "execution_time": execution_time
        })
        
        # Extract summary
        if isinstance(results, dict):
            if "results" in results:
                completed = results.get("completed_count", 0)
                failed = results.get("failed_count", 0)
                total = results.get("total_count", 0)
            else:
                completed = len([k for k in results.keys() if k != 'workflow_state'])
                failed = len([k for k, v in results.items() if isinstance(v, dict) and v.get('error')])
                total = completed + failed
            
            executions[execution_id]["summary"] = {
                "completed": completed,
                "failed": failed,
                "total": total,
                "success_rate": (completed / total * 100) if total > 0 else 0
            }
        
        logger.info(f"‚úÖ [{execution_id}] Workflow execution completed successfully")
        
    except Exception as e:
        error_msg = str(e)
        error_trace = traceback.format_exc()
        
        logger.error(f"‚ùå [{execution_id}] Workflow execution failed: {error_msg}")
        
        executions[execution_id].update({
            "status": "failed",
            "end_time": datetime.now().isoformat(),
            "error": error_msg,
            "traceback": error_trace
        })

# Main web interface route
@app.route("/", methods=["GET"])
async def web_ui(request: Request):
    """Serve the main web interface"""
    template_path = os.path.join(TEMPLATES_DIR, "index.html")
    
    if os.path.exists(template_path):
        with open(template_path, 'r', encoding='utf-8') as f:
            html_content = f.read()
        return response.html(html_content)
    else:
        return response.json({
            "error": "Template not found", 
            "message": "Please create templates/index.html file",
            "template_path": template_path
        }, status=404)

if __name__ == "__main__":
    print("üöÄ Starting Complete Async Workflow Executor API")
    print("üìã Available endpoints:")
    print("  GET  /health              - Enhanced health check")
    print("  GET  /test                - Simple test endpoint")
    print("  GET  /                    - Main web interface")
    print("  POST /upload-workflow     - Upload and analyze JSON workflow")
    print("  POST /upload-data         - Upload data files")
    print("  POST /execute-workflow    - Execute workflow with complete framework")
    print("  GET  /execution-status/<id> - Check execution status")
    print("  GET  /list-workflows      - List all workflows with analysis")
    print("  GET  /list-executions     - List all executions")
    print("  GET  /framework-info      - Get framework and tools information")
    print("üåê Access web dashboard at: http://localhost:8000")
    print("üìã API health check: http://localhost:8000/health")
    print("üß™ Test endpoint: http://localhost:8000/test")
    
    # Create directories if they don't exist
    for dir_path in [UPLOAD_DIR, WORKFLOWS_DIR, RESULTS_DIR, TEMPLATES_DIR]:
        os.makedirs(dir_path, exist_ok=True)
        print(f"üìÅ Directory ready: {dir_path}")
    
    # Log framework availability
    if COMPLETE_FRAMEWORK_AVAILABLE:
        print("‚úÖ Complete Async Framework available")
    else:
        print("‚ö†Ô∏è  Complete Async Framework not available, using fallback")
    
    # Log configuration
    print(f"ü§ñ Model: {config['default_model']}")
    print(f"üåê Endpoint: {config['endpoint']}")
    print(f"üêå Rate limit: {config['rate_limit_per_second']} requests/second")
    
    app.run(host="0.0.0.0", port=8000, debug=True)


==================================================
FILE: new_microsoft_based_platform_runner.py
==================================================

#!/usr/bin/env python3
"""
JSON-Fixed Microsoft Workflow Runner
Fixes JSON parsing issues and improves output format handling
"""

import asyncio
import json
import time
import os
import logging
import sys
import re
from datetime import datetime
from typing import Dict, Any, List, Optional

# Fix Windows encoding issues
if sys.platform == "win32":
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer)
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer)

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow_execution.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("workflow_runner")

def create_json_optimized_workflow():
    """Create workflow optimized for JSON output"""
    
    workflow = {
        "workflow_name": "Microsoft_Intelligence_JSON_Optimized",
        "description": "Microsoft competitive analysis with improved JSON formatting",
        "steps": [
            {
                "agent": "microsoft_stock_analyzer",
                "content": """Research Microsoft's current stock performance and financial metrics. 

IMPORTANT: You must respond with a valid JSON object only. Format your response exactly like this:
{
  "stock_price": "current MSFT stock price",
  "market_cap": "Microsoft market capitalization",
  "recent_earnings": {"revenue": "Q4 revenue", "profit": "Q4 profit", "growth": "growth rate"},
  "analyst_ratings": ["rating1", "rating2", "rating3"]
}

Do not include any text before or after the JSON object.""",
                "tools": ["research:combined_search"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "stock_price": "string",
                        "market_cap": "string",
                        "recent_earnings": "object",
                        "analyst_ratings": "array"
                    }
                }
            },
            
            {
                "agent": "azure_market_researcher", 
                "content": """Research Microsoft Azure's market share in cloud computing compared to AWS and Google Cloud.

IMPORTANT: Respond with valid JSON only. Format exactly like this:
{
  "azure_market_share": "Azure percentage",
  "aws_comparison": {"market_share": "AWS percentage", "position": "leader/follower"},
  "gcp_comparison": {"market_share": "GCP percentage", "position": "leader/follower"},
  "growth_trends": ["trend1", "trend2", "trend3"]
}

No additional text outside the JSON object.""",
                "tools": ["research:combined_search"],
                "readFrom": ["microsoft_stock_analyzer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "azure_market_share": "string",
                        "aws_comparison": "object",
                        "gcp_comparison": "object",
                        "growth_trends": "array"
                    }
                }
            },

            {
                "agent": "office365_analyst",
                "content": """Analyze Microsoft Office 365 vs Google Workspace competition.

IMPORTANT: Respond with valid JSON only:
{
  "office365_users": "number of Office 365 users",
  "google_workspace_users": "number of Google Workspace users",
  "competitive_advantages": ["advantage1", "advantage2", "advantage3"],
  "market_trends": ["trend1", "trend2"]
}

Only JSON, no other text.""",
                "tools": ["research:combined_search"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "office365_users": "string",
                        "google_workspace_users": "string",
                        "competitive_advantages": "array",
                        "market_trends": "array"
                    }
                }
            },

            {
                "agent": "microsoft_ai_analyst",
                "content": """Research Microsoft's AI strategy including Copilot and OpenAI partnership.

IMPORTANT: Respond with valid JSON only:
{
  "copilot_status": "current status of Microsoft Copilot",
  "openai_partnership": {"status": "partnership status", "impact": "business impact"},
  "ai_revenue": "AI-related revenue or projections",
  "competitive_position": "vs Google and others"
}

JSON format only, no additional text.""",
                "tools": ["research:combined_search"],
                "output_format": {
                    "type": "json", 
                    "schema": {
                        "copilot_status": "string",
                        "openai_partnership": "object",
                        "ai_revenue": "string",
                        "competitive_position": "string"
                    }
                }
            },

            {
                "agent": "google_threat_analyst",
                "content": """Analyze Google as a competitive threat to Microsoft across cloud, productivity, and AI.

IMPORTANT: Valid JSON response only:
{
  "cloud_threat": {"gcp_growth": "growth rate", "threat_level": "high/medium/low"},
  "workspace_threat": {"market_position": "vs Office 365", "threat_level": "high/medium/low"},
  "ai_threat": {"google_ai_products": ["product1", "product2"], "threat_level": "high/medium/low"},
  "overall_assessment": "overall competitive threat level"
}

JSON only.""",
                "tools": ["research:combined_search"],
                "readFrom": ["azure_market_researcher", "office365_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "cloud_threat": "object",
                        "workspace_threat": "object", 
                        "ai_threat": "object",
                        "overall_assessment": "string"
                    }
                }
            },

            {
                "agent": "aws_threat_analyst",
                "content": """Analyze Amazon AWS as Microsoft Azure's main competitor.

IMPORTANT: JSON response only:
{
  "aws_market_dominance": {"market_share": "AWS percentage", "lead_over_azure": "percentage difference"},
  "competitive_advantages": ["advantage1", "advantage2", "advantage3"],
  "azure_response_strategy": "how Microsoft can respond",
  "threat_assessment": "high/medium/low"
}

Only JSON format.""",
                "tools": ["research:combined_search"],
                "readFrom": ["azure_market_researcher"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "aws_market_dominance": "object",
                        "competitive_advantages": "array",
                        "azure_response_strategy": "string",
                        "threat_assessment": "string"
                    }
                }
            },

            {
                "agent": "strategic_synthesizer",
                "content": """Synthesize the competitive analysis into strategic recommendations for Microsoft.

IMPORTANT: JSON response only:
{
  "competitive_position": "Microsoft's overall market position",
  "key_strengths": ["strength1", "strength2", "strength3"],
  "major_threats": ["threat1", "threat2", "threat3"],
  "strategic_recommendations": ["recommendation1", "recommendation2", "recommendation3"],
  "investment_focus": ["area1", "area2", "area3"]
}

JSON format only.""",
                "tools": ["research:generate_summary"],
                "readFrom": ["google_threat_analyst", "aws_threat_analyst", "microsoft_ai_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "competitive_position": "string",
                        "key_strengths": "array",
                        "major_threats": "array",
                        "strategic_recommendations": "array",
                        "investment_focus": "array"
                    }
                }
            },

            {
                "agent": "investment_advisor",
                "content": """Create investment recommendation for Microsoft stock based on the competitive analysis.

IMPORTANT: JSON response only:
{
  "recommendation": "BUY/HOLD/SELL",
  "price_target": "target stock price",
  "confidence_level": "high/medium/low",
  "key_catalysts": ["catalyst1", "catalyst2", "catalyst3"],
  "risk_factors": ["risk1", "risk2", "risk3"],
  "time_horizon": "investment time frame"
}

JSON only.""",
                "tools": ["research:analyze_content"],
                "readFrom": ["microsoft_stock_analyzer", "strategic_synthesizer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "recommendation": "string",
                        "price_target": "string",
                        "confidence_level": "string",
                        "key_catalysts": "array",
                        "risk_factors": "array",
                        "time_horizon": "string"
                    }
                }
            }
        ]
    }
    
    return workflow

def enhanced_json_extractor(response_text: str) -> Dict[str, Any]:
    """Enhanced JSON extraction with multiple fallback methods"""
    
    if not response_text or not isinstance(response_text, str):
        return {"error": "Empty or invalid response", "raw_response": str(response_text)}
    
    # Method 1: Try direct JSON parsing
    try:
        return json.loads(response_text.strip())
    except json.JSONDecodeError:
        pass
    
    # Method 2: Extract JSON from code blocks
    json_blocks = re.findall(r'```(?:json)?\s*([\s\S]*?)\s*```', response_text)
    for block in json_blocks:
        try:
            return json.loads(block.strip())
        except json.JSONDecodeError:
            continue
    
    # Method 3: Find JSON-like structures with improved regex
    json_patterns = [
        r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}',  # Nested objects
        r'\{[\s\S]*\}',  # Any content between braces
    ]
    
    for pattern in json_patterns:
        matches = re.findall(pattern, response_text, re.DOTALL)
        for match in matches:
            try:
                # Clean up the match
                cleaned = match.strip()
                return json.loads(cleaned)
            except json.JSONDecodeError:
                continue
    
    # Method 4: Try to construct JSON from structured text
    if ":" in response_text and any(word in response_text.lower() for word in ["market", "share", "percentage", "$"]):
        try:
            # Extract key-value pairs from text
            extracted_data = {}
            lines = response_text.split('\n')
            
            for line in lines:
                if ':' in line and not line.strip().startswith('#'):
                    parts = line.split(':', 1)
                    if len(parts) == 2:
                        key = parts[0].strip().lower().replace(' ', '_')
                        value = parts[1].strip()
                        # Clean up value
                        value = re.sub(r'^["\'\s]*|["\'\s]*$', '', value)
                        extracted_data[key] = value
            
            if extracted_data:
                return {"extracted_data": extracted_data, "source": "text_parsing"}
        except Exception:
            pass
    
    # Method 5: If all else fails, return structured error with content
    return {
        "error": "Could not parse JSON", 
        "raw_response": response_text[:500] + "..." if len(response_text) > 500 else response_text,
        "content_available": True
    }

class ImprovedAsyncToolExecutor:
    """Wrapper to improve JSON handling in the async tool executor"""
    
    def __init__(self, base_executor):
        self.base_executor = base_executor
    
    async def execute_workflow_with_improved_json(self, workflow_steps):
        """Execute workflow with improved JSON parsing"""
        
        # Temporarily monkey patch the JSON extraction
        original_extract_json = getattr(self.base_executor, '_extract_json_from_response', None)
        
        def improved_extract_json(response_text):
            return enhanced_json_extractor(response_text)
        
        # Apply the patch
        if hasattr(self.base_executor, '_extract_json_from_response'):
            self.base_executor._extract_json_from_response = improved_extract_json
        
        try:
            # Execute the workflow
            result = await self.base_executor.execute_workflow_with_tools(workflow_steps)
            return result
        finally:
            # Restore original method
            if original_extract_json:
                self.base_executor._extract_json_from_response = original_extract_json

async def execute_json_fixed_workflow():
    """Execute workflow with JSON fixes"""
    
    print("=" * 60)
    print("Microsoft Intelligence Workflow - JSON Fixed Version")
    print("=" * 60)
    
    # Create optimized workflow
    workflow = create_json_optimized_workflow()
    
    print(f"[INFO] JSON-Optimized Workflow:")
    print(f"   Total Agents: {len(workflow['steps'])}")
    print(f"   Each agent has explicit JSON formatting instructions")
    print(f"   Enhanced JSON parsing enabled")
    print()
    
    # Save workflow
    workflow_file = "microsoft_json_optimized.json"
    with open(workflow_file, 'w', encoding='utf-8') as f:
        json.dump(workflow, f, indent=2)
    
    print(f"[SUCCESS] Workflow saved to: {workflow_file}")
    
    try:
        # Setup framework
        from tool_manager import tool_manager
        from async_tool_integration import AsyncToolIntegratedExecutor
        from utils import get_config
        
        print("[INFO] Discovering tools...")
        num_tools = tool_manager.discover_tools()
        print(f"[SUCCESS] Discovered {num_tools} tools")
        
        config = get_config()
        print(f"[INFO] Using model: {config.get('default_model')}")
        
        print("\n[START] Starting JSON-optimized execution...")
        start_time = time.time()
        
        # Use improved executor
        async with AsyncToolIntegratedExecutor(config, max_concurrent=3) as base_executor:
            improved_executor = ImprovedAsyncToolExecutor(base_executor)
            results = await improved_executor.execute_workflow_with_improved_json(workflow['steps'])
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Analyze results with focus on JSON success
        print(f"\n{'='*60}")
        print("[COMPLETE] JSON-OPTIMIZED EXECUTION COMPLETED")
        print(f"{'='*60}")
        
        if isinstance(results, dict) and "results" in results:
            exec_results = results["results"]
            total_agents = len(workflow['steps'])
            
            # Count successful JSON vs errors
            json_success = 0
            json_errors = 0
            content_available = 0
            
            print(f"[ANALYSIS] JSON Parsing Results:")
            for agent_name, result in exec_results.items():
                if isinstance(result, dict):
                    if "error" in result:
                        if "Could not parse JSON" in result.get("error", ""):
                            json_errors += 1
                            if result.get("content_available"):
                                content_available += 1
                            print(f"   [JSON_ERROR] {agent_name}: {result['error']}")
                        else:
                            print(f"   [OTHER_ERROR] {agent_name}: {result['error']}")
                    else:
                        json_success += 1
                        print(f"   [JSON_SUCCESS] {agent_name}: Valid JSON received")
                else:
                    json_success += 1
                    print(f"   [SUCCESS] {agent_name}: Content received")
            
            print(f"\n[STATS] JSON Processing Statistics:")
            print(f"   Total Agents: {total_agents}")
            print(f"   JSON Success: {json_success}")
            print(f"   JSON Errors: {json_errors}")
            print(f"   Content Available: {content_available}")
            print(f"   JSON Success Rate: {(json_success/total_agents)*100:.1f}%")
            print(f"   Execution Time: {execution_time:.1f} seconds")
            
            # Show successful JSON examples
            if json_success > 0:
                print(f"\n[EXAMPLES] Successful JSON Results:")
                count = 0
                for agent_name, result in exec_results.items():
                    if isinstance(result, dict) and "error" not in result and count < 2:
                        print(f"   {agent_name}:")
                        print(f"      Keys: {list(result.keys())}")
                        count += 1
            
            # Show content from "failed" JSON (still has useful data)
            if content_available > 0:
                print(f"\n[CONTENT] Agents with parsing errors but useful content:")
                for agent_name, result in exec_results.items():
                    if isinstance(result, dict) and result.get("content_available"):
                        content = result.get("raw_response", "")[:150]
                        print(f"   {agent_name}: {content}...")
            
            # Save results
            timestamp = int(start_time)
            results_file = f"microsoft_json_fixed_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str, ensure_ascii=False)
            
            print(f"\n[SAVE] Results saved to: {results_file}")
            
            return results
            
    except Exception as e:
        print(f"[ERROR] Execution failed: {e}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """Main execution with JSON focus"""
    
    print("Microsoft Intelligence Analysis - JSON Fixed")
    print("Addresses JSON parsing issues with enhanced extraction")
    print("=" * 60)
    
    results = asyncio.run(execute_json_fixed_workflow())
    
    if results:
        print("\n[SUCCESS] JSON-optimized workflow completed!")
        print("[INFO] Check the results file for improved JSON parsing")
        print("[NEXT] If JSON errors persist, consider using text output format")
    else:
        print("\n[ERROR] Workflow failed to execute")

if __name__ == "__main__":
    main()


==================================================
FILE: ollama_config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "qwen2.5:7b",
    "api_key": "",  # Ollama doesn't need API key
    "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    "timeout": 1200  # Increase timeout
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: openrouter_config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "deepseek/deepseek-chat:free",
#     "default_model": "openrouter/quasar-alpha",
    "api_key": "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",
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db" 
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: optimized_agent_runner.py
==================================================

#!/usr/bin/env python3

import os
import sys
import json
import asyncio
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
import logging

# Import optimized components
from async_executor import AsyncWorkflowExecutor, execute_large_workflow
from workflow_optimizer import WorkflowOptimizer, analyze_workflow_file, optimize_workflow_file
from optimized_call_api import APIConnectionPool, call_api_batch

# Import existing components
from tool_manager import tool_manager
from utils import extract_json_from_text, get_config

logger = logging.getLogger("optimized_agent_runner")

class OptimizedWorkflowRunner:
    """High-performance workflow runner for large-scale operations"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50):
        self.config = config
        self.max_concurrent = max_concurrent
        self.optimizer = WorkflowOptimizer()
        
        # Performance tracking
        self.start_time = None
        self.end_time = None
        self.total_agents = 0
        self.completed_agents = 0
        self.failed_agents = 0
        
    async def run_workflow_optimized(self, workflow_file: str, data_file: str = None) -> Dict[str, Any]:
        """Run workflow with all optimizations enabled"""
        
        self.start_time = time.time()
        logger.info(f"üöÄ Starting optimized workflow execution: {workflow_file}")
        
        try:
            # Step 1: Analyze and optimize workflow
            logger.info("üìä Analyzing workflow for optimization...")
            optimization = optimize_workflow_file(workflow_file, self.max_concurrent)
            metrics = optimization["metrics"]
            
            logger.info(f"Workflow Analysis:")
            logger.info(f"  - Total Agents: {metrics.total_agents}")
            logger.info(f"  - Estimated Time: {metrics.estimated_time_seconds:.1f}s")
            logger.info(f"  - Estimated Cost: ${metrics.estimated_cost_usd:.2f}")
            logger.info(f"  - Max Parallelization: {metrics.max_parallel}")
            
            self.total_agents = metrics.total_agents
            
            # Step 2: Auto-discover tools
            num_tools = tool_manager.discover_tools()
            logger.info(f"üîß Discovered {num_tools} tools")
            
            # Step 3: Execute with async executor
            logger.info("‚ö° Executing workflow asynchronously...")
            
            # Adjust concurrency based on recommendations
            recommended_concurrent = optimization["optimizations"].get(
                "recommended_concurrent", self.max_concurrent
            )
            
            results = await execute_large_workflow(
                workflow_file=workflow_file,
                config=self.config,
                data_file=data_file,
                max_concurrent=recommended_concurrent
            )
            
            # Step 4: Process results
            self.completed_agents = results.get("completed_count", 0)
            self.failed_agents = results.get("failed_count", 0)
            self.end_time = time.time()
            
            # Step 5: Save results with compression
            await self._save_results_optimized(results)
            
            # Step 6: Generate performance report
            performance_report = self._generate_performance_report(optimization)
            
            logger.info("‚úÖ Optimized workflow execution completed")
            
            return {
                "execution_results": results,
                "optimization_analysis": optimization,
                "performance_report": performance_report,
                "success": True
            }
            
        except Exception as e:
            self.end_time = time.time()
            error_msg = f"‚ùå Workflow execution failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "error": error_msg,
                "success": False,
                "performance_report": self._generate_performance_report() if self.start_time else None
            }
    
    async def _save_results_optimized(self, results: Dict[str, Any]):
        """Save results with optimization for large datasets"""
        
        output_dir = Path(self.config["output_dir"])
        output_dir.mkdir(exist_ok=True)
        
        # Save main results
        results_file = output_dir / "optimized_workflow_results.json"
        
        try:
            # For very large results, save with compression
            if self.total_agents > 1000:
                import gzip
                
                compressed_file = output_dir / "optimized_workflow_results.json.gz"
                with gzip.open(compressed_file, 'wt', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, default=str)
                logger.info(f"Large results saved compressed to {compressed_file}")
            else:
                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, default=str)
                logger.info(f"Results saved to {results_file}")
                
        except Exception as e:
            logger.error(f"Error saving results: {e}")
    
    def _generate_performance_report(self, optimization: Dict = None) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        
        execution_time = (self.end_time - self.start_time) if self.start_time and self.end_time else 0
        
        report = {
            "execution_summary": {
                "total_agents": self.total_agents,
                "completed_agents": self.completed_agents,
                "failed_agents": self.failed_agents,
                "success_rate": (self.completed_agents / self.total_agents * 100) if self.total_agents > 0 else 0,
                "execution_time_seconds": execution_time,
                "execution_time_minutes": execution_time / 60,
                "agents_per_second": self.completed_agents / execution_time if execution_time > 0 else 0
            }
        }
        
        if optimization:
            metrics = optimization["metrics"]
            report["optimization_analysis"] = {
                "estimated_vs_actual_time": {
                    "estimated_seconds": metrics.estimated_time_seconds,
                    "actual_seconds": execution_time,
                    "accuracy_percent": (metrics.estimated_time_seconds / execution_time * 100) if execution_time > 0 else 0
                },
                "parallelization_efficiency": {
                    "max_possible_parallel": metrics.max_parallel,
                    "actual_speedup": metrics.critical_path_length / execution_time if execution_time > 0 else 0
                },
                "cost_analysis": {
                    "estimated_cost_usd": metrics.estimated_cost_usd,
                    "cost_per_agent": metrics.estimated_cost_usd / metrics.total_agents if metrics.total_agents > 0 else 0
                }
            }
        
        return report

# Main execution functions
async def run_large_workflow_async(workflow_file: str, config: Dict = None, 
                                  data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Main async function for large workflow execution"""
    
    if config is None:
        config = get_config()
    
    runner = OptimizedWorkflowRunner(config, max_concurrent)
    return await runner.run_workflow_optimized(workflow_file, data_file)

def run_large_workflow_sync(workflow_file: str, config: Dict = None, 
                           data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Synchronous wrapper for large workflow execution"""
    
    if config is None:
        config = get_config()
    
    return asyncio.run(run_large_workflow_async(workflow_file, config, data_file, max_concurrent))

def analyze_workflow_performance(workflow_file: str) -> str:
    """Analyze workflow and generate performance report"""
    
    optimizer = WorkflowOptimizer()
    report = optimizer.generate_optimization_report(workflow_file)
    
    # Save report
    report_file = f"{workflow_file}_performance_analysis.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"Performance analysis saved to: {report_file}")
    return report

# Backward compatibility for existing code
def run_universal_workflow(workflow_file: str, data_file: str = None, 
                          optimize: bool = True) -> Dict[str, Any]:
    """Backward compatible function with optional optimization"""
    
    config = get_config()
    
    if optimize:
        # Use optimized execution for better performance
        return run_large_workflow_sync(workflow_file, config, data_file)
    else:
        # Fall back to original implementation
        from agent_runner import run_universal_workflow as original_runner
        return original_runner(workflow_file, data_file)

# CLI interface
def main():
    """Enhanced main function with optimization options"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Optimized Workflow Runner")
    parser.add_argument("--workflow", required=True, help="Workflow JSON file")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=50, help="Max concurrent agents")
    parser.add_argument("--analyze", action="store_true", help="Analyze workflow performance")
    parser.add_argument("--optimize", action="store_true", default=True, help="Enable optimizations")
    parser.add_argument("--report", action="store_true", help="Generate performance report only")
    
    args = parser.parse_args()
    
    if args.report:
        # Generate performance analysis report
        report = analyze_workflow_performance(args.workflow)
        print("\n" + "="*50)
        print("PERFORMANCE ANALYSIS REPORT")
        print("="*50)
        print(report)
        return
    
    if args.analyze:
        # Quick analysis without execution
        metrics = analyze_workflow_file(args.workflow)
        print(f"\nüìä Workflow Analysis:")
        print(f"Total Agents: {metrics.total_agents}")
        print(f"Critical Path: {metrics.critical_path_length} agents")
        print(f"Max Parallelization: {metrics.max_parallel} concurrent")
        print(f"Estimated Time: {metrics.estimated_time_seconds:.1f} seconds")
        print(f"Estimated Cost: ${metrics.estimated_cost_usd:.2f}")
        print(f"Memory Usage: {metrics.memory_usage_mb:.1f} MB")
        return
    
    # Execute workflow
    print(f"üöÄ Starting workflow execution...")
    print(f"Workflow: {args.workflow}")
    print(f"Max Concurrent: {args.concurrent}")
    print(f"Optimizations: {'Enabled' if args.optimize else 'Disabled'}")
    
    start_time = time.time()
    
    if args.optimize:
        results = run_large_workflow_sync(
            workflow_file=args.workflow,
            data_file=args.data,
            max_concurrent=args.concurrent
        )
    else:
        results = run_universal_workflow(args.workflow, args.data, optimize=False)
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    # Print results summary
    print(f"\n{'='*50}")
    print("EXECUTION SUMMARY")
    print(f"{'='*50}")
    
    if results.get("success"):
        exec_results = results.get("execution_results", {})
        print(f"‚úÖ Workflow completed successfully")
        print(f"Total Agents: {exec_results.get('total_count', 'Unknown')}")
        print(f"Completed: {exec_results.get('completed_count', 'Unknown')}")
        print(f"Failed: {exec_results.get('failed_count', 'Unknown')}")
        print(f"Execution Time: {execution_time:.1f} seconds ({execution_time/60:.1f} minutes)")
        
        if "performance_report" in results:
            perf = results["performance_report"]["execution_summary"]
            print(f"Success Rate: {perf['success_rate']:.1f}%")
            print(f"Agents/Second: {perf['agents_per_second']:.2f}")
    else:
        print(f"‚ùå Workflow failed: {results.get('error', 'Unknown error')}")

if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    main()


==================================================
FILE: optimized_call_api.py
==================================================

#!/usr/bin/env python3

import asyncio
import aiohttp
import json
import time
import hashlib
from typing import Dict, List, Optional
from collections import deque
import logging
import os

logger = logging.getLogger("optimized_call_api")

class APIConnectionPool:
    """Manages HTTP connections and implements caching/rate limiting"""
    
    def __init__(self, config: Dict, max_connections: int = 100, 
                 rate_limit: float = 20.0, cache_dir: str = "./api_cache"):
        self.config = config
        self.max_connections = max_connections
        self.rate_limit = rate_limit
        self.cache_dir = cache_dir
        
        # Rate limiting
        self.request_times = deque()
        self.semaphore = asyncio.Semaphore(max_connections)
        
        # Connection pooling
        self.session = None
        self.connector = None
        
        # Response caching
        self.enable_cache = True
        os.makedirs(cache_dir, exist_ok=True)
        
        # Statistics
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "failed_requests": 0,
            "rate_limited": 0
        }
    
    async def __aenter__(self):
        """Initialize connection pool"""
        self.connector = aiohttp.TCPConnector(
            limit=self.max_connections,
            limit_per_host=30,
            keepalive_timeout=60,
            enable_cleanup_closed=True,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(total=300, connect=15, sock_read=120)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={
                'Content-Type': 'application/json',
                'Connection': 'keep-alive'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Cleanup connections"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    def _get_cache_key(self, conversation: List[Dict]) -> str:
        """Generate cache key for conversation"""
        content = json.dumps(conversation, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> str:
        """Get cache file path"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    def _load_from_cache(self, cache_key: str) -> Optional[str]:
        """Load response from cache"""
        if not self.enable_cache:
            return None
            
        cache_path = self._get_cache_path(cache_key)
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                    
                # Check if cache is still valid (24 hours)
                cache_time = cached_data.get('timestamp', 0)
                if time.time() - cache_time < 86400:  # 24 hours
                    self.stats["cache_hits"] += 1
                    return cached_data.get('response', '')
                    
        except Exception as e:
            logger.debug(f"Cache read error: {e}")
        
        self.stats["cache_misses"] += 1
        return None
    
    def _save_to_cache(self, cache_key: str, response: str):
        """Save response to cache"""
        if not self.enable_cache:
            return
            
        cache_path = self._get_cache_path(cache_key)
        try:
            cache_data = {
                'response': response,
                'timestamp': time.time()
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f)
        except Exception as e:
            logger.debug(f"Cache write error: {e}")
    
    async def _rate_limit(self):
        """Implement intelligent rate limiting"""
        async with self.semaphore:
            now = time.time()
            
            # Clean old request times
            while self.request_times and now - self.request_times[0] > 1.0:
                self.request_times.popleft()
            
            # Check rate limit
            if len(self.request_times) >= self.rate_limit:
                sleep_time = 1.0 - (now - self.request_times[0])
                if sleep_time > 0:
                    logger.debug(f"Rate limiting: sleeping {sleep_time:.2f}s")
                    await asyncio.sleep(sleep_time)
                    self.stats["rate_limited"] += 1
            
            self.request_times.append(now)
    
    async def call_api_async(self, conversation: List[Dict], retries: int = 3) -> str:
        """Make async API call with caching and rate limiting"""
        self.stats["total_requests"] += 1
        
        # Check cache first
        cache_key = self._get_cache_key(conversation)
        cached_response = self._load_from_cache(cache_key)
        if cached_response:
            logger.debug("Cache hit")
            return cached_response
        
        # Apply rate limiting
        await self._rate_limit()
        
        # Prepare request
        payload = {
            "model": self.config["default_model"],
            "messages": conversation,
            "temperature": 0.7,
            "max_tokens": 4000
        }
        
        headers = {}
        if self.config.get("api_key"):
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
        
        # Retry logic with exponential backoff
        for attempt in range(retries):
            try:
                async with self.session.post(
                    self.config["endpoint"],
                    json=payload,
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        content = (data.get('content', '') or 
                                 data.get('choices', [{}])[0].get('message', {}).get('content', ''))
                        
                        # Cache successful response
                        self._save_to_cache(cache_key, content)
                        return content
                        
                    elif response.status == 429:  # Rate limited
                        retry_after = response.headers.get('Retry-After', '60')
                        wait_time = min(int(retry_after), 2 ** attempt)
                        logger.warning(f"API rate limited, waiting {wait_time}s")
                        await asyncio.sleep(wait_time)
                        
                    elif response.status in [500, 502, 503, 504]:  # Server errors
                        wait_time = min(2 ** attempt, 60)
                        logger.warning(f"Server error {response.status}, retrying in {wait_time}s")
                        await asyncio.sleep(wait_time)
                        
                    else:
                        error_text = await response.text()
                        logger.error(f"API error {response.status}: {error_text}")
                        break
                        
            except asyncio.TimeoutError:
                wait_time = min(2 ** attempt, 60)
                logger.warning(f"Request timeout, retrying in {wait_time}s")
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"Request error: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        self.stats["failed_requests"] += 1
        return f"Error: API request failed after {retries} attempts"
    
    def get_stats(self) -> Dict:
        """Get connection pool statistics"""
        cache_hit_rate = 0
        if self.stats["total_requests"] > 0:
            cache_hit_rate = self.stats["cache_hits"] / self.stats["total_requests"] * 100
            
        return {
            **self.stats,
            "cache_hit_rate_percent": round(cache_hit_rate, 2)
        }

# Batch processing for multiple API calls
class BatchAPIProcessor:
    """Processes multiple API calls in optimized batches"""
    
    def __init__(self, connection_pool: APIConnectionPool, batch_size: int = 20):
        self.connection_pool = connection_pool
        self.batch_size = batch_size
    
    async def process_conversations_batch(self, conversations: List[List[Dict]]) -> List[str]:
        """Process multiple conversations in batches"""
        results = []
        
        # Process in batches to avoid overwhelming the API
        for i in range(0, len(conversations), self.batch_size):
            batch = conversations[i:i + self.batch_size]
            
            # Create tasks for concurrent execution
            tasks = [
                self.connection_pool.call_api_async(conv) 
                for conv in batch
            ]
            
            # Execute batch concurrently
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle results and exceptions
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append(f"Error: {str(result)}")
                else:
                    results.append(result)
            
            # Small delay between batches
            if i + self.batch_size < len(conversations):
                await asyncio.sleep(0.1)
        
        return results

# Backward compatibility function
def call_api(conversation: List[Dict], config: Dict) -> str:
    """Synchronous wrapper for backward compatibility"""
    async def _async_call():
        async with APIConnectionPool(config) as pool:
            return await pool.call_api_async(conversation)
    
    # Run in event loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, create a task
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, _async_call())
                return future.result(timeout=300)
        else:
            return loop.run_until_complete(_async_call())
    except:
        # Fallback to new event loop
        return asyncio.run(_async_call())

# Utility functions
async def call_api_batch(conversations: List[List[Dict]], config: Dict, 
                        max_concurrent: int = 20) -> List[str]:
    """Process multiple conversations asynchronously"""
    async with APIConnectionPool(config, max_connections=max_concurrent) as pool:
        processor = BatchAPIProcessor(pool, batch_size=max_concurrent)
        return await processor.process_conversations_batch(conversations)

def estimate_api_cost(num_requests: int, cost_per_request: float = 0.01) -> float:
    """Estimate API costs for large workflows"""
    return num_requests * cost_per_request


==================================================
FILE: quick_setup.py
==================================================

#!/usr/bin/env python3
"""
Quick Setup Script for Async Framework
Run this first to set up everything
"""

import os
import sys
import subprocess
import json

def check_dependencies():
    """Check if required dependencies are installed"""
    print("üîç Checking dependencies...")
    
    required = ["aiohttp", "networkx"]
    missing = []
    
    for pkg in required:
        try:
            __import__(pkg)
            print(f"‚úÖ {pkg} - OK")
        except ImportError:
            missing.append(pkg)
            print(f"‚ùå {pkg} - Missing")
    
    return missing

def install_dependencies(packages):
    """Install missing dependencies"""
    if not packages:
        return True
    
    print(f"\nüì¶ Installing missing packages: {', '.join(packages)}")
    
    for pkg in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
            print(f"‚úÖ Installed {pkg}")
        except subprocess.CalledProcessError:
            print(f"‚ùå Failed to install {pkg}")
            return False
    
    return True

def create_config():
    """Create basic configuration file"""
    config_content = '''#!/usr/bin/env python3

import os

# Basic configuration for async framework
CONFIG = {
    "output_dir": "./async_outputs",
    "default_model": "deepseek/deepseek-chat:free",
    "api_key": "",  # Add your API key here
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
}

# Ensure output directory exists
os.makedirs(CONFIG["output_dir"], exist_ok=True)
'''
    
    if not os.path.exists("config.py"):
        with open("config.py", "w") as f:
            f.write(config_content)
        print("‚úÖ Created config.py")
    else:
        print("‚ÑπÔ∏è  config.py already exists")

def create_simple_test():
    """Create a simple test workflow"""
    test_workflow = {
        "steps": [
            {
                "agent": "hello_agent",
                "content": "Say hello and introduce yourself as an AI assistant",
                "output_format": {
                    "type": "json",
                    "schema": {
                        "greeting": "string",
                        "introduction": "string"
                    }
                }
            },
            {
                "agent": "follow_up",
                "content": "Based on the greeting, ask the user how you can help them today",
                "readFrom": ["hello_agent"]
            }
        ]
    }
    
    with open("simple_test.json", "w") as f:
        json.dump(test_workflow, f, indent=2)
    
    print("‚úÖ Created simple_test.json")

def main():
    """Main setup function"""
    print("üöÄ Async Framework Quick Setup")
    print("=" * 40)
    
    # Check dependencies
    missing = check_dependencies()
    
    # Install missing dependencies
    if missing:
        if not install_dependencies(missing):
            print("‚ùå Setup failed - could not install dependencies")
            return False
    
    # Create configuration
    create_config()
    
    # Create test workflow
    create_simple_test()
    
    # Create output directory
    os.makedirs("async_outputs", exist_ok=True)
    
    print("\nüéâ Setup complete!")
    print("\nüìñ Next steps:")
    print("1. Edit config.py and add your API key")
    print("2. Test the framework:")
    print("   python async_framework_main.py --workflow simple_test.json")
    print("3. Create example workflows:")
    print("   python async_framework_main.py --create-example")
    
    return True

if __name__ == "__main__":
    if main():
        sys.exit(0)
    else:
        sys.exit(1)


==================================================
FILE: running_the_platform_one.py
==================================================

#!/usr/bin/env python3
"""
Fixed Workflow Runner for Microsoft Intelligence Analysis
Properly handles complex workflows with real agent execution
"""

import asyncio
import json
import time
import os
import logging
from datetime import datetime
from typing import Dict, Any, List, Optional

# Setup detailed logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('workflow_execution.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger("workflow_runner")

def create_simplified_microsoft_workflow():
    """Create a simplified workflow that works with current framework"""
    
    workflow = {
        "workflow_name": "Microsoft_Intelligence_Simplified",
        "description": "Simplified Microsoft competitive analysis with real agents",
        "steps": [
            {
                "agent": "microsoft_stock_analyzer",
                "content": "Research Microsoft's current stock performance, market cap, and recent earnings. Find latest financial metrics and analyst ratings for MSFT stock.",
                "tools": ["research:combined_search"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "stock_price": "string",
                        "market_cap": "string",
                        "recent_earnings": "object",
                        "analyst_ratings": "array"
                    }
                }
            },
            
            {
                "agent": "azure_market_researcher", 
                "content": "Research Microsoft Azure's current market share in cloud computing. Compare with Amazon AWS and Google Cloud Platform market positions.",
                "tools": ["research:combined_search", "cite:sources"],
                "readFrom": ["microsoft_stock_analyzer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "azure_market_share": "string",
                        "aws_comparison": "object",
                        "gcp_comparison": "object",
                        "growth_trends": "array"
                    }
                }
            },

            {
                "agent": "office365_competitive_analyst",
                "content": "Analyze Microsoft Office 365 competitive position against Google Workspace. Research user adoption rates and feature comparisons.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["microsoft_stock_analyzer"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "office365_users": "string",
                        "google_workspace_threat": "object",
                        "feature_advantages": "array",
                        "market_trends": "array"
                    }
                }
            },

            {
                "agent": "microsoft_ai_strategy_analyst",
                "content": "Research Microsoft's AI strategy including Copilot, OpenAI partnership, and competition with Google's AI offerings.",
                "tools": ["research:combined_search", "research:generate_summary"],
                "readFrom": ["azure_market_researcher"],
                "output_format": {
                    "type": "json", 
                    "schema": {
                        "copilot_adoption": "string",
                        "openai_partnership": "object",
                        "google_ai_competition": "object",
                        "ai_revenue_impact": "string"
                    }
                }
            },

            {
                "agent": "google_competitive_threat_analyzer",
                "content": "Analyze Google as Microsoft's primary competitor. Research Google Cloud, Workspace, and AI initiatives that threaten Microsoft's market position.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["azure_market_researcher", "office365_competitive_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "google_cloud_growth": "object",
                        "workspace_vs_office365": "object",
                        "google_ai_threat": "object",
                        "strategic_moves": "array"
                    }
                }
            },

            {
                "agent": "amazon_aws_threat_analyzer",
                "content": "Analyze Amazon AWS as Microsoft Azure's biggest competitor. Research AWS market dominance and competitive strategies.",
                "tools": ["research:combined_search", "cite:sources"],
                "readFrom": ["azure_market_researcher"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "aws_market_dominance": "object",
                        "azure_vs_aws_features": "object",
                        "pricing_competition": "object",
                        "enterprise_adoption": "array"
                    }
                }
            },

            {
                "agent": "microsoft_financial_trend_analyzer",
                "content": "Analyze Microsoft's financial trends, revenue growth by segment, and future projections based on competitive landscape.",
                "tools": ["research:combined_search", "research:generate_summary"],
                "readFrom": ["microsoft_stock_analyzer", "azure_market_researcher", "office365_competitive_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "revenue_by_segment": "object",
                        "growth_projections": "object",
                        "competitive_risks": "array",
                        "investment_thesis": "string"
                    }
                }
            },

            {
                "agent": "tech_industry_trends_analyst",
                "content": "Research broader technology industry trends affecting Microsoft: cloud adoption, AI transformation, remote work, cybersecurity.",
                "tools": ["research:combined_search", "research:analyze_content"],
                "readFrom": ["microsoft_ai_strategy_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "cloud_adoption_trends": "object",
                        "ai_transformation_impact": "object",
                        "remote_work_influence": "object",
                        "cybersecurity_opportunities": "array"
                    }
                }
            },

            {
                "agent": "competitive_landscape_synthesizer",
                "content": "Synthesize all competitive intelligence into Microsoft's overall strategic position and recommendations.",
                "tools": ["research:generate_summary", "cite:format_citations"],
                "readFrom": ["google_competitive_threat_analyzer", "amazon_aws_threat_analyzer", "microsoft_ai_strategy_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "competitive_position": "string",
                        "key_strengths": "array",
                        "major_threats": "array",
                        "strategic_recommendations": "array"
                    }
                }
            },

            {
                "agent": "microsoft_investment_advisor",
                "content": "Create investment recommendation for Microsoft stock based on competitive analysis and market trends.",
                "tools": ["research:analyze_content", "cite:sources"],
                "readFrom": ["microsoft_financial_trend_analyzer", "competitive_landscape_synthesizer", "tech_industry_trends_analyst"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "investment_recommendation": "string",
                        "price_target": "string",
                        "key_catalysts": "array",
                        "risk_factors": "array",
                        "time_horizon": "string"
                    }
                }
            },

            {
                "agent": "executive_report_generator",
                "content": "Generate comprehensive executive report summarizing Microsoft's competitive position, opportunities, and strategic recommendations.",
                "tools": ["research:generate_summary", "cite:format_citations"],
                "readFrom": ["*"],
                "output_format": {
                    "type": "markdown",
                    "sections": [
                        "Executive Summary",
                        "Market Position Analysis", 
                        "Competitive Threats",
                        "Growth Opportunities",
                        "Financial Outlook",
                        "Strategic Recommendations"
                    ]
                }
            },

            {
                "agent": "workflow_analytics_monitor",
                "content": "Analyze workflow execution performance and data quality metrics.",
                "tools": ["research:analyze_content"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "execution_stats": "object",
                        "data_quality": "object",
                        "agent_performance": "array",
                        "recommendations": "array"
                    }
                }
            }
        ]
    }
    
    return workflow

async def execute_microsoft_workflow_fixed():
    """Execute Microsoft workflow with proper framework integration"""
    
    print("üöÄ Microsoft Intelligence Workflow - Fixed Version")
    print("=" * 60)
    
    # Create simplified workflow
    workflow = create_simplified_microsoft_workflow()
    
    print(f"üìä Workflow Overview:")
    print(f"   Total Agents: {len(workflow['steps'])}")
    print(f"   Workflow: {workflow['workflow_name']}")
    print()
    
    # Save workflow to file
    workflow_file = "microsoft_simplified.json"
    with open(workflow_file, 'w', encoding='utf-8') as f:
        json.dump(workflow, f, indent=2)
    
    print(f"‚úÖ Workflow saved to: {workflow_file}")
    
    # Import and configure the framework
    try:
        # First try to discover tools
        print("üîß Discovering available tools...")
        from tool_manager import tool_manager
        num_tools = tool_manager.discover_tools()
        print(f"   Discovered {num_tools} tools")
        
        # Show available research tools
        research_tools = [tool for tool in tool_manager.get_all_tools() if tool.startswith('research:')]
        print(f"   Available research tools: {len(research_tools)}")
        for tool in research_tools[:5]:  # Show first 5
            print(f"     - {tool}")
        
        # Get configuration
        from utils import get_config
        config = get_config()
        print(f"   API Endpoint: {config.get('endpoint', 'Not configured')}")
        print(f"   Model: {config.get('default_model', 'Not configured')}")
        
        # Check if API key is configured
        if not config.get('api_key'):
            print("‚ö†Ô∏è  Warning: No API key configured. Some tools may not work.")
        
        print("\nüöÄ Starting workflow execution...")
        start_time = time.time()
        
        # Use the AsyncToolIntegratedExecutor for better tool support
        from async_tool_integration import AsyncToolIntegratedExecutor
        
        async with AsyncToolIntegratedExecutor(config, max_concurrent=8) as executor:
            results = await executor.execute_workflow_with_tools(workflow['steps'])
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Analyze and display results
        print(f"\n{'='*60}")
        print("üéâ WORKFLOW EXECUTION COMPLETED")
        print(f"{'='*60}")
        
        if isinstance(results, dict) and "results" in results:
            exec_results = results["results"]
            total_agents = len(workflow['steps'])
            completed_agents = results.get("completed_count", 0)
            failed_agents = results.get("failed_count", 0)
            success_rate = (completed_agents / total_agents) * 100 if total_agents > 0 else 0
            
            print(f"üìà Execution Statistics:")
            print(f"   Total Agents: {total_agents}")
            print(f"   Completed Successfully: {completed_agents}")
            print(f"   Failed: {failed_agents}")
            print(f"   Success Rate: {success_rate:.1f}%")
            print(f"   Execution Time: {execution_time:.1f} seconds")
            print(f"   Average Time per Agent: {execution_time/total_agents:.2f} seconds")
            
            # Show successful agents with preview
            print(f"\n‚úÖ Successful Agents:")
            successful_count = 0
            for agent_name, result in exec_results.items():
                if not isinstance(result, dict) or "error" not in result:
                    successful_count += 1
                    print(f"   ‚úì {agent_name}")
                    
                    # Show preview of result
                    if isinstance(result, dict) and "content" in result:
                        content = str(result["content"])
                        preview = content[:100] + "..." if len(content) > 100 else content
                        print(f"     Preview: {preview}")
            
            # Show failed agents with errors
            print(f"\n‚ùå Failed Agents:")
            failed_count = 0
            for agent_name, result in exec_results.items():
                if isinstance(result, dict) and "error" in result:
                    failed_count += 1
                    error_msg = result.get('error', 'Unknown error')
                    print(f"   ‚ùå {agent_name}")
                    print(f"     Error: {error_msg}")
            
            if failed_count == 0:
                print("   üéâ No failed agents!")
            
            # Save detailed results
            timestamp = int(start_time)
            results_file = f"microsoft_results_{timestamp}.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"\nüìÅ Detailed results saved to: {results_file}")
            
            # Performance analysis
            if execution_time > 0:
                agents_per_second = completed_agents / execution_time
                print(f"\nüöÄ Performance Metrics:")
                print(f"   Agents per Second: {agents_per_second:.2f}")
                print(f"   Total Tools Discovered: {num_tools}")
                print(f"   Framework: AsyncToolIntegratedExecutor")
            
            # Data quality check
            research_agents = [name for name in exec_results.keys() if 'research' in name or 'analyst' in name]
            print(f"\nüìä Data Quality:")
            print(f"   Research Agents: {len(research_agents)}")
            print(f"   Synthesis Agents: {len([n for n in exec_results.keys() if 'synthesizer' in n or 'generator' in n])}")
            
            return results
            
        else:
            print(f"‚ùå Unexpected result format: {type(results)}")
            return results
            
    except ImportError as e:
        print(f"‚ùå Import error: {e}")
        print("üí° Make sure all framework files are available")
        return None
        
    except Exception as e:
        print(f"‚ùå Execution failed: {e}")
        import traceback
        traceback.print_exc()
        return None

async def test_individual_agent():
    """Test a single agent to verify tool integration"""
    
    print("\nüß™ Testing Individual Agent...")
    
    try:
        from tool_manager import tool_manager
        from async_tool_integration import AsyncToolIntegratedExecutor
        from utils import get_config
        
        config = get_config()
        
        # Test single agent
        test_agent = {
            "agent": "test_microsoft_researcher",
            "content": "Research Microsoft's current stock price and recent news.",
            "tools": ["research:combined_search"],
            "output_format": {
                "type": "json",
                "schema": {
                    "stock_info": "object",
                    "recent_news": "array"
                }
            }
        }
        
        print(f"Testing agent: {test_agent['agent']}")
        
        async with AsyncToolIntegratedExecutor(config, max_concurrent=1) as executor:
            result = await executor._execute_agent_with_tools(
                executor.create_task_from_step(test_agent, 0)
            )
        
        print(f"‚úÖ Test agent result: {type(result)}")
        if isinstance(result, dict):
            if "error" in result:
                print(f"‚ùå Test failed: {result['error']}")
            else:
                print(f"‚úÖ Test successful!")
                print(f"   Result keys: {list(result.keys())}")
        
        return result
        
    except Exception as e:
        print(f"‚ùå Individual agent test failed: {e}")
        return None

def main():
    """Main execution function"""
    
    print("üè¢ Microsoft Enterprise Intelligence Analysis")
    print("Fixed version with proper agent execution")
    print("=" * 60)
    
    # First test individual agent
    test_result = asyncio.run(test_individual_agent())
    
    if test_result and isinstance(test_result, dict) and "error" not in test_result:
        print("\n‚úÖ Individual agent test passed - proceeding with full workflow")
        
        # Run full workflow
        results = asyncio.run(execute_microsoft_workflow_fixed())
        
        if results:
            print("\nüéØ Workflow Analysis Complete!")
            print("Review the results file for detailed Microsoft competitive intelligence.")
        
    else:
        print("\n‚ùå Individual agent test failed - checking configuration...")
        print("\nüîß Troubleshooting Steps:")
        print("1. Verify API key is configured in utils.py or environment")
        print("2. Check that research tools are properly registered")
        print("3. Ensure network connectivity for API calls")
        print("4. Review tool_manager discovery logs")

if __name__ == "__main__":
    main()


==================================================
FILE: setup_optimized.py
==================================================

#!/usr/bin/env python3

import subprocess
import sys
import os

def install_requirements():
    """Install required packages for optimized framework"""
    
    requirements = [
        "aiohttp>=3.8.0",
        "aiofiles>=23.0.0", 
        "networkx>=3.0",
        "requests>=2.25.0"
    ]
    
    # Add uvloop for better performance on Unix systems
    if sys.platform != "win32":
        requirements.append("uvloop>=0.17.0")
    
    print("üîß Installing optimized framework requirements...")
    
    for req in requirements:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", req])
            print(f"‚úÖ Installed: {req}")
        except subprocess.CalledProcessError as e:
            print(f"‚ùå Failed to install {req}: {e}")
            return False
    
    return True

def create_directories():
    """Create necessary directories"""
    directories = [
        "agent_outputs",
        "api_cache", 
        "logs",
        "workflows",
        "optimized_results"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"‚úÖ Created directory: {directory}")

def create_optimized_config():
    """Create optimized configuration file"""
    
    config_content = '''#!/usr/bin/env python3

import os

# Optimized configuration for large-scale workflows
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "deepseek/deepseek-chat:free",  # Change this to your model
    "api_key": "your-api-key-here",  # Add your OpenRouter API key
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    
    # Optimization settings
    "max_concurrent_agents": 50,
    "enable_caching": True,
    "cache_dir": "./api_cache",
    "rate_limit_per_second": 20.0,
    "max_connections": 100,
    "enable_compression": True
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)
os.makedirs(CONFIG["cache_dir"], exist_ok=True)
'''
    
    with open("optimized_config.py", "w", encoding="utf-8") as f:
        f.write(config_content)
    
    print("‚úÖ Created optimized_config.py")

def create_test_workflow():
    """Create a test workflow for validation"""
    
    test_workflow = {
        "steps": [
            {
                "agent": "test_agent_1",
                "content": "Analyze the concept of artificial intelligence and provide a brief summary.",
                "output_format": {
                    "type": "json",
                    "schema": {
                        "summary": "string",
                        "key_points": "array",
                        "complexity": "string"
                    }
                }
            },
            {
                "agent": "test_agent_2", 
                "content": "Based on the AI analysis, suggest practical applications.",
                "readFrom": ["test_agent_1"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "applications": "array",
                        "feasibility": "string",
                        "timeline": "string"
                    }
                }
            },
            {
                "agent": "test_agent_3",
                "content": "Create a summary report of the AI analysis and applications.",
                "readFrom": ["*"],
                "output_format": {
                    "type": "markdown",
                    "sections": [
                        "Overview",
                        "Key Insights", 
                        "Applications",
                        "Conclusion"
                    ]
                }
            }
        ]
    }
    
    with open("test_workflow.json", "w", encoding="utf-8") as f:
        json.dump(test_workflow, f, indent=2)
    
    print("‚úÖ Created test_workflow.json")

def create_large_test_workflow():
    """Create a larger test workflow for performance testing"""
    
    # Generate 100 agents for stress testing
    steps = []
    
    # First batch: 10 independent analysis agents
    for i in range(10):
        steps.append({
            "agent": f"analyzer_{i+1}",
            "content": f"Analyze topic #{i+1}: Create analysis on different aspects of technology trends.",
            "output_format": {
                "type": "json",
                "schema": {
                    "analysis": "string",
                    "score": "number",
                    "recommendations": "array"
                }
            }
        })
    
    # Second batch: 20 agents that depend on first batch
    for i in range(20):
        dependency_idx = i % 10
        steps.append({
            "agent": f"synthesizer_{i+1}",
            "content": f"Synthesize insights from the analysis and provide recommendations.",
            "readFrom": [f"analyzer_{dependency_idx+1}"],
            "output_format": {
                "type": "json",
                "schema": {
                    "synthesis": "string",
                    "insights": "array",
                    "priority": "string"
                }
            }
        })
    
    # Third batch: 5 summary agents
    for i in range(5):
        start_idx = i * 4
        end_idx = start_idx + 4
        dependencies = [f"synthesizer_{j+1}" for j in range(start_idx, min(end_idx, 20))]
        
        steps.append({
            "agent": f"summarizer_{i+1}",
            "content": "Create comprehensive summary of synthesized insights.",
            "readFrom": dependencies,
            "output_format": {
                "type": "json",
                "schema": {
                    "summary": "string",
                    "key_findings": "array",
                    "conclusions": "string"
                }
            }
        })
    
    # Final agent: Overall report
    steps.append({
        "agent": "final_report",
        "content": "Generate comprehensive final report based on all analyses.",
        "readFrom": ["*"],
        "output_format": {
            "type": "markdown", 
            "sections": [
                "Executive Summary",
                "Key Findings",
                "Detailed Analysis",
                "Recommendations",
                "Conclusion"
            ]
        }
    })
    
    large_workflow = {"steps": steps}
    
    with open("large_test_workflow.json", "w", encoding="utf-8") as f:
        json.dump(large_workflow, f, indent=2)
    
    print(f"‚úÖ Created large_test_workflow.json with {len(steps)} agents")

def main():
    """Main setup function"""
    print("üöÄ Setting up Optimized Workflow Framework...")
    
    # Install requirements
    if not install_requirements():
        print("‚ùå Failed to install requirements")
        sys.exit(1)
    
    # Create directories
    create_directories()
    
    # Create configuration
    create_optimized_config()
    
    # Create test workflows
    import json
    create_test_workflow()
    create_large_test_workflow()
    
    print("\nüéâ Optimized framework setup complete!")
    print("\nüìñ Next Steps:")
    print("1. Edit optimized_config.py and add your API key")
    print("2. Test small workflow: python optimized_agent_runner.py --workflow test_workflow.json")
    print("3. Analyze large workflow: python optimized_agent_runner.py --workflow large_test_workflow.json --analyze")
    print("4. Test optimized execution: python optimized_agent_runner.py --workflow large_test_workflow.json --concurrent 20")
    print("\nüí° Performance Tips:")
    print("- Start with --concurrent 10-20 for testing")
    print("- Use --analyze to estimate costs before running large workflows")
    print("- Enable caching in config for repeated testing")
    print("- Monitor API rate limits and adjust accordingly")

if __name__ == "__main__":
    main()


==================================================
FILE: tool_manager.py
==================================================

#!/usr/bin/env python3

import os
import sys
import json
import inspect
import importlib.util
import glob
from typing import Dict, Any, List, Optional, Callable, Set
from functools import wraps
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("tool_manager")

class ToolManager:
    """Enhanced tool manager with automatic function discovery"""
    
    def __init__(self):
        self.tools = {}  # Tool registry
        self.imported_modules = set()  # Track imported modules
        self.namespace_prefixes = {}  # Map module to namespace prefix
        self.excluded_files = {
            'agent_runner.py', 'tool_manager.py', 'utils.py', 'config.py', 'call_api.py',
            '__init__.py', 'cli.py', 'async_executor.py', 'async_framework_main.py',
            'workflow_executor.py', 'workflow_state.py', 'enhanced_agent_runner.py'
        }
        self.excluded_functions = {
            'main', '__init__', 'setup', 'teardown', 'test_', 'debug_'
        }
    
    def discover_tools(self, directories: List[str] = None) -> int:
        """Automatically discover all Python modules and their functions in given directories"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        if directories is None:
            directories = [current_dir]
            
            # IMPORTANT: Add COMPONENT directory if it exists
            component_dir = os.path.join(current_dir, "COMPONENT")
            if os.path.exists(component_dir):
                directories.append(component_dir)
                logger.info(f"Added COMPONENT directory: {component_dir}")
            
            # Also include any other directories within current_dir
            for item in os.listdir(current_dir):
                item_path = os.path.join(current_dir, item)
                if (os.path.isdir(item_path) and 
                    not item.startswith('.') and 
                    item not in ['__pycache__', 'COMPONENT', 'async_outputs', 'agent_outputs']):
                    directories.append(item_path)
        
        total_tools = 0
        logger.info(f"Scanning directories: {directories}")
        
        # Find all Python files
        for directory in directories:
            if not os.path.exists(directory):
                logger.warning(f"Directory does not exist: {directory}")
                continue
                
            python_files = glob.glob(os.path.join(directory, "*.py"))
            logger.info(f"Found {len(python_files)} Python files in {directory}")
            
            for py_file in python_files:
                filename = os.path.basename(py_file)
                
                # Skip excluded files
                if filename in self.excluded_files:
                    logger.debug(f"Skipping excluded file: {filename}")
                    continue
                
                module_name = filename[:-3]  # Remove .py
                
                # Skip if already imported
                if module_name in self.imported_modules:
                    logger.debug(f"Module already imported: {module_name}")
                    continue
                
                try:
                    # Import the module
                    spec = importlib.util.spec_from_file_location(module_name, py_file)
                    if spec is None or spec.loader is None:
                        logger.warning(f"Could not create spec for {py_file}")
                        continue
                        
                    module = importlib.util.module_from_spec(spec)
                    
                    # Add the module's directory to sys.path temporarily
                    module_dir = os.path.dirname(py_file)
                    if module_dir not in sys.path:
                        sys.path.insert(0, module_dir)
                        added_to_path = True
                    else:
                        added_to_path = False
                    
                    try:
                        spec.loader.exec_module(module)
                        self.imported_modules.add(module_name)
                        
                        # Determine namespace prefix
                        if hasattr(module, 'TOOL_NAMESPACE'):
                            prefix = getattr(module, 'TOOL_NAMESPACE')
                        else:
                            # By default, use the module name
                            prefix = module_name
                            
                            # Special case: if it ends with _adapter, remove that part
                            if prefix.endswith("_adapter"):
                                prefix = prefix[:-8]  # Remove "_adapter"
                        
                        self.namespace_prefixes[module_name] = prefix
                        
                        # Priority 1: If the module has TOOL_REGISTRY, register those tools
                        registry_tools = 0
                        if hasattr(module, 'TOOL_REGISTRY'):
                            tool_registry = getattr(module, 'TOOL_REGISTRY')
                            if isinstance(tool_registry, dict):
                                for tool_id, tool_handler in tool_registry.items():
                                    full_tool_id = f"{prefix}:{tool_id}" if ':' not in tool_id else tool_id
                                    self.register_tool(full_tool_id, tool_handler)
                                    registry_tools += 1
                                    total_tools += 1
                                logger.info(f"Registered {registry_tools} tools from TOOL_REGISTRY in {module_name}")
                        
                        # Priority 2: Auto-discover functions that should be registered as tools
                        auto_tools = self._discover_module_tools(module, prefix)
                        total_tools += len(auto_tools)
                        
                        logger.info(f"Imported module {module_name} with {registry_tools} registry tools + {len(auto_tools)} auto-discovered functions")
                        
                    finally:
                        # Remove from sys.path if we added it
                        if added_to_path and module_dir in sys.path:
                            sys.path.remove(module_dir)
                    
                except Exception as e:
                    logger.error(f"Error importing {module_name} from {py_file}: {e}")
                    # Continue with other modules even if one fails
                    continue
        
        logger.info(f"üîß Total tools discovered: {total_tools}")
        return total_tools
    
    def _discover_module_tools(self, module, prefix: str) -> List[str]:
        """Discover and register tools from a module"""
        discovered_tools = []
        
        # Get all functions from the module
        for name, obj in inspect.getmembers(module):
            # Skip private functions, special methods, and non-functions
            if name.startswith('_') or not inspect.isfunction(obj):
                continue
            
            # Skip functions that start with excluded prefixes
            if any(name.startswith(excluded) for excluded in self.excluded_functions):
                continue
                
            # Skip functions that are already in TOOL_REGISTRY (avoid duplicates)
            if hasattr(module, 'TOOL_REGISTRY'):
                tool_registry = getattr(module, 'TOOL_REGISTRY')
                if isinstance(tool_registry, dict) and any(handler == obj for handler in tool_registry.values()):
                    continue
            
            # Check if function has a docstring (we only want documented functions)
            if obj.__doc__ and obj.__doc__.strip():
                # Create a tool ID based on the prefix and function name
                tool_id = f"{prefix}:{name}"
                self.register_tool(tool_id, obj)
                discovered_tools.append(tool_id)
                logger.debug(f"Auto-registered tool: {tool_id}")
        
        return discovered_tools
    
    def register_tool(self, tool_id: str, handler: Callable) -> None:
        """Register a function as a tool with flexible parameter handling"""
        @wraps(handler)
        def flexible_handler(**kwargs):
            try:
                # Get function signature
                sig = inspect.signature(handler)
                
                # If function takes **kwargs, pass everything
                if any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values()):
                    return handler(**kwargs)
                
                # Otherwise, filter kwargs to match function signature
                filtered_kwargs = {}
                for param_name, param in sig.parameters.items():
                    if param_name in kwargs:
                        filtered_kwargs[param_name] = kwargs[param_name]
                    elif param.default == param.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
                        # Required parameter missing
                        logger.warning(f"Required parameter '{param_name}' missing for tool {tool_id}")
                        return {"error": f"Required parameter '{param_name}' missing"}
                
                return handler(**filtered_kwargs)
                
            except Exception as e:
                logger.error(f"Tool execution failed for {tool_id}: {str(e)}")
                return {"error": f"Tool execution failed: {str(e)}"}
        
        self.tools[tool_id] = flexible_handler
        logger.debug(f"Registered tool: {tool_id}")
    
    def execute_tool(self, tool_id: str, **kwargs) -> Any:
        """Execute a registered tool"""
        if tool_id not in self.tools:
            # Try to find it by prefix and auto-load
            if ':' in tool_id:
                prefix = tool_id.split(':', 1)[0]
                
                # Look for modules that might contain this tool
                for module_name, module_prefix in self.namespace_prefixes.items():
                    if module_prefix == prefix and module_name not in self.imported_modules:
                        # Try to import module
                        self._try_load_module(module_name, prefix)
                        break
        
        if tool_id not in self.tools:
            available_tools = self.get_available_tools_by_prefix(tool_id.split(':', 1)[0] if ':' in tool_id else '')
            error_msg = f"Unknown tool: {tool_id}"
            if available_tools:
                error_msg += f". Available tools with similar prefix: {', '.join(available_tools[:5])}"
            return {"error": error_msg}
        
        try:
            logger.info(f"Executing tool {tool_id} with params: {list(kwargs.keys())}")
            result = self.tools[tool_id](**kwargs)
            logger.debug(f"Tool {tool_id} executed successfully")
            return result
        except Exception as e:
            logger.error(f"Error executing tool {tool_id}: {str(e)}")
            return {"error": str(e)}
    
    def _try_load_module(self, module_name: str, prefix: str):
        """Try to load a module that might contain tools"""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # Check multiple possible locations
            possible_paths = [
                os.path.join(current_dir, f"{module_name}.py"),
                os.path.join(current_dir, "COMPONENT", f"{module_name}.py"),
                os.path.join(current_dir, "tools", f"{module_name}.py"),
                os.path.join(current_dir, "adapters", f"{module_name}.py"),
            ]
            
            for module_path in possible_paths:
                if os.path.exists(module_path):
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        self._discover_module_tools(module, prefix)
                        self.imported_modules.add(module_name)
                        logger.info(f"Dynamically loaded module: {module_name}")
                        break
                        
        except Exception as e:
            logger.warning(f"Failed to dynamically load module {module_name}: {e}")
    
    def get_all_tools(self) -> List[str]:
        """Get a list of all registered tool IDs"""
        return sorted(list(self.tools.keys()))
    
    def get_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get tools by namespace prefix"""
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(f"{prefix}:")]
    
    def get_available_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get available tools that match a prefix"""
        if not prefix:
            return []
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(prefix)]
    
    def is_tool_available(self, tool_id: str) -> bool:
        """Check if a tool is available"""
        return tool_id in self.tools
    
    def get_tool_info(self, tool_id: str) -> Dict[str, Any]:
        """Get information about a specific tool"""
        if tool_id not in self.tools:
            return {"error": f"Tool not found: {tool_id}"}
        
        try:
            # Get the original function from the wrapper
            original_func = self.tools[tool_id].__wrapped__ if hasattr(self.tools[tool_id], '__wrapped__') else self.tools[tool_id]
            
            # Get function signature and docstring
            sig = inspect.signature(original_func)
            
            return {
                "tool_id": tool_id,
                "name": original_func.__name__,
                "module": original_func.__module__ if hasattr(original_func, '__module__') else 'unknown',
                "docstring": original_func.__doc__ or "No documentation available",
                "parameters": {
                    name: {
                        "type": str(param.annotation) if param.annotation != param.empty else "Any",
                        "default": str(param.default) if param.default != param.empty else "Required",
                        "kind": str(param.kind)
                    }
                    for name, param in sig.parameters.items()
                }
            }
        except Exception as e:
            return {"error": f"Could not get tool info: {str(e)}"}
    
    def list_tools_by_module(self) -> Dict[str, List[str]]:
        """List tools organized by module/namespace"""
        tools_by_module = {}
        for tool_id in self.tools.keys():
            if ':' in tool_id:
                prefix, _ = tool_id.split(':', 1)
                if prefix not in tools_by_module:
                    tools_by_module[prefix] = []
                tools_by_module[prefix].append(tool_id)
            else:
                if 'global' not in tools_by_module:
                    tools_by_module['global'] = []
                tools_by_module['global'].append(tool_id)
        
        return tools_by_module
    
    def get_stats(self) -> Dict[str, Any]:
        """Get statistics about the tool manager"""
        tools_by_module = self.list_tools_by_module()
        
        return {
            "total_tools": len(self.tools),
            "total_modules": len(self.imported_modules),
            "tools_by_module": {k: len(v) for k, v in tools_by_module.items()},
            "namespaces": list(self.namespace_prefixes.values())
        }

# Create a global tool manager instance
tool_manager = ToolManager()

# Additional utility functions that can be used as tools themselves
def list_all_tools(**kwargs) -> Dict[str, Any]:
    """List all available tools"""
    return {
        "tools": tool_manager.get_all_tools(),
        "stats": tool_manager.get_stats(),
        "by_module": tool_manager.list_tools_by_module()
    }

def get_tool_info(**kwargs) -> Dict[str, Any]:
    """Get information about a specific tool"""
    tool_id = kwargs.get('tool_id', '')
    if not tool_id:
        return {"error": "tool_id parameter is required"}
    
    return tool_manager.get_tool_info(tool_id)

def reload_tools(**kwargs) -> Dict[str, Any]:
    """Reload and rediscover all tools"""
    try:
        # Clear current tools and modules
        tool_manager.tools.clear()
        tool_manager.imported_modules.clear()
        tool_manager.namespace_prefixes.clear()
        
        # Rediscover tools
        total_tools = tool_manager.discover_tools()
        
        return {
            "success": True,
            "total_tools": total_tools,
            "stats": tool_manager.get_stats()
        }
    except Exception as e:
        return {"error": f"Failed to reload tools: {str(e)}"}

# Register these utility tools
tool_manager.register_tool("tools:list_all", list_all_tools)
tool_manager.register_tool("tools:get_info", get_tool_info)
tool_manager.register_tool("tools:reload", reload_tools)


==================================================
FILE: utils.py
==================================================

#!/usr/bin/env python3

import os
import json
import re
import datetime
from typing import Dict, Any, List, Optional, Set

def log_api(agent_name, prompt, response):
    """Simple function to log API calls and responses to a file."""
    logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file = os.path.join(logs_dir, f"{agent_name}_api.log")
    
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n==== API CALL AT {timestamp} ====\n")
        f.write(f"PROMPT:\n{prompt}")
        f.write(f"\n\n==== API RESPONSE ====\n")
        f.write(f"{response}")
        f.write("\n\n")

def extract_json_from_text(text: str) -> Dict[str, Any]:
    """Extract JSON from text, handling various formats"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # Try to find JSON within code blocks
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try to find anything that looks like a JSON object
    object_pattern = r'({[\s\S]*?})'
    match = re.search(object_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    return {"error": "Could not extract valid JSON from response", "text": text[:500]}

def extract_tool_calls(response_content: str) -> List[Dict[str, Any]]:
    """Extract all tool calls from a response"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    tool_calls = []
    
    matches = re.finditer(tool_usage_pattern, response_content, re.DOTALL)
    for match in matches:
        tool_name = match.group(1).strip()
        params_text = "{" + match.group(2) + "}"
        try:
            params = json.loads(params_text)
            tool_calls.append({
                "tool_name": tool_name,
                "params": params,
                "full_text": match.group(0)
            })
        except json.JSONDecodeError:
            continue
    
    return tool_calls

def process_single_tool_call(response_content):
    """Process a response that may contain a single tool call"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    match = re.search(tool_usage_pattern, response_content, re.DOTALL)
    
    if not match:
        return None, response_content
    
    tool_name = match.group(1).strip()
    params_text = "{" + match.group(2) + "}"
    
    try:
        params = json.loads(params_text)
        return {
            "tool_name": tool_name,
            "params": params
        }, response_content
    except json.JSONDecodeError:
        return None, response_content

def is_hashable(obj):
    """Check if an object can be used as a dictionary key"""
    try:
        hash(obj)
        return True
    except TypeError:
        return False

def get_config():
    """Get configuration or create default config"""
    try:
        from config import CONFIG
        return CONFIG
    except ImportError:
        try:
            from openrouter_config import CONFIG
            return CONFIG
        except ImportError:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            CONFIG = {
                "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "default_model": "deepseek/deepseek-chat:free",
                "output_dir": os.path.join(current_dir, "async_outputs")
            }
            os.makedirs(CONFIG["output_dir"], exist_ok=True)
            return CONFIG


==================================================
FILE: workflow_optimizer.py
==================================================

#!/usr/bin/env python3

import json
import networkx as nx
from typing import Dict, Any, List, Set, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger("workflow_optimizer")

@dataclass
class OptimizationMetrics:
    """Metrics for workflow optimization"""
    total_agents: int
    max_parallel: int
    estimated_batches: int
    estimated_time_seconds: float
    estimated_cost_usd: float
    memory_usage_mb: float
    critical_path_length: int

class WorkflowOptimizer:
    """Optimizes workflows for large-scale execution"""
    
    def __init__(self, avg_agent_time: float = 2.0, cost_per_request: float = 0.01):
        self.avg_agent_time = avg_agent_time  # seconds per agent
        self.cost_per_request = cost_per_request  # USD per API call
        self.avg_result_size = 5  # KB per result
    
    def analyze_workflow(self, workflow: List[Dict]) -> OptimizationMetrics:
        """Analyze workflow and provide optimization metrics"""
        
        # Build dependency graph
        graph = self._build_networkx_graph(workflow)
        
        # Calculate metrics
        total_agents = len(workflow)
        critical_path = self._find_critical_path(graph)
        critical_path_length = len(critical_path)
        
        # Estimate parallelization
        levels = self._calculate_execution_levels(graph)
        max_parallel = max(len(level) for level in levels) if levels else 1
        estimated_batches = len(levels)
        
        # Time estimation (critical path determines minimum time)
        estimated_time = critical_path_length * self.avg_agent_time
        
        # Cost estimation
        estimated_cost = total_agents * self.cost_per_request
        
        # Memory estimation
        memory_usage = total_agents * self.avg_result_size / 1024  # MB
        
        return OptimizationMetrics(
            total_agents=total_agents,
            max_parallel=max_parallel,
            estimated_batches=estimated_batches,
            estimated_time_seconds=estimated_time,
            estimated_cost_usd=estimated_cost,
            memory_usage_mb=memory_usage,
            critical_path_length=critical_path_length
        )
    
    def optimize_workflow(self, workflow: List[Dict], max_concurrent: int = 50) -> Dict[str, Any]:
        """Optimize workflow for execution"""
        
        # Analyze current workflow
        metrics = self.analyze_workflow(workflow)
        
        # Build recommendations
        recommendations = []
        optimizations = {}
        
        # Memory optimization
        if metrics.memory_usage_mb > 1000:  # > 1GB
            recommendations.append("Consider implementing result streaming for large workflows")
            optimizations["use_disk_cache"] = True
        
        # Parallelization optimization
        if metrics.max_parallel > max_concurrent:
            recommendations.append(f"Workflow can utilize {metrics.max_parallel} parallel agents, "
                                 f"but limited to {max_concurrent} by configuration")
            optimizations["recommended_concurrent"] = min(metrics.max_parallel, max_concurrent * 2)
        
        # Cost optimization
        if metrics.estimated_cost_usd > 10:
            recommendations.append("Consider result caching to reduce API costs")
            optimizations["enable_caching"] = True
        
        # Execution order optimization
        optimized_batches = self._create_optimal_batches(workflow, max_concurrent)
        
        return {
            "metrics": metrics,
            "recommendations": recommendations,
            "optimizations": optimizations,
            "execution_plan": {
                "batches": len(optimized_batches),
                "max_batch_size": max(len(batch) for batch in optimized_batches),
                "execution_order": optimized_batches
            }
        }
    
    def _build_networkx_graph(self, workflow: List[Dict]) -> nx.DiGraph:
        """Build NetworkX directed graph from workflow"""
        graph = nx.DiGraph()
        
        # Add nodes
        for step in workflow:
            agent_name = step.get("agent", "")
            if agent_name:
                graph.add_node(agent_name, **step)
        
        # Add edges (dependencies)
        for step in workflow:
            agent_name = step.get("agent", "")
            if not agent_name:
                continue
                
            read_from = step.get("readFrom", [])
            for ref in read_from:
                if isinstance(ref, str) and ref != "*":
                    if ref in graph.nodes:
                        graph.add_edge(ref, agent_name)
                elif ref == "*":
                    # Depends on all previous agents
                    for prev_step in workflow:
                        if prev_step.get("agent") == agent_name:
                            break
                        prev_agent = prev_step.get("agent")
                        if prev_agent and prev_agent in graph.nodes:
                            graph.add_edge(prev_agent, agent_name)
        
        return graph
    
    def _find_critical_path(self, graph: nx.DiGraph) -> List[str]:
        """Find the critical path (longest path) in the dependency graph"""
        try:
            # For DAG, find longest path
            if nx.is_directed_acyclic_graph(graph):
                topo_order = list(nx.topological_sort(graph))
                
                # Calculate longest path to each node
                distances = {node: 0 for node in graph.nodes}
                predecessors = {}
                
                for node in topo_order:
                    for successor in graph.successors(node):
                        if distances[node] + 1 > distances[successor]:
                            distances[successor] = distances[node] + 1
                            predecessors[successor] = node
                
                # Find node with maximum distance
                end_node = max(distances, key=distances.get)
                
                # Reconstruct path
                path = []
                current = end_node
                while current is not None:
                    path.append(current)
                    current = predecessors.get(current)
                
                return list(reversed(path))
            else:
                logger.warning("Graph contains cycles - using topological sort")
                return list(nx.topological_sort(graph))[:10]  # Limit to prevent issues
                
        except Exception as e:
            logger.error(f"Error finding critical path: {e}")
            return []
    
    def _calculate_execution_levels(self, graph: nx.DiGraph) -> List[List[str]]:
        """Calculate execution levels for parallel processing"""
        try:
            if not nx.is_directed_acyclic_graph(graph):
                logger.warning("Graph contains cycles - cannot calculate levels properly")
                return [[node] for node in graph.nodes]
            
            levels = []
            remaining_nodes = set(graph.nodes)
            
            while remaining_nodes:
                # Find nodes with no dependencies in remaining nodes
                current_level = []
                for node in remaining_nodes.copy():
                    predecessors = set(graph.predecessors(node))
                    if not predecessors.intersection(remaining_nodes):
                        current_level.append(node)
                        remaining_nodes.remove(node)
                
                if current_level:
                    levels.append(current_level)
                else:
                    # Handle remaining nodes (possible cycle)
                    levels.append(list(remaining_nodes))
                    break
            
            return levels
            
        except Exception as e:
            logger.error(f"Error calculating execution levels: {e}")
            return [[node] for node in graph.nodes]
    
    def _create_optimal_batches(self, workflow: List[Dict], max_concurrent: int) -> List[List[str]]:
        """Create optimal execution batches"""
        graph = self._build_networkx_graph(workflow)
        levels = self._calculate_execution_levels(graph)
        
        # Split large levels into smaller batches
        batches = []
        for level in levels:
            if len(level) <= max_concurrent:
                batches.append(level)
            else:
                # Split level into smaller batches
                for i in range(0, len(level), max_concurrent):
                    batch = level[i:i + max_concurrent]
                    batches.append(batch)
        
        return batches
    
    def generate_optimization_report(self, workflow_file: str, max_concurrent: int = 50) -> str:
        """Generate a comprehensive optimization report"""
        
        # Load workflow
        with open(workflow_file, 'r', encoding='utf-8') as f:
            workflow_data = json.load(f)
        
        if isinstance(workflow_data, dict) and "steps" in workflow_data:
            workflow = workflow_data["steps"]
        else:
            workflow = workflow_data
        
        # Analyze and optimize
        optimization = self.optimize_workflow(workflow, max_concurrent)
        metrics = optimization["metrics"]
        
        # Generate report
        report = f"""
# Workflow Optimization Report

## Workflow Overview
- **Total Agents**: {metrics.total_agents}
- **Critical Path Length**: {metrics.critical_path_length} agents
- **Maximum Parallelization**: {metrics.max_parallel} concurrent agents

## Performance Estimates
- **Estimated Execution Time**: {metrics.estimated_time_seconds:.1f} seconds ({metrics.estimated_time_seconds/60:.1f} minutes)
- **Estimated Cost**: ${metrics.estimated_cost_usd:.2f}
- **Memory Usage**: {metrics.memory_usage_mb:.1f} MB
- **Execution Batches**: {metrics.estimated_batches}

## Optimization Recommendations
"""
        
        for rec in optimization["recommendations"]:
            report += f"- {rec}\n"
        
        report += f"""
## Execution Plan
- **Number of Batches**: {optimization['execution_plan']['batches']}
- **Largest Batch Size**: {optimization['execution_plan']['max_batch_size']} agents
- **Recommended Concurrency**: {optimization['optimizations'].get('recommended_concurrent', max_concurrent)}

## Optimization Settings
"""
        
        for key, value in optimization["optimizations"].items():
            report += f"- **{key}**: {value}\n"
        
        return report

# Utility functions
def analyze_workflow_file(workflow_file: str) -> OptimizationMetrics:
    """Quick analysis of a workflow file"""
    optimizer = WorkflowOptimizer()
    
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow_data = json.load(f)
    
    if isinstance(workflow_data, dict) and "steps" in workflow_data:
        workflow = workflow_data["steps"]
    else:
        workflow = workflow_data
    
    return optimizer.analyze_workflow(workflow)

def optimize_workflow_file(workflow_file: str, max_concurrent: int = 50) -> Dict[str, Any]:
    """Optimize a workflow file"""
    optimizer = WorkflowOptimizer()
    
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow_data = json.load(f)
    
    if isinstance(workflow_data, dict) and "steps" in workflow_data:
        workflow = workflow_data["steps"]
    else:
        workflow = workflow_data
    
    return optimizer.optimize_workflow(workflow, max_concurrent)


==================================================
SUMMARY: Processed 18 Python files
Output saved to: all_python_files.txt
