Python Files Content Aggregation
Generated on 2025-07-15 at 14:05:04
==================================================

==================================================
FILE: async_dynamic_agents.py
==================================================

#!/usr/bin/env python3

import asyncio
import json
from typing import Dict, Any, List, Optional
import logging

logger = logging.getLogger("async_dynamic_agents")

class AsyncDynamicAgentExecutor:
    """Async executor with dynamic agent support"""
    
    def __init__(self, base_executor):
        self.base_executor = base_executor
        self.dynamic_results = {}
    
    async def execute_dynamic_workflow(self, workflow: List[Dict[str, Any]], 
                                     data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with dynamic agent support"""
        
        results = {}
        
        for step_index, step in enumerate(workflow):
            step_type = step.get("type", "agent")
            
            if step_type == "dynamic":
                # Handle dynamic agent
                dynamic_result = await self._execute_dynamic_agent(step, results, data_file)
                
                agent_name = step.get("agent")
                if agent_name:
                    results[agent_name] = dynamic_result
                
                # Check if dynamic agent selected an action
                action_key = await self._extract_action_from_result(dynamic_result)
                
                if action_key:
                    action_name = f"{agent_name}_action"
                    results[action_name] = action_key
                    logger.info(f"🔍 Dynamic agent selected action: {action_key}")
                    
                    # Execute the selected action
                    await self._execute_dynamic_action(step, action_key, results, data_file)
            
            else:
                # Handle regular agent
                if hasattr(self.base_executor, '_execute_step_async'):
                    result = await self.base_executor._execute_step_async(step, data_file)
                else:
                    # Fallback to regular execution
                    result = await self._execute_regular_agent(step, results, data_file)
                
                agent_name = step.get("agent")
                if agent_name:
                    results[agent_name] = result
        
        return results
    
    async def _execute_dynamic_agent(self, step: Dict[str, Any], 
                                   current_results: Dict[str, Any], 
                                   data_file: str = None) -> Dict[str, Any]:
        """Execute dynamic agent that can choose its action"""
        
        agent_name = step.get("agent")
        initial_prompt = step.get("initial_prompt", "")
        output_format = step.get("output_format", {})
        required_tools = step.get("tools", [])
        
        # Collect references for dynamic agent
        references = self._collect_references(step.get("readFrom", []), current_results)
        
        # Create enhanced prompt for dynamic decision making
        enhanced_prompt = initial_prompt
        
        # Add available actions information
        actions = step.get("actions", {})
        if actions:
            enhanced_prompt += "\n\n### Available Actions:\n"
            for action_key, action_info in actions.items():
                action_desc = action_info.get("description", action_key)
                enhanced_prompt += f"- **{action_key}**: {action_desc}\n"
            
            enhanced_prompt += "\nBased on the context and available information, select the most appropriate action."
        
        # Add reference information
        if references:
            enhanced_prompt += "\n\n### Reference Information:\n"
            for ref_name, ref_content in references.items():
                enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                if isinstance(ref_content, dict):
                    enhanced_prompt += json.dumps(ref_content, indent=2)
                else:
                    enhanced_prompt += str(ref_content)
        
        # Execute the dynamic agent
        result = await self._call_agent_async(
            agent_name=agent_name,
            prompt=enhanced_prompt,
            output_format=output_format,
            required_tools=required_tools,
            file_path=data_file if step.get("file") else None
        )
        
        return result
    
    async def _execute_dynamic_action(self, dynamic_step: Dict[str, Any], 
                                    action_key: str, 
                                    current_results: Dict[str, Any],
                                    data_file: str = None):
        """Execute the action selected by dynamic agent"""
        
        actions = dynamic_step.get("actions", {})
        
        if action_key not in actions:
            logger.warning(f"Selected action '{action_key}' not found in available actions")
            return
        
        action = actions[action_key]
        next_agent_name = action.get("agent")
        
        if not next_agent_name:
            logger.warning(f"Action '{action_key}' has no agent specified")
            return
        
        action_content = action.get("content", "")
        action_output_format = action.get("output_format")
        action_tools = action.get("tools", [])
        
        # Collect references for the action
        action_refs = self._collect_references(action.get("readFrom", []), current_results)
        
        # Execute the action agent
        action_result = await self._call_agent_async(
            agent_name=next_agent_name,
            prompt=action_content,
            output_format=action_output_format,
            required_tools=action_tools,
            file_path=data_file if action.get("file") else None,
            references=action_refs
        )
        
        # Store action result
        current_results[next_agent_name] = action_result
        logger.info(f"✅ Dynamic action '{action_key}' executed by {next_agent_name}")
    
    async def _extract_action_from_result(self, result: Dict[str, Any]) -> Optional[str]:
        """Extract selected action from dynamic agent result"""
        
        if not isinstance(result, dict):
            return None
        
        # Try various possible action keys
        for key in ["response_action", "action", "selected_focus", "selected_action", "choice"]:
            if key in result:
                action_value = result[key]
                if isinstance(action_value, str):
                    return action_value
                elif isinstance(action_value, dict) and "action" in action_value:
                    return action_value["action"]
        
        # Try to parse from content
        content = result.get("content", "")
        if isinstance(content, str):
            # Look for action indicators in content
            import re
            action_patterns = [
                r"selected action:\s*([a-zA-Z_][a-zA-Z0-9_]*)",
                r"action:\s*([a-zA-Z_][a-zA-Z0-9_]*)",
                r"choose:\s*([a-zA-Z_][a-zA-Z0-9_]*)"
            ]
            
            for pattern in action_patterns:
                match = re.search(pattern, content, re.IGNORECASE)
                if match:
                    return match.group(1)
        
        return None
    
    async def _call_agent_async(self, agent_name: str, prompt: str, 
                              output_format: Optional[Dict[str, Any]] = None,
                              required_tools: List[str] = None,
                              file_path: Optional[str] = None,
                              references: Optional[Dict[str, Any]] = None) -> Dict[str, Any]:
        """Call agent asynchronously with full feature support"""
        
        # Build enhanced prompt
        enhanced_prompt = prompt
        
        # Add reference information
        if references:
            enhanced_prompt += "\n\n### Reference Information:\n"
            for ref_name, ref_content in references.items():
                enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                if isinstance(ref_content, dict):
                    enhanced_prompt += json.dumps(ref_content, indent=2)
                else:
                    enhanced_prompt += str(ref_content)
        
        # Add file content if provided
        if file_path:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_content = f.read()
                    if len(file_content) > 10000:
                        file_content = file_content[:10000]
                    enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
            except Exception as e:
                logger.warning(f"Could not read file {file_path}: {e}")
        
        # Add tool information
        if required_tools:
            enhanced_prompt += f"\n\nYou have access to these tools: {', '.join(required_tools)}"
        
        # Add format instructions
        if output_format:
            output_type = output_format.get("type", "text")
            schema = output_format.get("schema")
            
            if output_type == "json" and schema:
                enhanced_prompt += "\n\n### Response Format Instructions:\n"
                enhanced_prompt += "You MUST respond with a valid JSON object exactly matching this schema:\n"
                enhanced_prompt += f"```json\n{json.dumps(schema, indent=2)}\n```\n"
                enhanced_prompt += "\nReturning properly formatted JSON is CRITICAL."
        
        # Build conversation
        system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
        conversation = [
            {"role": "system", "content": system_message},
            {"role": "user", "content": enhanced_prompt}
        ]
        
        # Use base executor's API call method
        if hasattr(self.base_executor, '_call_api_async'):
            response = await self.base_executor._call_api_async(conversation)
        else:
            # Fallback - simulate async call
            await asyncio.sleep(0.1)
            response = f"Simulated response from {agent_name}"
        
        # Process response based on output format
        if output_format and output_format.get("type") == "json":
            return self._extract_json_from_response(response)
        else:
            return {"content": response}
    
    async def _execute_regular_agent(self, step: Dict[str, Any], 
                                   current_results: Dict[str, Any],
                                   data_file: str = None) -> Dict[str, Any]:
        """Execute regular (non-dynamic) agent"""
        
        agent_name = step.get("agent")
        content = step.get("content", "")
        output_format = step.get("output_format", {})
        required_tools = step.get("tools", [])
        
        # Collect references
        references = self._collect_references(step.get("readFrom", []), current_results)
        
        # Execute agent
        return await self._call_agent_async(
            agent_name=agent_name,
            prompt=content,
            output_format=output_format,
            required_tools=required_tools,
            file_path=data_file if step.get("file") else None,
            references=references
        )
    
    def _collect_references(self, read_from: List[str], current_results: Dict[str, Any]) -> Dict[str, Any]:
        """Collect references from current results"""
        references = {}
        
        for ref in read_from:
            if ref == "*":
                # Include all current results
                references.update(current_results)
            elif isinstance(ref, str) and ref in current_results:
                references[ref] = current_results[ref]
        
        return references
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """Extract JSON from response with fallback"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    return {"error": "Could not parse JSON", "raw_response": response}
            else:
                return {"error": "No JSON found", "raw_response": response}

# Integration class that combines all async features
class FullAsyncWorkflowExecutor:
    """Complete async workflow executor with all features"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50):
        self.config = config
        self.max_concurrent = max_concurrent
        
        # Initialize sub-executors
        from async_executor import AsyncWorkflowExecutor
        self.base_executor = AsyncWorkflowExecutor(config, max_concurrent)
        
        # Add tool integration if available
        try:
            from async_tool_integration import AsyncToolIntegratedExecutor
            self.tool_executor = AsyncToolIntegratedExecutor(config, max_concurrent)
        except ImportError as e:
            logger.warning(f"Tool integration not available: {e}")
            self.tool_executor = None
        
        # Add flow control if available
        try:
            from tool_manager import tool_manager
            from async_flow_control import AsyncFlowControlExecutor
            self.flow_executor = AsyncFlowControlExecutor(tool_manager, max_concurrent)
        except ImportError as e:
            logger.warning(f"Flow control not available: {e}")
            self.flow_executor = None
        
        # Add dynamic agent support
        self.dynamic_executor = AsyncDynamicAgentExecutor(self.base_executor)
    
    async def execute_complete_workflow(self, workflow: List[Dict[str, Any]], 
                                      data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with all async features enabled"""
        
        logger.info("🚀 Starting complete async workflow execution")
        
        # Analyze workflow to determine features needed
        features_needed = self._analyze_workflow_features(workflow)
        logger.info(f"Detected features: {', '.join(features_needed)}")
        
        try:
            # Choose appropriate executor based on features
            if "dynamic_agents" in features_needed:
                if "flow_control" in features_needed and self.flow_executor:
                    # Complex workflow with both dynamic agents and flow control
                    return await self._execute_complex_workflow(workflow, data_file)
                else:
                    # Dynamic agents only
                    return await self.dynamic_executor.execute_dynamic_workflow(workflow, data_file)
            
            elif "flow_control" in features_needed and self.flow_executor:
                # Flow control workflow
                return await self.flow_executor.execute_workflow_with_flow_control(workflow, data_file)
            
            elif "tools" in features_needed and self.tool_executor:
                # Tool-enabled workflow
                return await self.tool_executor.execute_workflow_with_tools(workflow, data_file)
            
            else:
                # Basic async workflow
                async with self.base_executor as executor:
                    return await executor.execute_workflow_async(workflow, data_file)
        
        except Exception as e:
            logger.error(f"❌ Complete async workflow execution failed: {e}")
            return {"error": str(e), "success": False}
    
    async def _execute_complex_workflow(self, workflow: List[Dict[str, Any]], 
                                      data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with multiple advanced features"""
        
        # This would combine dynamic agents, flow control, and tools
        # For now, prioritize dynamic agents with fallback to flow control
        
        try:
            return await self.dynamic_executor.execute_dynamic_workflow(workflow, data_file)
        except Exception as e:
            logger.warning(f"Dynamic execution failed, falling back to flow control: {e}")
            if self.flow_executor:
                return await self.flow_executor.execute_workflow_with_flow_control(workflow, data_file)
            else:
                raise e
    
    def _analyze_workflow_features(self, workflow: List[Dict[str, Any]]) -> List[str]:
        """Analyze workflow to determine what features are needed"""
        
        features = []
        
        for step in workflow:
            # Check for dynamic agents
            if step.get("type") == "dynamic":
                features.append("dynamic_agents")
            
            # Check for flow control
            if step.get("type") in ["loop", "conditional", "parallel", "state"]:
                features.append("flow_control")
            
            # Check for tools
            if step.get("tools"):
                features.append("tools")
        
        return list(set(features))  # Remove duplicates

# Utility function for easy usage
async def execute_advanced_workflow(workflow_file: str, config: Dict[str, Any], 
                                  data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Execute workflow with all advanced async features"""
    
    # Load workflow
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow = json.load(f)
    
    if isinstance(workflow, dict) and "steps" in workflow:
        workflow = workflow["steps"]
    
    # Execute with full feature support
    executor = FullAsyncWorkflowExecutor(config, max_concurrent)
    return await executor.execute_complete_workflow(workflow, data_file)


==================================================
FILE: async_executor.py
==================================================

#!/usr/bin/env python3

import asyncio
import aiohttp
import json
import time
from typing import Dict, Any, List, Optional, Set
from dataclasses import dataclass
from collections import defaultdict, deque
import logging

logger = logging.getLogger("async_executor")

@dataclass
class AgentTask:
    """Represents a single agent execution task"""
    agent_name: str
    prompt: str
    file_path: Optional[str] = None
    output_format: Optional[Dict[str, Any]] = None
    references: Optional[Dict[str, Any]] = None
    required_tools: List[str] = None
    dependencies: Set[str] = None
    priority: int = 0
    step_index: int = 0  # Add step tracking

class AsyncWorkflowExecutor:
    """High-performance async workflow executor with enhanced logging"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50, 
                 max_connections: int = 100, rate_limit: float = 10.0):
        self.config = config
        self.max_concurrent = max_concurrent
        self.max_connections = max_connections
        self.rate_limit = rate_limit  # requests per second
        
        # Rate limiting
        self.request_times = deque()
        self.rate_limiter = asyncio.Semaphore(max_concurrent)
        
        # Results storage
        self.results = {}
        self.completed_agents = set()
        self.failed_agents = set()
        
        # Enhanced tracking
        self.workflow_start_time = None
        self.step_timings = {}
        self.current_batch = 0
        self.total_batches = 0
        
        # Connection management - initialize as None
        self.session = None
        self.connector = None
        
        # Session management state
        self._session_initialized = False
        self._session_lock = asyncio.Lock()
    
    async def __aenter__(self):
        """Async context manager entry"""
        await self._ensure_session()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Async context manager exit"""
        await self._close_session()
    
    async def _ensure_session(self):
        """Ensure HTTP session is properly initialized"""
        async with self._session_lock:
            if not self._session_initialized:
                try:
                    self.connector = aiohttp.TCPConnector(
                        limit=self.max_connections,
                        limit_per_host=20,
                        keepalive_timeout=30,
                        enable_cleanup_closed=True
                    )
                    
                    timeout = aiohttp.ClientTimeout(total=120, connect=10)
                    self.session = aiohttp.ClientSession(
                        connector=self.connector,
                        timeout=timeout,
                        headers={'Content-Type': 'application/json'}
                    )
                    self._session_initialized = True
                    logger.info("✅ HTTP session initialized successfully")
                except Exception as e:
                    logger.error(f"❌ Failed to initialize HTTP session: {e}")
                    raise
    
    async def _close_session(self):
        """Properly close HTTP session"""
        async with self._session_lock:
            if self.session:
                try:
                    await self.session.close()
                    logger.debug("Session closed")
                except Exception as e:
                    logger.warning(f"Error closing session: {e}")
                finally:
                    self.session = None
            
            if self.connector:
                try:
                    await self.connector.close()
                    logger.debug("Connector closed")
                except Exception as e:
                    logger.warning(f"Error closing connector: {e}")
                finally:
                    self.connector = None
            
            self._session_initialized = False
    
    async def _rate_limit(self):
        """Implement rate limiting"""
        async with self.rate_limiter:
            now = time.time()
            
            # Remove old requests outside the window
            while self.request_times and now - self.request_times[0] > 1.0:
                self.request_times.popleft()
            
            # Check if we need to wait
            if len(self.request_times) >= self.rate_limit:
                sleep_time = 1.0 - (now - self.request_times[0])
                if sleep_time > 0:
                    await asyncio.sleep(sleep_time)
            
            self.request_times.append(now)
    
    async def _call_api_async(self, conversation: List[Dict], retries: int = 3, agent_name: str = "unknown") -> str:
        """Async API call with retries and rate limiting - enhanced with agent context"""
        # Ensure session is available
        await self._ensure_session()
        
        if not self.session:
            logger.error(f"❌ Session not available for API call - Agent: {agent_name}")
            return "Error: Session not initialized"
        
        await self._rate_limit()
        
        payload = {
            "model": self.config["default_model"],
            "messages": conversation
        }
        
        headers = {}
        if self.config.get("api_key"):
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
        
        for attempt in range(retries):
            try:
                async with self.session.post(
                    self.config["endpoint"],
                    json=payload,
                    headers=headers
                ) as response:
                    if response.status == 200:
                        data = await response.json()
                        content = (data.get('content', '') or 
                                 data.get('choices', [{}])[0].get('message', {}).get('content', ''))
                        return content
                    elif response.status == 429:  # Rate limited
                        wait_time = 2 ** attempt
                        logger.warning(f"⏱️ Agent '{agent_name}' rate limited, waiting {wait_time}s (attempt {attempt + 1}/{retries})")
                        await asyncio.sleep(wait_time)
                    else:
                        error_text = await response.text()
                        logger.error(f"❌ Agent '{agent_name}' API error {response.status}: {error_text}")
                        
            except asyncio.TimeoutError:
                logger.warning(f"⏱️ Agent '{agent_name}' API timeout (attempt {attempt + 1}/{retries})")
            except Exception as e:
                logger.error(f"❌ Agent '{agent_name}' API error (attempt {attempt + 1}/{retries}): {e}")
            
            if attempt < retries - 1:
                await asyncio.sleep(2 ** attempt)
        
        logger.error(f"❌ Agent '{agent_name}' failed after {retries} attempts")
        return f"Error: API request failed for {agent_name} after {retries} attempts"
    
    async def _execute_agent_async(self, task: AgentTask) -> Dict[str, Any]:
        """Execute a single agent asynchronously with enhanced logging"""
        step_start_time = time.time()
        
        # Log agent start with context
        logger.info(f"🚀 Starting Agent: '{task.agent_name}' (Step {task.step_index + 1})")
        if task.dependencies:
            logger.debug(f"   📋 Dependencies: {', '.join(task.dependencies)}")
        if task.required_tools:
            logger.debug(f"   🔧 Tools: {', '.join(task.required_tools)}")
        
        try:
            # Build the prompt
            enhanced_prompt = task.prompt
            
            # Add reference information
            if task.references:
                enhanced_prompt += "\n\n### Reference Information:\n"
                for ref_name, ref_content in task.references.items():
                    enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                    if isinstance(ref_content, dict):
                        enhanced_prompt += json.dumps(ref_content, indent=2)
                    else:
                        enhanced_prompt += str(ref_content)
                
                logger.debug(f"   📊 Using {len(task.references)} reference(s): {', '.join(task.references.keys())}")
            
            # Add file content if provided
            if task.file_path:
                try:
                    with open(task.file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                        original_size = len(file_content)
                        if len(file_content) > 10000:
                            file_content = file_content[:10000]
                        enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
                        
                    logger.debug(f"   📂 File loaded: {task.file_path} ({original_size} chars, {original_size//1024}KB)")
                except Exception as e:
                    logger.warning(f"   ⚠️ Could not read file {task.file_path}: {e}")
            
            # Add format instructions
            if task.output_format:
                output_type = task.output_format.get("type", "text")
                schema = task.output_format.get("schema")
                
                if output_type == "json" and schema:
                    enhanced_prompt += "\n\n### Response Format Instructions:\n"
                    enhanced_prompt += "You MUST respond with a valid JSON object exactly matching this schema:\n"
                    enhanced_prompt += f"```json\n{json.dumps(schema, indent=2)}\n```\n"
                    enhanced_prompt += "\nReturning properly formatted JSON is CRITICAL."
                
                logger.debug(f"   📝 Output format: {output_type}")
            
            # Build conversation
            system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
            conversation = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Make API call with agent context
            logger.debug(f"   🌐 Making API call for '{task.agent_name}'...")
            response = await self._call_api_async(conversation, agent_name=task.agent_name)
            
            # Process response based on output format
            if task.output_format and task.output_format.get("type") == "json":
                try:
                    result = json.loads(response)
                    logger.debug(f"   ✅ JSON parsing successful for '{task.agent_name}'")
                except json.JSONDecodeError:
                    # Try to extract JSON from response
                    import re
                    json_match = re.search(r'\{.*\}', response, re.DOTALL)
                    if json_match:
                        try:
                            result = json.loads(json_match.group())
                            logger.debug(f"   ✅ JSON extracted from response for '{task.agent_name}'")
                        except:
                            result = {"error": "Could not parse JSON", "raw_response": response}
                            logger.warning(f"   ⚠️ JSON extraction failed for '{task.agent_name}'")
                    else:
                        result = {"error": "No JSON found", "raw_response": response}
                        logger.warning(f"   ⚠️ No JSON found in response for '{task.agent_name}'")
            else:
                result = {"content": response}
            
            # Log completion with timing
            step_duration = time.time() - step_start_time
            self.step_timings[task.agent_name] = step_duration
            
            logger.info(f"✅ Completed Agent: '{task.agent_name}' in {step_duration:.1f}s")
            return result
            
        except Exception as e:
            step_duration = time.time() - step_start_time
            error_result = {"error": f"Agent execution failed: {str(e)}"}
            logger.error(f"❌ Failed Agent: '{task.agent_name}' after {step_duration:.1f}s - {e}")
            return error_result
    
    def _build_dependency_graph(self, workflow: List[Dict]) -> Dict[str, Set[str]]:
        """Build dependency graph from workflow"""
        dependencies = {}
        
        for step in workflow:
            agent_name = step.get("agent", "")
            deps = set()
            
            # Check readFrom dependencies
            read_from = step.get("readFrom", [])
            for ref in read_from:
                if isinstance(ref, str) and ref != "*":
                    deps.add(ref)
                elif ref == "*":
                    # Depends on all previous agents
                    for prev_step in workflow:
                        if prev_step.get("agent") == agent_name:
                            break
                        prev_agent = prev_step.get("agent")
                        if prev_agent:
                            deps.add(prev_agent)
            
            dependencies[agent_name] = deps
        
        return dependencies
    
    def _create_execution_batches(self, workflow: List[Dict]) -> List[List[AgentTask]]:
        """Create batches of agents that can be executed in parallel"""
        dependencies = self._build_dependency_graph(workflow)
        
        # Create tasks with step indices
        tasks = {}
        for step_index, step in enumerate(workflow):
            agent_name = step.get("agent", "")
            if agent_name:
                task = AgentTask(
                    agent_name=agent_name,
                    prompt=step.get("content", ""),
                    file_path=step.get("file"),
                    output_format=step.get("output_format"),
                    required_tools=step.get("tools", []),
                    dependencies=dependencies.get(agent_name, set()),
                    step_index=step_index  # Add step tracking
                )
                tasks[agent_name] = task
        
        # Create batches using topological sort
        batches = []
        completed = set()
        remaining = set(tasks.keys())
        
        while remaining:
            # Find tasks with no pending dependencies
            ready = []
            for agent_name in remaining:
                task = tasks[agent_name]
                if task.dependencies.issubset(completed):
                    ready.append(task)
            
            if not ready:
                # Circular dependency or error - add remaining tasks
                logger.warning("⚠️ Possible circular dependency detected")
                ready = [tasks[name] for name in remaining]
            
            # Create batch (limit size for memory management)
            batch_size = min(len(ready), self.max_concurrent)
            batch = ready[:batch_size]
            batches.append(batch)
            
            # Update completed and remaining
            for task in batch:
                completed.add(task.agent_name)
                remaining.discard(task.agent_name)
        
        return batches
    
    async def execute_workflow_async(self, workflow: List[Dict], data_file: str = None) -> Dict[str, Any]:
        """Execute workflow asynchronously with enhanced logging"""
        self.workflow_start_time = time.time()
        
        logger.info(f"🚀 Starting async workflow execution with {len(workflow)} agents")
        
        # Ensure session is ready
        await self._ensure_session()
        
        # Create execution batches
        batches = self._create_execution_batches(workflow)
        self.total_batches = len(batches)
        
        logger.info(f"📦 Created {len(batches)} execution batches")
        
        # Log batch overview
        for i, batch in enumerate(batches):
            agent_names = [task.agent_name for task in batch]
            logger.info(f"   Batch {i + 1}: {len(batch)} agents - {', '.join(agent_names[:3])}{'...' if len(batch) > 3 else ''}")
        
        results = {}
        
        try:
            for batch_idx, batch in enumerate(batches):
                self.current_batch = batch_idx + 1
                batch_start_time = time.time()
                
                agent_names = [task.agent_name for task in batch]
                logger.info(f"📦 BATCH {batch_idx + 1}/{len(batches)}: Executing {len(batch)} agents")
                logger.info(f"   🎯 Agents: {', '.join(agent_names)}")
                
                # Update references for tasks in this batch
                for task in batch:
                    if task.required_tools:
                        # Add references from completed agents
                        refs = {}
                        for dep in task.dependencies:
                            if dep in results:
                                refs[dep] = results[dep]
                        task.references = refs
                
                # Execute batch concurrently
                batch_tasks = [self._execute_agent_async(task) for task in batch]
                batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
                
                # Store results and log individual completions
                successful_agents = []
                failed_agents = []
                
                for task, result in zip(batch, batch_results):
                    if isinstance(result, Exception):
                        results[task.agent_name] = {"error": str(result)}
                        self.failed_agents.add(task.agent_name)
                        failed_agents.append(task.agent_name)
                    else:
                        results[task.agent_name] = result
                        self.completed_agents.add(task.agent_name)
                        successful_agents.append(task.agent_name)
                
                # Log batch completion
                batch_duration = time.time() - batch_start_time
                completed_count = len(self.completed_agents)
                failed_count = len(self.failed_agents)
                total_count = len(workflow)
                
                logger.info(f"✅ BATCH {batch_idx + 1} COMPLETED in {batch_duration:.1f}s")
                logger.info(f"   ✅ Successful: {len(successful_agents)} - {', '.join(successful_agents)}")
                if failed_agents:
                    logger.warning(f"   ❌ Failed: {len(failed_agents)} - {', '.join(failed_agents)}")
                
                # Overall progress update
                progress_percent = (completed_count / total_count) * 100
                total_elapsed = time.time() - self.workflow_start_time
                
                logger.info(f"📊 OVERALL PROGRESS: {completed_count}/{total_count} ({progress_percent:.1f}%) completed, {failed_count} failed")
                logger.info(f"⏱️ Total elapsed: {total_elapsed:.1f}s ({total_elapsed/60:.1f}min)")
                
                # Estimate remaining time
                if completed_count > 0:
                    avg_time_per_agent = total_elapsed / completed_count
                    remaining_agents = total_count - completed_count
                    estimated_remaining = avg_time_per_agent * remaining_agents
                    logger.info(f"🔮 Estimated remaining time: {estimated_remaining:.1f}s ({estimated_remaining/60:.1f}min)")
                
                # Small delay between batches to prevent overwhelming
                if batch_idx < len(batches) - 1:
                    await asyncio.sleep(0.1)
        
        except Exception as e:
            logger.error(f"❌ Batch execution failed: {e}")
            return {"error": str(e), "results": results}
        
        # Final summary
        total_execution_time = time.time() - self.workflow_start_time
        logger.info("🎉 Async workflow execution completed")
        logger.info(f"📊 Final Results: {len(self.completed_agents)} completed, {len(self.failed_agents)} failed")
        logger.info(f"⏱️ Total execution time: {total_execution_time:.1f}s ({total_execution_time/60:.1f}min)")
        
        # Log slowest agents
        if self.step_timings:
            slowest_agents = sorted(self.step_timings.items(), key=lambda x: x[1], reverse=True)[:3]
            logger.info(f"🐌 Slowest agents: {', '.join([f'{name}({time:.1f}s)' for name, time in slowest_agents])}")
        
        return {
            "results": results,
            "completed_count": len(self.completed_agents),
            "failed_count": len(self.failed_agents),
            "total_count": len(workflow),
            "execution_time": total_execution_time,
            "step_timings": self.step_timings
        }

# Utility function for easy usage
async def execute_large_workflow(workflow_file: str, config: Dict[str, Any], 
                                data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Execute a large workflow asynchronously"""
    
    # Load workflow
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow = json.load(f)
    
    if isinstance(workflow, dict) and "steps" in workflow:
        workflow = workflow["steps"]
    
    async with AsyncWorkflowExecutor(config, max_concurrent=max_concurrent) as executor:
        return await executor.execute_workflow_async(workflow, data_file)


==================================================
FILE: async_flow_control.py
==================================================

#!/usr/bin/env python3

import asyncio
import json
import copy
from typing import Dict, Any, List, Optional, Union
from datetime import datetime
import logging

# Import workflow state components (assuming they exist)
try:
    from workflow_state import WorkflowContext, WorkflowState, ConditionEvaluator
except ImportError:
    # Minimal implementations if not available
    class WorkflowState:
        def __init__(self):
            self.variables = {}
            self.scopes = []
            self.execution_history = []
            self.checkpoints = {}
            self.current_scope = "global"
            self.iteration_counters = {}
        
        def set_variable(self, name: str, value: Any, scope: str = None):
            self.variables[name] = value
        
        def get_variable(self, name: str, scope: str = None) -> Any:
            return self.variables.get(name)
        
        def get_state_summary(self) -> Dict[str, Any]:
            return {"variables": self.variables}
    
    class ConditionEvaluator:
        def __init__(self, state): 
            self.state = state
        def evaluate(self, condition: str) -> bool:
            return True  # Simplified
    
    class WorkflowContext:
        def __init__(self):
            self.state = WorkflowState()
            self.condition_evaluator = ConditionEvaluator(self.state)
            self.results = {}

logger = logging.getLogger("async_flow_control")

class AsyncFlowControlExecutor:
    """Async executor with advanced flow control capabilities"""
    
    def __init__(self, tool_manager, max_concurrent: int = 50):
        self.tool_manager = tool_manager
        self.max_concurrent = max_concurrent
        self.context = WorkflowContext()
    
    async def execute_workflow_with_flow_control(self, workflow: List[Dict[str, Any]], 
                                               data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with full flow control support"""
        
        try:
            logger.info("Starting async workflow execution with flow control")
            
            # Initialize workflow variables if present
            if isinstance(workflow, dict) and "variables" in workflow:
                for var_name, var_value in workflow["variables"].items():
                    self.context.state.set_variable(var_name, var_value)
                workflow = workflow.get("steps", [])
            
            # Execute all steps with flow control
            for step_index, step in enumerate(workflow):
                logger.info(f"Executing step {step_index + 1}/{len(workflow)}")
                
                try:
                    result = await self._execute_step_async(step, data_file)
                    
                    # Store result if agent name is present
                    if "agent" in step:
                        agent_name = step["agent"]
                        self.context.results[agent_name] = result
                        logger.info(f"✅ Step {step_index + 1} ({agent_name}) completed")
                    
                except Exception as e:
                    error_msg = f"❌ Step {step_index + 1} failed: {str(e)}"
                    logger.error(error_msg)
                    
                    # Handle error based on policy
                    error_policy = step.get("on_error", "stop")
                    if error_policy == "stop":
                        raise Exception(error_msg)
                    elif error_policy == "continue":
                        logger.warning("Continuing execution despite error")
                        continue
            
            # Return final results
            final_results = dict(self.context.results)
            final_results["workflow_state"] = self.context.state.get_state_summary()
            
            logger.info("✅ Async flow control workflow execution completed")
            return final_results
            
        except Exception as e:
            logger.error(f"❌ Async flow control workflow execution failed: {str(e)}")
            return {"error": str(e), "partial_results": dict(self.context.results)}
    
    async def _execute_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute a single workflow step with async flow control"""
        
        # Resolve template variables in step
        resolved_step = self._resolve_step_variables(step)
        
        # Get step type
        step_type = resolved_step.get("type", "agent")
        
        # Route to appropriate async handler
        if step_type == "agent":
            return await self._execute_agent_step_async(resolved_step, data_file)
        elif step_type == "loop":
            return await self._execute_loop_step_async(resolved_step, data_file)
        elif step_type == "conditional":
            return await self._execute_conditional_step_async(resolved_step, data_file)
        elif step_type == "parallel":
            return await self._execute_parallel_step_async(resolved_step, data_file)
        elif step_type == "state":
            return await self._execute_state_step_async(resolved_step)
        else:
            raise ValueError(f"Unknown step type: {step_type}")
    
    async def _execute_agent_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute agent step asynchronously"""
        # This would integrate with your AsyncToolIntegratedExecutor
        # For now, simplified implementation
        agent_name = step.get("agent")
        content = step.get("content", "")
        
        # Simulate async agent execution
        await asyncio.sleep(0.1)  # Simulate work
        
        return {"content": f"Result from {agent_name}: {content[:50]}..."}
    
    async def _execute_loop_step_async(self, step: Dict[str, Any], data_file: str = None) -> List[Any]:
        """Execute async loop with various loop types"""
        
        loop_type = step.get("loop_type", "while")
        max_iterations = step.get("max_iterations", 100)
        steps = step.get("steps", [])
        
        if not steps:
            logger.warning("Loop step has no substeps")
            return []
        
        results = []
        iteration = 0
        
        # Create loop scope
        loop_scope_name = f"loop_{datetime.now().timestamp()}"
        self.context.state.set_variable("current_loop", loop_scope_name)
        
        try:
            if loop_type == "while":
                condition = step.get("condition", "true")
                
                while iteration < max_iterations:
                    # Update iteration counter
                    self.context.state.set_variable("iteration", iteration)
                    
                    # Check condition
                    if not self._evaluate_condition(condition):
                        logger.info(f"Loop condition failed at iteration {iteration}")
                        break
                    
                    # Execute substeps concurrently if possible
                    iteration_results = []
                    if step.get("parallel_substeps", False):
                        # Execute substeps in parallel
                        tasks = [self._execute_step_async(substep, data_file) for substep in steps]
                        iteration_results = await asyncio.gather(*tasks, return_exceptions=True)
                    else:
                        # Execute substeps sequentially
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                    
                    results.append(iteration_results)
                    iteration += 1
                    
                    logger.debug(f"Completed async while loop iteration {iteration}")
            
            elif loop_type == "for_count":
                count = step.get("count", 1)
                
                # Execute iterations with controlled concurrency
                semaphore = asyncio.Semaphore(min(self.max_concurrent, count))
                
                async def execute_iteration(i):
                    async with semaphore:
                        self.context.state.set_variable("iteration", i)
                        self.context.state.set_variable("index", i)
                        
                        iteration_results = []
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                        
                        return iteration_results
                
                # Execute all iterations concurrently with semaphore control
                tasks = [execute_iteration(i) for i in range(min(count, max_iterations))]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                logger.debug(f"Completed async for_count loop with {len(results)} iterations")
            
            elif loop_type == "for_each":
                collection = step.get("collection", [])
                item_variable = step.get("item_variable", "current_item")
                
                # Resolve collection if it's a variable reference
                if isinstance(collection, str):
                    collection = self.context.state.get_variable(collection) or []
                
                # Execute with controlled concurrency
                semaphore = asyncio.Semaphore(min(self.max_concurrent, len(collection)))
                
                async def execute_for_item(i, item):
                    async with semaphore:
                        self.context.state.set_variable("iteration", i)
                        self.context.state.set_variable("index", i)
                        self.context.state.set_variable(item_variable, item)
                        
                        iteration_results = []
                        for substep in steps:
                            result = await self._execute_step_async(substep, data_file)
                            iteration_results.append(result)
                        
                        return iteration_results
                
                # Execute all items concurrently with semaphore control
                tasks = [execute_for_item(i, item) for i, item in enumerate(collection[:max_iterations])]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                logger.debug(f"Completed async for_each loop with {len(results)} items")
            
            else:
                raise ValueError(f"Unknown loop type: {loop_type}")
        
        finally:
            # Cleanup loop scope
            pass
        
        logger.info(f"Async loop completed with {len(results)} iterations")
        return results
    
    async def _execute_conditional_step_async(self, step: Dict[str, Any], data_file: str = None) -> Any:
        """Execute async conditional step"""
        
        condition = step.get("condition", "true")
        true_steps = step.get("true_steps", step.get("then_steps", []))
        false_steps = step.get("false_steps", step.get("else_steps", []))
        
        # Evaluate condition
        if self._evaluate_condition(condition):
            logger.info(f"Condition '{condition}' evaluated to True")
            
            # Execute true steps (potentially in parallel)
            if step.get("parallel_execution", False):
                tasks = [self._execute_step_async(substep, data_file) for substep in true_steps]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                results = []
                for substep in true_steps:
                    result = await self._execute_step_async(substep, data_file)
                    results.append(result)
            
            return results
        else:
            logger.info(f"Condition '{condition}' evaluated to False")
            
            # Execute false steps
            if step.get("parallel_execution", False):
                tasks = [self._execute_step_async(substep, data_file) for substep in false_steps]
                results = await asyncio.gather(*tasks, return_exceptions=True)
            else:
                results = []
                for substep in false_steps:
                    result = await self._execute_step_async(substep, data_file)
                    results.append(result)
            
            return results
    
    async def _execute_parallel_step_async(self, step: Dict[str, Any], data_file: str = None) -> List[Any]:
        """Execute parallel step with controlled concurrency"""
        
        parallel_steps = step.get("steps", step.get("agents", []))
        max_parallel = step.get("max_parallel", self.max_concurrent)
        
        if not parallel_steps:
            logger.warning("Parallel step has no substeps")
            return []
        
        # Use semaphore to control concurrency
        semaphore = asyncio.Semaphore(max_parallel)
        
        async def execute_with_semaphore(substep):
            async with semaphore:
                return await self._execute_step_async(substep, data_file)
        
        # Execute all substeps with controlled concurrency
        tasks = [execute_with_semaphore(substep) for substep in parallel_steps]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Handle exceptions
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"Parallel substep {i} failed: {result}")
                processed_results.append({"error": str(result)})
            else:
                processed_results.append(result)
        
        logger.info(f"Async parallel execution completed with {len(parallel_steps)} substeps")
        return processed_results
    
    async def _execute_state_step_async(self, step: Dict[str, Any]) -> Any:
        """Execute async state management step"""
        
        operation = step.get("operation")
        if not operation:
            raise ValueError("State step must have 'operation' field")
        
        # Most state operations are synchronous, so just wrap them
        if operation == "set_variable":
            var_name = step.get("variable_name")
            var_value = step.get("value")
            if var_name:
                self.context.state.set_variable(var_name, var_value)
                return {"success": True, "variable": var_name, "value": var_value}
        
        elif operation == "get_variable":
            var_name = step.get("variable_name")
            if var_name:
                value = self.context.state.get_variable(var_name)
                return {"variable": var_name, "value": value}
        
        elif operation == "delay":
            delay_seconds = step.get("seconds", 1.0)
            await asyncio.sleep(delay_seconds)
            return {"success": True, "delayed_seconds": delay_seconds}
        
        # Add other state operations as needed
        return {"success": True, "operation": operation}
    
    def _resolve_step_variables(self, step: Dict[str, Any]) -> Dict[str, Any]:
        """Resolve template variables in step"""
        # Simplified implementation - could use workflow_state's resolve_step_variables
        return step
    
    def _evaluate_condition(self, condition: str) -> bool:
        """Evaluate condition"""
        # Use the condition evaluator if available
        if hasattr(self.context, 'condition_evaluator'):
            return self.context.condition_evaluator.evaluate(condition)
        
        # Simplified evaluation
        if condition.lower() in ['true', '1', 'yes']:
            return True
        elif condition.lower() in ['false', '0', 'no']:
            return False
        
        # Try to evaluate variable
        if hasattr(self.context.state, 'get_variable'):
            value = self.context.state.get_variable(condition)
            return bool(value)
        
        return True  # Default to true for safety



==================================================
FILE: async_framework_main.py
==================================================


#!/usr/bin/env python3
"""
Complete Async Framework Integration
Main entry point for async workflow execution with all features
"""

import asyncio
import json
import logging
import sys
import os
from typing import Dict, Any, List, Optional
from pathlib import Path

# Add this at the top of async_framework_main.py after the existing imports:

# Import with error handling
try:
    from utils import get_config
except ImportError:
    def get_config():
        return {
            "default_model": "deepseek/deepseek-chat:free",
            "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
            "endpoint": "https://openrouter.ai/api/v1/chat/completions",
            "output_dir": "./async_outputs"
        }

try:
    from tool_manager import tool_manager
except ImportError:
    tool_manager = None
# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger("async_framework")

class AsyncFrameworkManager:
    """Manages the complete async framework with all features"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or self._get_default_config()
        self.available_features = self._check_available_features()
        
        logger.info(f"🚀 Async Framework initialized")
        logger.info(f"Available features: {', '.join(self.available_features)}")
    
    def _get_default_config(self) -> Dict[str, Any]:
        """Get default configuration"""
        try:
            from utils import get_config
            return get_config()
        except ImportError:
            return {
                "default_model": "deepseek/deepseek-chat:free",
                "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "output_dir": "./async_outputs"
            }
    
    def _check_available_features(self) -> List[str]:
        """Check which features are available"""
        features = ["basic_async"]
        
        try:
            from async_tool_integration import AsyncToolIntegratedExecutor
            features.append("tools")
        except ImportError:
            logger.warning("Tool integration not available")
        
        try:
            from async_flow_control import AsyncFlowControlExecutor
            features.append("flow_control")
        except ImportError:
            logger.warning("Flow control not available")
        
        try:
            from async_dynamic_agents import AsyncDynamicAgentExecutor
            features.append("dynamic_agents")
        except ImportError:
            logger.warning("Dynamic agents not available")
        
        return features
    
    async def execute_workflow(self, workflow_file: str, 
                             data_file: Optional[str] = None,
                             max_concurrent: int = 50,
                             features: Optional[List[str]] = None) -> Dict[str, Any]:
        """Execute workflow with specified features"""
        
        # Load workflow
        workflow = self._load_workflow(workflow_file)
        if not workflow:
            return {"error": "Failed to load workflow", "success": False}
        
        # Determine which executor to use
        requested_features = features or self._analyze_workflow_features(workflow)
        executor_class = self._select_executor(requested_features)
        
        logger.info(f"Using executor: {executor_class.__name__}")
        logger.info(f"Features: {', '.join(requested_features)}")
        
        try:
            # Execute with selected executor
            if executor_class.__name__ == "FullAsyncWorkflowExecutor":
                from async_dynamic_agents import FullAsyncWorkflowExecutor
                executor = FullAsyncWorkflowExecutor(self.config, max_concurrent)
                return await executor.execute_complete_workflow(workflow, data_file)
            
            elif executor_class.__name__ == "AsyncToolIntegratedExecutor":
                from async_tool_integration import AsyncToolIntegratedExecutor
                async with AsyncToolIntegratedExecutor(self.config, max_concurrent) as executor:
                    return await executor.execute_workflow_with_tools(workflow, data_file)
            
            elif executor_class.__name__ == "AsyncFlowControlExecutor":
                from async_flow_control import AsyncFlowControlExecutor
                from tool_manager import tool_manager
                executor = AsyncFlowControlExecutor(tool_manager, max_concurrent)
                return await executor.execute_workflow_with_flow_control(workflow, data_file)
            
            else:
                # Default to basic async executor
                from async_executor import AsyncWorkflowExecutor
                async with AsyncWorkflowExecutor(self.config, max_concurrent) as executor:
                    return await executor.execute_workflow_async(workflow, data_file)
        
        except Exception as e:
            logger.error(f"Workflow execution failed: {e}")
            return {"error": str(e), "success": False}
    
    def _load_workflow(self, workflow_file: str) -> Optional[List[Dict[str, Any]]]:
        """Load workflow from file"""
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
            
            if isinstance(workflow_data, dict) and "steps" in workflow_data:
                return workflow_data["steps"]
            elif isinstance(workflow_data, list):
                return workflow_data
            else:
                logger.error("Invalid workflow format")
                return None
                
        except Exception as e:
            logger.error(f"Failed to load workflow {workflow_file}: {e}")
            return None
    
    def _analyze_workflow_features(self, workflow: List[Dict[str, Any]]) -> List[str]:
        """Analyze workflow to determine needed features"""
        features = []
        
        for step in workflow:
            # Check for dynamic agents
            if step.get("type") == "dynamic":
                features.append("dynamic_agents")
            
            # Check for flow control
            if step.get("type") in ["loop", "conditional", "parallel", "state"]:
                features.append("flow_control")
            
            # Check for tools
            if step.get("tools"):
                features.append("tools")
        
        return list(set(features))
    
    def _select_executor(self, requested_features: List[str]):
        """Select the best executor based on requested features"""
        
        # Priority order: dynamic_agents > flow_control > tools > basic
        if "dynamic_agents" in requested_features and "dynamic_agents" in self.available_features:
            from async_dynamic_agents import FullAsyncWorkflowExecutor
            return FullAsyncWorkflowExecutor
        
        elif "flow_control" in requested_features and "flow_control" in self.available_features:
            from async_flow_control import AsyncFlowControlExecutor
            return AsyncFlowControlExecutor
        
        elif "tools" in requested_features and "tools" in self.available_features:
            from async_tool_integration import AsyncToolIntegratedExecutor
            return AsyncToolIntegratedExecutor
        
        else:
            from async_executor import AsyncWorkflowExecutor
            return AsyncWorkflowExecutor
    
    def create_example_workflow(self, filename: str = "example_async_workflow.json"):
        """Create an example workflow file"""
        
        example_workflow = {
            "variables": {
                "project_name": "AsyncDemo",
                "analysis_depth": "detailed"
            },
            "steps": [
                {
                    "agent": "data_collector",
                    "content": "Collect and analyze data about {{project_name}} with {{analysis_depth}} level",
                    "output_format": {
                        "type": "json",
                        "schema": {
                            "summary": "string",
                            "data_points": "array",
                            "confidence": "number"
                        }
                    }
                },
                {
                    "type": "conditional",
                    "condition": "{{data_collector.confidence}} > 0.8",
                    "true_steps": [
                        {
                            "agent": "detailed_analyzer",
                            "content": "Perform detailed analysis based on high-confidence data",
                            "readFrom": ["data_collector"]
                        }
                    ],
                    "false_steps": [
                        {
                            "agent": "basic_analyzer", 
                            "content": "Perform basic analysis due to low confidence",
                            "readFrom": ["data_collector"]
                        }
                    ]
                },
                {
                    "type": "parallel",
                    "max_parallel": 3,
                    "steps": [
                        {
                            "agent": "summary_generator",
                            "content": "Generate executive summary",
                            "readFrom": ["*"]
                        },
                        {
                            "agent": "report_formatter",
                            "content": "Format detailed report",
                            "readFrom": ["*"]
                        },
                        {
                            "agent": "metrics_calculator",
                            "content": "Calculate performance metrics",
                            "readFrom": ["*"]
                        }
                    ]
                }
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(example_workflow, f, indent=2)
        
        logger.info(f"✅ Created example workflow: {filename}")
        return filename

# Utility functions for easy usage
async def run_async_workflow(workflow_file: str, 
                           config: Optional[Dict[str, Any]] = None,
                           data_file: Optional[str] = None,
                           max_concurrent: int = 50,
                           features: Optional[List[str]] = None) -> Dict[str, Any]:
    """Run async workflow with automatic feature detection"""
    
    framework = AsyncFrameworkManager(config)
    return await framework.execute_workflow(
        workflow_file=workflow_file,
        data_file=data_file,
        max_concurrent=max_concurrent,
        features=features
    )

def run_async_workflow_sync(workflow_file: str, 
                          config: Optional[Dict[str, Any]] = None,
                          data_file: Optional[str] = None,
                          max_concurrent: int = 50,
                          features: Optional[List[str]] = None) -> Dict[str, Any]:
    """Synchronous wrapper for async workflow execution"""
    
    return asyncio.run(run_async_workflow(
        workflow_file, config, data_file, max_concurrent, features
    ))

# CLI Interface
def main():
    """Command-line interface for async framework"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Async Workflow Framework")
    parser.add_argument("--workflow", required=True, help="Workflow JSON file")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=50, help="Max concurrent agents")
    parser.add_argument("--features", nargs="*", 
                       choices=["basic_async", "tools", "flow_control", "dynamic_agents"],
                       help="Force specific features")
    parser.add_argument("--config", help="Config file (Python module)")
    parser.add_argument("--create-example", action="store_true", 
                       help="Create example workflow file")
    parser.add_argument("--analyze", action="store_true", 
                       help="Analyze workflow without execution")
    parser.add_argument("--verbose", "-v", action="store_true", 
                       help="Verbose logging")
    
    args = parser.parse_args()
    
    # Setup logging level
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Load custom config if specified
    config = None
    if args.config:
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("custom_config", args.config)
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            config = config_module.CONFIG
            logger.info(f"Loaded custom config from {args.config}")
        except Exception as e:
            logger.error(f"Failed to load config {args.config}: {e}")
            sys.exit(1)
    
    # Create example workflow if requested
    if args.create_example:
        framework = AsyncFrameworkManager(config)
        example_file = framework.create_example_workflow()
        print(f"✅ Created example workflow: {example_file}")
        print("Run with: python async_framework_main.py --workflow example_async_workflow.json")
        return
    
    # Analyze workflow if requested
    if args.analyze:
        framework = AsyncFrameworkManager(config)
        workflow = framework._load_workflow(args.workflow)
        if workflow:
            features = framework._analyze_workflow_features(workflow)
            print(f"\n📊 Workflow Analysis:")
            print(f"File: {args.workflow}")
            print(f"Total Steps: {len(workflow)}")
            print(f"Required Features: {', '.join(features) if features else 'basic_async'}")
            print(f"Available Features: {', '.join(framework.available_features)}")
            
            # Check compatibility
            missing_features = set(features) - set(framework.available_features)
            if missing_features:
                print(f"⚠️  Missing Features: {', '.join(missing_features)}")
                print("Some workflow features may not work properly.")
            else:
                print("✅ All required features are available!")
        return
    
    # Execute workflow
    print(f"🚀 Starting async workflow execution...")
    print(f"Workflow: {args.workflow}")
    print(f"Max Concurrent: {args.concurrent}")
    if args.features:
        print(f"Forced Features: {', '.join(args.features)}")
    
    try:
        import time
        start_time = time.time()
        
        results = run_async_workflow_sync(
            workflow_file=args.workflow,
            config=config,
            data_file=args.data,
            max_concurrent=args.concurrent,
            features=args.features
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Print results summary
        print(f"\n{'='*50}")
        print("EXECUTION SUMMARY")
        print(f"{'='*50}")
        
        if results.get("success", True):  # Default to True if not specified
            print(f"✅ Workflow completed successfully")
            
            # Extract metrics based on result structure
            if "results" in results:
                # FullAsyncWorkflowExecutor format
                exec_results = results["results"]
                total_count = results.get("total_count", len(exec_results))
                completed_count = results.get("completed_count", total_count)
                failed_count = results.get("failed_count", 0)
            else:
                # Simple format - count results
                total_count = len([k for k in results.keys() if not k.endswith('_action') and k != 'success'])
                completed_count = total_count
                failed_count = 0
            
            print(f"Total Agents: {total_count}")
            print(f"Completed: {completed_count}")
            print(f"Failed: {failed_count}")
            print(f"Execution Time: {execution_time:.1f} seconds ({execution_time/60:.1f} minutes)")
            
            if total_count > 0:
                success_rate = (completed_count / total_count) * 100
                agents_per_second = completed_count / execution_time if execution_time > 0 else 0
                print(f"Success Rate: {success_rate:.1f}%")
                print(f"Agents/Second: {agents_per_second:.2f}")
            
            # Save results
            output_dir = Path("./async_outputs")
            output_dir.mkdir(exist_ok=True)
            results_file = output_dir / f"results_{int(start_time)}.json"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"📁 Results saved to: {results_file}")
            
        else:
            print(f"❌ Workflow failed: {results.get('error', 'Unknown error')}")
            
    except KeyboardInterrupt:
        print("\n🛑 Execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"❌ Execution failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()


==================================================
FILE: async_tool_integration.py
==================================================

#!/usr/bin/env python3

import asyncio
from typing import Dict, Any, List
from async_executor import AsyncWorkflowExecutor, AgentTask
from tool_manager import tool_manager
from utils import extract_tool_calls
import json
import logging

logger = logging.getLogger("async_tool_executor")

class AsyncToolIntegratedExecutor(AsyncWorkflowExecutor):
    """Enhanced async executor with tool management integration"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50, 
                 max_connections: int = 100, rate_limit: float = 10.0):
        super().__init__(config, max_concurrent, max_connections, rate_limit)
        
        # Auto-discover tools
        self.num_tools = tool_manager.discover_tools()
        logger.info(f"🔧 Auto-discovered {self.num_tools} tools for async execution")
    
    async def _execute_agent_with_tools(self, task: AgentTask) -> Dict[str, Any]:
        """Execute agent with tool support"""
        try:
            # Build enhanced prompt with tool information
            enhanced_prompt = task.prompt
            
            # Add tool information if required_tools is provided
            if task.required_tools:
                available_tools = []
                unavailable_tools = []
                
                for tool_id in task.required_tools:
                    if tool_manager.is_tool_available(tool_id):
                        available_tools.append(tool_id)
                    else:
                        unavailable_tools.append(tool_id)
                
                if available_tools:
                    enhanced_prompt += f"\n\nYou have access to these tools: {', '.join(available_tools)}"
                    enhanced_prompt += """
To use a tool, format your response like this:

I need to use the tool: $TOOL_NAME
Parameters:
{
  "param1": "value1",
  "param2": "value2"
}

Wait for the tool result before continuing.
"""
                
                if unavailable_tools:
                    enhanced_prompt += f"\n\nNote: The following tools are not available: {', '.join(unavailable_tools)}"
            
            # Add reference information
            if task.references:
                enhanced_prompt += "\n\n### Reference Information:\n"
                for ref_name, ref_content in task.references.items():
                    enhanced_prompt += f"\n#### Output from {ref_name}:\n"
                    if isinstance(ref_content, dict):
                        enhanced_prompt += json.dumps(ref_content, indent=2)
                    else:
                        enhanced_prompt += str(ref_content)
            
            # Add file content if provided
            if task.file_path:
                try:
                    with open(task.file_path, 'r', encoding='utf-8') as f:
                        file_content = f.read()
                        if len(file_content) > 10000:
                            file_content = file_content[:10000]
                        enhanced_prompt += f"\n\nFile content:\n```\n{file_content}\n```"
                except Exception as e:
                    logger.warning(f"Could not read file {task.file_path}: {e}")
            
            # Build system message
            system_message = f"You are a specialized assistant. Your responses must strictly follow the format specified."
            
            # Add domain-specific additions to system message
            agent_name = task.agent_name.lower()
            if "security" in agent_name or "threat" in agent_name or "cyber" in enhanced_prompt.lower():
                system_message = "You are a cybersecurity analysis assistant. " + system_message
            elif "journey" in agent_name or "customer" in agent_name or "segment" in enhanced_prompt.lower():
                system_message = "You are a customer journey analysis assistant. " + system_message
            elif "finance" in agent_name or "investment" in agent_name or "portfolio" in enhanced_prompt.lower():
                system_message = "You are a financial analysis assistant. " + system_message
            
            # Build conversation
            conversation = [
                {"role": "system", "content": system_message},
                {"role": "user", "content": enhanced_prompt}
            ]
            
            # Make initial API call
            response_content = await self._call_api_async(conversation)
            
            # Handle tool calls
            if "I need to use the tool:" in response_content:
                logger.info(f"Detected tool call attempts in response for {task.agent_name}")
                
                # Process all tool calls in the response
                all_tool_calls = extract_tool_calls(response_content)
                
                if all_tool_calls:
                    # Process tool calls (limit to 5 for safety)
                    for idx, tool_call in enumerate(all_tool_calls[:5]):
                        tool_name = tool_call["tool_name"]
                        params = tool_call["params"]
                        
                        logger.info(f"📡 Processing tool call {idx+1}: {tool_name}")
                        
                        # Execute tool (this is synchronous, but fast)
                        tool_result = tool_manager.execute_tool(tool_name, **params)
                        tool_result_str = json.dumps(tool_result, indent=2)
                        
                        # Replace tool call with result
                        tool_call_text = tool_call["full_text"]
                        response_content = response_content.replace(
                            tool_call_text,
                            f"Tool result for {tool_name}:\n```json\n{tool_result_str}\n```"
                        )
                    
                    # Get final response with tool results
                    final_prompt = f"Here is the result of executing your tool calls:\n\n{response_content}\n\nBased on these results, please provide your final response."
                    
                    conversation.append({"role": "assistant", "content": response_content})
                    conversation.append({"role": "user", "content": final_prompt})
                    
                    # Get final response
                    response_content = await self._call_api_async(conversation)
            
            # Process response based on output format
            if task.output_format and task.output_format.get("type") == "json":
                result = self._extract_json_from_response(response_content)
            else:
                result = {"content": response_content}
            
            logger.info(f"✅ Agent {task.agent_name} completed successfully")
            return result
            
        except Exception as e:
            error_result = {"error": f"Agent execution failed: {str(e)}"}
            logger.error(f"❌ Agent {task.agent_name} failed: {e}")
            return error_result
    
    def _extract_json_from_response(self, response: str) -> Dict[str, Any]:
        """Extract JSON from response with fallback"""
        try:
            return json.loads(response)
        except json.JSONDecodeError:
            # Try to extract JSON from response
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except:
                    return {"error": "Could not parse JSON", "raw_response": response}
            else:
                return {"error": "No JSON found", "raw_response": response}
    
    async def execute_workflow_with_tools(self, workflow: List[Dict], data_file: str = None) -> Dict[str, Any]:
        """Execute workflow with full tool integration"""
        logger.info(f"Starting async workflow execution with tools: {len(workflow)} agents")
        
        # Create execution batches (same as parent)
        batches = self._create_execution_batches(workflow)
        logger.info(f"Created {len(batches)} execution batches with tool support")
        
        results = {}
        
        for batch_idx, batch in enumerate(batches):
            logger.info(f"Executing batch {batch_idx + 1}/{len(batches)} with {len(batch)} agents")
            
            # Update references for tasks in this batch
            for task in batch:
                # Add references from completed agents
                refs = {}
                for dep in task.dependencies:
                    if dep in results:
                        refs[dep] = results[dep]
                task.references = refs
            
            # Execute batch with tool support
            batch_tasks = [self._execute_agent_with_tools(task) for task in batch]
            batch_results = await asyncio.gather(*batch_tasks, return_exceptions=True)
            
            # Store results
            for task, result in zip(batch, batch_results):
                if isinstance(result, Exception):
                    results[task.agent_name] = {"error": str(result)}
                    self.failed_agents.add(task.agent_name)
                else:
                    results[task.agent_name] = result
                    self.completed_agents.add(task.agent_name)
            
            # Progress update
            completed_count = len(self.completed_agents)
            failed_count = len(self.failed_agents)
            total_count = len(workflow)
            
            logger.info(f"Progress: {completed_count}/{total_count} completed, {failed_count} failed")
            
            # Small delay between batches
            if batch_idx < len(batches) - 1:
                await asyncio.sleep(0.1)
        
        logger.info("✅ Async workflow with tools execution completed")
        
        return {
            "results": results,
            "completed_count": len(self.completed_agents),
            "failed_count": len(self.failed_agents),
            "total_count": len(workflow),
            "tools_discovered": self.num_tools
        }


==================================================
FILE: complete_async_framework.py
==================================================

#!/usr/bin/env python3
"""
Complete Async Framework Integration
Advanced features + Guaranteed real API calls + Enhanced logging
"""

import asyncio
import json
import logging
import sys
import os
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

# Enhanced logging setup
def setup_enhanced_logging(verbose: bool = False):
    """Setup enhanced logging with better formatting"""
    log_level = logging.DEBUG if verbose else logging.INFO
    
    # Custom formatter with colors and step tracking
    class EnhancedFormatter(logging.Formatter):
        COLORS = {
            'DEBUG': '\033[36m',    # Cyan
            'INFO': '\033[32m',     # Green
            'WARNING': '\033[33m',  # Yellow
            'ERROR': '\033[31m',    # Red
            'CRITICAL': '\033[35m', # Magenta
        }
        RESET = '\033[0m'
        
        def format(self, record):
            if record.levelname in self.COLORS:
                record.levelname = f"{self.COLORS[record.levelname]}{record.levelname}{self.RESET}"
            
            if hasattr(record, 'step_info'):
                return f"{record.asctime} - {record.levelname} - [{record.step_info}] {record.message}"
            else:
                return f"{record.asctime} - {record.levelname} - {record.name} - {record.message}"
    
    # Configure logging
    logging.basicConfig(
        level=log_level,
        format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',
        handlers=[logging.StreamHandler(sys.stdout)]
    )
    
    # Get framework logger with custom formatter
    logger = logging.getLogger("async_framework")
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setLevel(log_level)
    console_handler.setFormatter(EnhancedFormatter())
    
    logger.handlers.clear()
    logger.addHandler(console_handler)
    logger.setLevel(log_level)
    
    return logger

# Import with error handling - Support both Ollama and OpenRouter
try:
    from utils import get_config
except ImportError:
    def get_config():
        # Try Ollama first, then OpenRouter
        if os.path.exists("config.py"):
            try:
                import importlib.spec_from_file_location
                spec = importlib.util.spec_from_file_location("config", "config.py")
                config_module = importlib.util.module_from_spec(spec)
                spec.loader.exec_module(config_module)
                return config_module.CONFIG
            except:
                pass
        
        # Default to Ollama config
        return {
            "output_dir": "./agent_outputs",
            "memory_dir": "./agent_memory",
            "default_model": "qwen2.5:7b",
            "api_key": "",  # Ollama doesn't need API key
            "endpoint": "http://localhost:11434/v1/chat/completions",
            "memory_db": "agent_memory.db",
            "sqlite_db": "test_sqlite.db",
            "timeout": 1200
        }

try:
    from tool_manager import tool_manager
except ImportError:
    tool_manager = None

# Workflow progress tracker
class WorkflowProgressTracker:
    """Track and log workflow execution progress"""
    
    def __init__(self, logger, total_steps: int):
        self.logger = logger
        self.total_steps = total_steps
        self.completed_steps = 0
        self.failed_steps = 0
        self.current_step = None
        self.start_time = time.time()
        self.step_start_times = {}
        
    def start_step(self, step_name: str, step_index: int):
        """Log step start"""
        self.current_step = step_name
        self.step_start_times[step_name] = time.time()
        
        progress = f"{step_index + 1}/{self.total_steps}"
        elapsed = time.time() - self.start_time
        
        self.logger.info(
            f"🚀 STEP {progress}: Starting '{step_name}' [Elapsed: {elapsed:.1f}s]",
            extra={'step_info': f"STEP {progress}"}
        )
    
    def step_completed(self, step_name: str, step_index: int):
        """Log step completion"""
        self.completed_steps += 1
        step_time = time.time() - self.step_start_times.get(step_name, time.time())
        total_elapsed = time.time() - self.start_time
        
        progress = f"{step_index + 1}/{self.total_steps}"
        
        self.logger.info(
            f"✅ STEP {progress}: Completed '{step_name}' [Step: {step_time:.1f}s, Total: {total_elapsed:.1f}s]",
            extra={'step_info': f"STEP {progress}"}
        )
    
    def step_failed(self, step_name: str, step_index: int, error: str):
        """Log step failure"""
        self.failed_steps += 1
        step_time = time.time() - self.step_start_times.get(step_name, time.time())
        total_elapsed = time.time() - self.start_time
        
        progress = f"{step_index + 1}/{self.total_steps}"
        
        self.logger.error(
            f"❌ STEP {progress}: Failed '{step_name}' - {error} [Step: {step_time:.1f}s, Total: {total_elapsed:.1f}s]",
            extra={'step_info': f"STEP {progress}"}
        )
    
    def workflow_summary(self):
        """Log final workflow summary"""
        total_time = time.time() - self.start_time
        success_rate = (self.completed_steps / self.total_steps * 100) if self.total_steps > 0 else 0
        
        self.logger.info(
            f"📊 WORKFLOW SUMMARY: {self.completed_steps}/{self.total_steps} completed ({success_rate:.1f}%), "
            f"{self.failed_steps} failed, Total time: {total_time:.1f}s ({total_time/60:.1f}min)",
            extra={'step_info': 'SUMMARY'}
        )

class CompleteAsyncFrameworkManager:
    """Complete async framework with all features and guaranteed real API calls"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None, verbose: bool = False):
        self.config = config or get_config()
        self.available_features = self._check_available_features()
        self.logger = setup_enhanced_logging(verbose)
        self.verbose = verbose
        
        # Create output directories
        os.makedirs(self.config.get("output_dir", "./agent_outputs"), exist_ok=True)
        os.makedirs(self.config.get("memory_dir", "./agent_memory"), exist_ok=True)
        
        self.logger.info("🚀 Complete Async Framework initialized")
        self.logger.info(f"🤖 Model: {self.config['default_model']}")
        self.logger.info(f"🌐 Endpoint: {self.config['endpoint']}")
        self.logger.info(f"Available features: {', '.join(self.available_features)}")
        
        # Check API configuration
        if "localhost" in self.config["endpoint"] or "127.0.0.1" in self.config["endpoint"]:
            self.logger.info("🔧 Using local endpoint (Ollama) - no API key required")
        else:
            if self.config.get("api_key"):
                self.logger.info(f"🔑 API key configured: {self.config['api_key'][:10]}...")
            else:
                self.logger.warning("⚠️ No API key found for external endpoint")
    
    def _check_available_features(self) -> List[str]:
        """Check which features are available"""
        features = ["basic_async"]
        
        try:
            from async_tool_integration import AsyncToolIntegratedExecutor
            features.append("tools")
        except ImportError:
            if self.verbose:
                self.logger.debug("Tool integration not available")
        
        try:
            from async_flow_control import AsyncFlowControlExecutor
            features.append("flow_control")
        except ImportError:
            if self.verbose:
                self.logger.debug("Flow control not available")
        
        try:
            from async_dynamic_agents import AsyncDynamicAgentExecutor
            features.append("dynamic_agents")
        except ImportError:
            if self.verbose:
                self.logger.debug("Dynamic agents not available")
        
        return features
    
    async def execute_workflow(self, workflow_file: str, 
                             data_file: Optional[str] = None,
                             max_concurrent: int = 50,
                             features: Optional[List[str]] = None) -> Dict[str, Any]:
        """Execute workflow with all features and guaranteed real API calls"""
        
        workflow_start_time = time.time()
        
        # Load workflow
        workflow = self._load_workflow(workflow_file)
        if not workflow:
            return {"error": "Failed to load workflow", "success": False}
        
        # Initialize progress tracker
        progress_tracker = WorkflowProgressTracker(self.logger, len(workflow))
        
        # Log comprehensive workflow overview
        self.logger.info(f"📋 Workflow loaded: {len(workflow)} steps from {workflow_file}")
        if data_file:
            try:
                file_size = os.path.getsize(data_file) if os.path.exists(data_file) else 0
                self.logger.info(f"📂 Data file: {data_file} ({file_size/1024:.1f}KB)")
            except:
                self.logger.info(f"📂 Data file: {data_file}")
        
        # Auto-discover tools
        if tool_manager:
            tool_count = tool_manager.discover_tools()
            self.logger.info(f"🔧 Discovered {tool_count} tools")
        
        # Log step overview
        self._log_workflow_steps(workflow)
        
        # Determine which executor to use
        requested_features = features or self._analyze_workflow_features(workflow)
        executor_class = self._select_executor(requested_features)
        
        self.logger.info(f"🔧 Using executor: {executor_class.__name__}")
        self.logger.info(f"⚙️ Features: {', '.join(requested_features)}")
        self.logger.info(f"🔄 Max concurrent: {max_concurrent}")
        
        try:
            self.logger.info("🚀 Starting workflow execution with guaranteed real API calls...")
            
            # Force real API calls by using the most appropriate executor
            if "tools" in requested_features or len(requested_features) == 0:
                # Prioritize tool-enabled executor for real API calls
                try:
                    from async_tool_integration import AsyncToolIntegratedExecutor
                    self.logger.info("🔧 Using AsyncToolIntegratedExecutor for guaranteed real API calls")
                    
                    async with AsyncToolIntegratedExecutor(self.config, max_concurrent) as executor:
                        results = await executor.execute_workflow_with_tools(workflow, data_file)
                        
                except ImportError:
                    self.logger.warning("AsyncToolIntegratedExecutor not available, falling back to basic executor")
                    from async_executor import AsyncWorkflowExecutor
                    
                    async with AsyncWorkflowExecutor(self.config, max_concurrent) as executor:
                        results = await executor.execute_workflow_async(workflow, data_file)
            
            elif "dynamic_agents" in requested_features:
                from async_dynamic_agents import FullAsyncWorkflowExecutor
                self.logger.info("🎯 Using FullAsyncWorkflowExecutor for dynamic agents")
                executor = FullAsyncWorkflowExecutor(self.config, max_concurrent)
                results = await executor.execute_complete_workflow(workflow, data_file)
            
            elif "flow_control" in requested_features:
                from async_flow_control import AsyncFlowControlExecutor
                self.logger.info("🔄 Using AsyncFlowControlExecutor for flow control")
                executor = AsyncFlowControlExecutor(tool_manager, max_concurrent)
                results = await executor.execute_workflow_with_flow_control(workflow, data_file)
            
            else:
                # Default to basic async executor with guaranteed API calls
                from async_executor import AsyncWorkflowExecutor
                self.logger.info("⚡ Using basic AsyncWorkflowExecutor")
                
                async with AsyncWorkflowExecutor(self.config, max_concurrent) as executor:
                    results = await executor.execute_workflow_async(workflow, data_file)
            
            # Log execution summary
            execution_time = time.time() - workflow_start_time
            self._log_execution_summary(results, execution_time, len(workflow))
            progress_tracker.workflow_summary()
            
            return results
        
        except Exception as e:
            execution_time = time.time() - workflow_start_time
            self.logger.error(f"❌ Workflow execution failed after {execution_time:.1f}s: {e}")
            progress_tracker.workflow_summary()
            return {"error": str(e), "success": False}
    
    def _log_workflow_steps(self, workflow: List[Dict[str, Any]]):
        """Log detailed workflow steps overview"""
        self.logger.info("📝 Workflow Steps Overview:")
        
        for i, step in enumerate(workflow[:15]):  # Show first 15 steps
            step_num = i + 1
            agent_name = step.get("agent", "unknown")
            step_type = step.get("type", "agent")
            tools = step.get("tools", [])
            read_from = step.get("readFrom", [])
            
            # Create step description
            desc_parts = []
            if step_type != "agent":
                desc_parts.append(f"type:{step_type}")
            if tools:
                tool_list = tools[:2] + ["..."] if len(tools) > 2 else tools
                desc_parts.append(f"tools:{','.join(tool_list)}")
            if read_from:
                deps = read_from[:2] + ["..."] if len(read_from) > 2 else read_from
                desc_parts.append(f"deps:{','.join(str(d) for d in deps)}")
            
            desc = f" [{', '.join(desc_parts)}]" if desc_parts else ""
            self.logger.info(f"  {step_num:2d}. {agent_name}{desc}")
        
        if len(workflow) > 15:
            self.logger.info(f"  ... and {len(workflow) - 15} more steps")
    
    def _log_execution_summary(self, results: Dict[str, Any], execution_time: float, total_steps: int):
        """Log comprehensive execution summary"""
        if isinstance(results, dict):
            # Extract metrics from different executor formats
            if "results" in results:
                exec_results = results["results"]
                completed = results.get("completed_count", len(exec_results))
                failed = results.get("failed_count", 0)
                total = results.get("total_count", total_steps)
                tools_discovered = results.get("tools_discovered", 0)
            else:
                completed = len([k for k in results.keys() if not k.endswith('_action') and k not in ['success', 'error', 'workflow_state']])
                failed = len([k for k, v in results.items() if isinstance(v, dict) and v.get('error')])
                total = total_steps
                tools_discovered = 0
            
            success_rate = (completed / total * 100) if total > 0 else 0
            agents_per_second = completed / execution_time if execution_time > 0 else 0
            
            self.logger.info("📊 COMPREHENSIVE EXECUTION SUMMARY:")
            self.logger.info(f"  ✅ Completed: {completed}/{total} ({success_rate:.1f}%)")
            self.logger.info(f"  ❌ Failed: {failed}")
            self.logger.info(f"  ⏱️ Total time: {execution_time:.1f}s ({execution_time/60:.1f}min)")
            self.logger.info(f"  ⚡ Speed: {agents_per_second:.2f} agents/second")
            if tools_discovered > 0:
                self.logger.info(f"  🔧 Tools used: {tools_discovered}")
    
    def _load_workflow(self, workflow_file: str) -> Optional[List[Dict[str, Any]]]:
        """Load workflow from file with enhanced error handling"""
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
            
            if isinstance(workflow_data, dict) and "steps" in workflow_data:
                return workflow_data["steps"]
            elif isinstance(workflow_data, list):
                return workflow_data
            else:
                self.logger.error("❌ Invalid workflow format - expected list of steps or dict with 'steps' key")
                return None
                
        except FileNotFoundError:
            self.logger.error(f"❌ Workflow file not found: {workflow_file}")
            return None
        except json.JSONDecodeError as e:
            self.logger.error(f"❌ Invalid JSON in workflow file: {e}")
            return None
        except Exception as e:
            self.logger.error(f"❌ Failed to load workflow {workflow_file}: {e}")
            return None
    
    def _analyze_workflow_features(self, workflow: List[Dict[str, Any]]) -> List[str]:
        """Analyze workflow to determine needed features"""
        features = []
        
        for step in workflow:
            if step.get("type") == "dynamic":
                features.append("dynamic_agents")
            if step.get("type") in ["loop", "conditional", "parallel", "state"]:
                features.append("flow_control")
            if step.get("tools"):
                features.append("tools")
        
        # If no special features, ensure we use tools for real API calls
        if not features:
            features.append("tools")
        
        return list(set(features))
    
    def _select_executor(self, requested_features: List[str]):
        """Select executor prioritizing real API calls"""
        
        # Priority: tools > dynamic_agents > flow_control > basic
        # Tools executor is most reliable for real API calls
        if "tools" in requested_features and "tools" in self.available_features:
            from async_tool_integration import AsyncToolIntegratedExecutor
            return AsyncToolIntegratedExecutor
        
        elif "dynamic_agents" in requested_features and "dynamic_agents" in self.available_features:
            from async_dynamic_agents import FullAsyncWorkflowExecutor
            return FullAsyncWorkflowExecutor
        
        elif "flow_control" in requested_features and "flow_control" in self.available_features:
            from async_flow_control import AsyncFlowControlExecutor
            return AsyncFlowControlExecutor
        
        else:
            from async_executor import AsyncWorkflowExecutor
            return AsyncWorkflowExecutor
    
    def create_example_workflow(self, filename: str = "example_complete_workflow.json"):
        """Create comprehensive example workflow"""
        
        example_workflow = {
            "variables": {
                "project_name": "CompleteAsyncDemo",
                "analysis_depth": "comprehensive"
            },
            "steps": [
                {
                    "agent": "research_planner",
                    "content": "Create a research plan for {{project_name}} with {{analysis_depth}} analysis",
                    "tools": ["planning_create_plan"],
                    "output_format": {
                        "type": "json",
                        "schema": {
                            "plan": "string",
                            "objectives": "array",
                            "timeline": "string"
                        }
                    }
                },
                {
                    "agent": "data_collector",
                    "content": "Collect data according to the research plan",
                    "readFrom": ["research_planner"],
                    "tools": ["research_combined_search"]
                },
                {
                    "type": "conditional",
                    "condition": "{{data_collector.status}} == 'success'",
                    "true_steps": [
                        {
                            "agent": "advanced_analyzer",
                            "content": "Perform advanced analysis on collected data",
                            "readFrom": ["data_collector"],
                            "tools": ["cognitive_create_session"]
                        }
                    ],
                    "false_steps": [
                        {
                            "agent": "basic_analyzer",
                            "content": "Perform basic analysis with limited data",
                            "readFrom": ["data_collector"]
                        }
                    ]
                },
                {
                    "type": "parallel",
                    "max_parallel": 3,
                    "steps": [
                        {
                            "agent": "report_generator",
                            "content": "Generate comprehensive report",
                            "readFrom": ["*"],
                            "tools": ["research_generate_summary"]
                        },
                        {
                            "agent": "visualization_creator",
                            "content": "Create data visualizations",
                            "readFrom": ["*"]
                        },
                        {
                            "agent": "recommendation_engine",
                            "content": "Generate strategic recommendations",
                            "readFrom": ["*"],
                            "tools": ["planning_react"]
                        }
                    ]
                }
            ]
        }
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(example_workflow, f, indent=2)
        
        self.logger.info(f"✅ Created comprehensive example workflow: {filename}")
        return filename

# Utility functions
async def run_complete_async_workflow(workflow_file: str, 
                                    config: Optional[Dict[str, Any]] = None,
                                    data_file: Optional[str] = None,
                                    max_concurrent: int = 50,
                                    features: Optional[List[str]] = None,
                                    verbose: bool = False) -> Dict[str, Any]:
    """Run complete async workflow with all features"""
    
    framework = CompleteAsyncFrameworkManager(config, verbose)
    return await framework.execute_workflow(
        workflow_file=workflow_file,
        data_file=data_file,
        max_concurrent=max_concurrent,
        features=features
    )

def run_complete_workflow_sync(workflow_file: str, 
                             config: Optional[Dict[str, Any]] = None,
                             data_file: Optional[str] = None,
                             max_concurrent: int = 50,
                             features: Optional[List[str]] = None,
                             verbose: bool = False) -> Dict[str, Any]:
    """Synchronous wrapper for complete workflow execution"""
    
    return asyncio.run(run_complete_async_workflow(
        workflow_file, config, data_file, max_concurrent, features, verbose
    ))

# Enhanced CLI Interface
def main():
    """Enhanced command-line interface"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Complete Async Workflow Framework")
    parser.add_argument("--workflow", required=True, help="Workflow JSON file")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=5, help="Max concurrent agents")
    parser.add_argument("--features", nargs="*", 
                       choices=["basic_async", "tools", "flow_control", "dynamic_agents"],
                       help="Force specific features")
    parser.add_argument("--config", help="Config file (Python module)")
    parser.add_argument("--create-example", action="store_true", 
                       help="Create comprehensive example workflow")
    parser.add_argument("--analyze", action="store_true", 
                       help="Analyze workflow without execution")
    parser.add_argument("--test-api", action="store_true", 
                       help="Test API connection")
    parser.add_argument("--verbose", "-v", action="store_true", 
                       help="Verbose logging")
    
    args = parser.parse_args()
    
    # Load custom config
    config = None
    if args.config:
        try:
            import importlib.util
            spec = importlib.util.spec_from_file_location("custom_config", args.config)
            config_module = importlib.util.module_from_spec(spec)
            spec.loader.exec_module(config_module)
            config = config_module.CONFIG
        except Exception as e:
            print(f"❌ Failed to load config {args.config}: {e}")
            sys.exit(1)
    
    # Create framework
    framework = CompleteAsyncFrameworkManager(config, args.verbose)
    
    # Test API connection
    if args.test_api:
        print("🧪 Testing API connection...")
        
        try:
            import requests
            
            headers = {"Content-Type": "application/json"}
            if framework.config.get("api_key"):
                headers["Authorization"] = f"Bearer {framework.config['api_key']}"
            
            payload = {
                "model": framework.config["default_model"],
                "messages": [{"role": "user", "content": "Hello, test response"}],
                "max_tokens": 20
            }
            
            response = requests.post(framework.config["endpoint"], json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print("✅ API connection successful!")
                data = response.json()
                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                print(f"📝 Response: {content}")
            else:
                print(f"❌ API error {response.status_code}: {response.text}")
                
        except Exception as e:
            print(f"❌ API test failed: {e}")
        
        return
    
    # Create example
    if args.create_example:
        example_file = framework.create_example_workflow()
        print(f"✅ Created example: {example_file}")
        print(f"Run with: python {sys.argv[0]} --workflow {example_file}")
        return
    
    # Analyze workflow
    if args.analyze:
        workflow = framework._load_workflow(args.workflow)
        if workflow:
            features = framework._analyze_workflow_features(workflow)
            print(f"\n📊 Workflow Analysis:")
            print(f"File: {args.workflow}")
            print(f"Total Steps: {len(workflow)}")
            print(f"Required Features: {', '.join(features) if features else 'basic_async'}")
            print(f"Available Features: {', '.join(framework.available_features)}")
            
            missing_features = set(features) - set(framework.available_features)
            if missing_features:
                print(f"⚠️ Missing Features: {', '.join(missing_features)}")
            else:
                print("✅ All features available!")
        return
    
    # Execute workflow
    print(f"🚀 Starting complete async workflow execution...")
    print(f"📋 Workflow: {args.workflow}")
    print(f"🔄 Max Concurrent: {args.concurrent}")
    if args.features:
        print(f"🎯 Forced Features: {', '.join(args.features)}")
    
    try:
        start_time = time.time()
        
        results = run_complete_workflow_sync(
            workflow_file=args.workflow,
            config=config,
            data_file=args.data,
            max_concurrent=args.concurrent,
            features=args.features,
            verbose=args.verbose
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Print final summary
        print(f"\n{'='*60}")
        print("🎉 FINAL EXECUTION SUMMARY")
        print(f"{'='*60}")
        
        if results.get("success", True):
            if "results" in results:
                exec_results = results["results"]
                total_count = results.get("total_count", len(exec_results))
                completed_count = results.get("completed_count", total_count)
                failed_count = results.get("failed_count", 0)
            else:
                total_count = len([k for k in results.keys() if k not in ['workflow_state', 'success']])
                completed_count = total_count
                failed_count = 0
            
            print(f"✅ Status: Successfully Completed")
            print(f"📊 Total Agents: {total_count}")
            print(f"✅ Completed: {completed_count}")
            print(f"❌ Failed: {failed_count}")
            print(f"⏱️ Execution Time: {execution_time:.1f}s ({execution_time/60:.1f}min)")
            
            if completed_count > 0:
                success_rate = (completed_count / total_count) * 100
                agents_per_second = completed_count / execution_time if execution_time > 0 else 0
                print(f"📈 Success Rate: {success_rate:.1f}%")
                print(f"⚡ Speed: {agents_per_second:.2f} agents/second")
            
            # Save results
            output_dir = Path(framework.config.get("output_dir", "./agent_outputs"))
            output_dir.mkdir(exist_ok=True)
            results_file = output_dir / f"complete_results_{int(start_time)}.json"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"📁 Results saved to: {results_file}")
            
            # Show sample results
            if "results" in results:
                print(f"\n📋 Sample Results Preview:")
                for i, (agent, result) in enumerate(list(results["results"].items())[:3]):
                    if isinstance(result, dict) and "content" in result:
                        content = result["content"][:120] + "..." if len(result["content"]) > 120 else result["content"]
                        print(f"  {i+1}. {agent}: {content}")
            
        else:
            print(f"❌ Workflow failed: {results.get('error', 'Unknown error')}")
            
    except KeyboardInterrupt:
        print("\n🛑 Execution interrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"❌ Execution failed: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()


==================================================
FILE: config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "qwen2.5:7b",
    "api_key": "",  # Ollama doesn't need API key
    "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    "timeout": 1200  # Increase timeout
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: openrouter_config.py
==================================================

#!/usr/bin/env python3

import os

# Configuration settings for the agent system
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "deepseek/deepseek-chat:free",
#     "default_model": "openrouter/quasar-alpha",
    "api_key": "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",
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db" 
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)


==================================================
FILE: optimized_agent_runner.py
==================================================

#!/usr/bin/env python3

import os
import sys
import json
import asyncio
import time
from typing import Dict, Any, List, Optional
from pathlib import Path
import logging

# Import optimized components
from async_executor import AsyncWorkflowExecutor, execute_large_workflow
from workflow_optimizer import WorkflowOptimizer, analyze_workflow_file, optimize_workflow_file
from optimized_call_api import APIConnectionPool, call_api_batch

# Import existing components
from tool_manager import tool_manager
from utils import extract_json_from_text, get_config

logger = logging.getLogger("optimized_agent_runner")

class OptimizedWorkflowRunner:
    """High-performance workflow runner for large-scale operations"""
    
    def __init__(self, config: Dict[str, Any], max_concurrent: int = 50):
        self.config = config
        self.max_concurrent = max_concurrent
        self.optimizer = WorkflowOptimizer()
        
        # Performance tracking
        self.start_time = None
        self.end_time = None
        self.total_agents = 0
        self.completed_agents = 0
        self.failed_agents = 0
        
    async def run_workflow_optimized(self, workflow_file: str, data_file: str = None) -> Dict[str, Any]:
        """Run workflow with all optimizations enabled"""
        
        self.start_time = time.time()
        logger.info(f"🚀 Starting optimized workflow execution: {workflow_file}")
        
        try:
            # Step 1: Analyze and optimize workflow
            logger.info("📊 Analyzing workflow for optimization...")
            optimization = optimize_workflow_file(workflow_file, self.max_concurrent)
            metrics = optimization["metrics"]
            
            logger.info(f"Workflow Analysis:")
            logger.info(f"  - Total Agents: {metrics.total_agents}")
            logger.info(f"  - Estimated Time: {metrics.estimated_time_seconds:.1f}s")
            logger.info(f"  - Estimated Cost: ${metrics.estimated_cost_usd:.2f}")
            logger.info(f"  - Max Parallelization: {metrics.max_parallel}")
            
            self.total_agents = metrics.total_agents
            
            # Step 2: Auto-discover tools
            num_tools = tool_manager.discover_tools()
            logger.info(f"🔧 Discovered {num_tools} tools")
            
            # Step 3: Execute with async executor
            logger.info("⚡ Executing workflow asynchronously...")
            
            # Adjust concurrency based on recommendations
            recommended_concurrent = optimization["optimizations"].get(
                "recommended_concurrent", self.max_concurrent
            )
            
            results = await execute_large_workflow(
                workflow_file=workflow_file,
                config=self.config,
                data_file=data_file,
                max_concurrent=recommended_concurrent
            )
            
            # Step 4: Process results
            self.completed_agents = results.get("completed_count", 0)
            self.failed_agents = results.get("failed_count", 0)
            self.end_time = time.time()
            
            # Step 5: Save results with compression
            await self._save_results_optimized(results)
            
            # Step 6: Generate performance report
            performance_report = self._generate_performance_report(optimization)
            
            logger.info("✅ Optimized workflow execution completed")
            
            return {
                "execution_results": results,
                "optimization_analysis": optimization,
                "performance_report": performance_report,
                "success": True
            }
            
        except Exception as e:
            self.end_time = time.time()
            error_msg = f"❌ Workflow execution failed: {str(e)}"
            logger.error(error_msg)
            
            return {
                "error": error_msg,
                "success": False,
                "performance_report": self._generate_performance_report() if self.start_time else None
            }
    
    async def _save_results_optimized(self, results: Dict[str, Any]):
        """Save results with optimization for large datasets"""
        
        output_dir = Path(self.config["output_dir"])
        output_dir.mkdir(exist_ok=True)
        
        # Save main results
        results_file = output_dir / "optimized_workflow_results.json"
        
        try:
            # For very large results, save with compression
            if self.total_agents > 1000:
                import gzip
                
                compressed_file = output_dir / "optimized_workflow_results.json.gz"
                with gzip.open(compressed_file, 'wt', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, default=str)
                logger.info(f"Large results saved compressed to {compressed_file}")
            else:
                with open(results_file, 'w', encoding='utf-8') as f:
                    json.dump(results, f, indent=2, default=str)
                logger.info(f"Results saved to {results_file}")
                
        except Exception as e:
            logger.error(f"Error saving results: {e}")
    
    def _generate_performance_report(self, optimization: Dict = None) -> Dict[str, Any]:
        """Generate comprehensive performance report"""
        
        execution_time = (self.end_time - self.start_time) if self.start_time and self.end_time else 0
        
        report = {
            "execution_summary": {
                "total_agents": self.total_agents,
                "completed_agents": self.completed_agents,
                "failed_agents": self.failed_agents,
                "success_rate": (self.completed_agents / self.total_agents * 100) if self.total_agents > 0 else 0,
                "execution_time_seconds": execution_time,
                "execution_time_minutes": execution_time / 60,
                "agents_per_second": self.completed_agents / execution_time if execution_time > 0 else 0
            }
        }
        
        if optimization:
            metrics = optimization["metrics"]
            report["optimization_analysis"] = {
                "estimated_vs_actual_time": {
                    "estimated_seconds": metrics.estimated_time_seconds,
                    "actual_seconds": execution_time,
                    "accuracy_percent": (metrics.estimated_time_seconds / execution_time * 100) if execution_time > 0 else 0
                },
                "parallelization_efficiency": {
                    "max_possible_parallel": metrics.max_parallel,
                    "actual_speedup": metrics.critical_path_length / execution_time if execution_time > 0 else 0
                },
                "cost_analysis": {
                    "estimated_cost_usd": metrics.estimated_cost_usd,
                    "cost_per_agent": metrics.estimated_cost_usd / metrics.total_agents if metrics.total_agents > 0 else 0
                }
            }
        
        return report

# Main execution functions
async def run_large_workflow_async(workflow_file: str, config: Dict = None, 
                                  data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Main async function for large workflow execution"""
    
    if config is None:
        config = get_config()
    
    runner = OptimizedWorkflowRunner(config, max_concurrent)
    return await runner.run_workflow_optimized(workflow_file, data_file)

def run_large_workflow_sync(workflow_file: str, config: Dict = None, 
                           data_file: str = None, max_concurrent: int = 50) -> Dict[str, Any]:
    """Synchronous wrapper for large workflow execution"""
    
    if config is None:
        config = get_config()
    
    return asyncio.run(run_large_workflow_async(workflow_file, config, data_file, max_concurrent))

def analyze_workflow_performance(workflow_file: str) -> str:
    """Analyze workflow and generate performance report"""
    
    optimizer = WorkflowOptimizer()
    report = optimizer.generate_optimization_report(workflow_file)
    
    # Save report
    report_file = f"{workflow_file}_performance_analysis.md"
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"Performance analysis saved to: {report_file}")
    return report

# Backward compatibility for existing code
def run_universal_workflow(workflow_file: str, data_file: str = None, 
                          optimize: bool = True) -> Dict[str, Any]:
    """Backward compatible function with optional optimization"""
    
    config = get_config()
    
    if optimize:
        # Use optimized execution for better performance
        return run_large_workflow_sync(workflow_file, config, data_file)
    else:
        # Fall back to original implementation
        from agent_runner import run_universal_workflow as original_runner
        return original_runner(workflow_file, data_file)

# CLI interface
def main():
    """Enhanced main function with optimization options"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Optimized Workflow Runner")
    parser.add_argument("--workflow", required=True, help="Workflow JSON file")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=50, help="Max concurrent agents")
    parser.add_argument("--analyze", action="store_true", help="Analyze workflow performance")
    parser.add_argument("--optimize", action="store_true", default=True, help="Enable optimizations")
    parser.add_argument("--report", action="store_true", help="Generate performance report only")
    
    args = parser.parse_args()
    
    if args.report:
        # Generate performance analysis report
        report = analyze_workflow_performance(args.workflow)
        print("\n" + "="*50)
        print("PERFORMANCE ANALYSIS REPORT")
        print("="*50)
        print(report)
        return
    
    if args.analyze:
        # Quick analysis without execution
        metrics = analyze_workflow_file(args.workflow)
        print(f"\n📊 Workflow Analysis:")
        print(f"Total Agents: {metrics.total_agents}")
        print(f"Critical Path: {metrics.critical_path_length} agents")
        print(f"Max Parallelization: {metrics.max_parallel} concurrent")
        print(f"Estimated Time: {metrics.estimated_time_seconds:.1f} seconds")
        print(f"Estimated Cost: ${metrics.estimated_cost_usd:.2f}")
        print(f"Memory Usage: {metrics.memory_usage_mb:.1f} MB")
        return
    
    # Execute workflow
    print(f"🚀 Starting workflow execution...")
    print(f"Workflow: {args.workflow}")
    print(f"Max Concurrent: {args.concurrent}")
    print(f"Optimizations: {'Enabled' if args.optimize else 'Disabled'}")
    
    start_time = time.time()
    
    if args.optimize:
        results = run_large_workflow_sync(
            workflow_file=args.workflow,
            data_file=args.data,
            max_concurrent=args.concurrent
        )
    else:
        results = run_universal_workflow(args.workflow, args.data, optimize=False)
    
    end_time = time.time()
    execution_time = end_time - start_time
    
    # Print results summary
    print(f"\n{'='*50}")
    print("EXECUTION SUMMARY")
    print(f"{'='*50}")
    
    if results.get("success"):
        exec_results = results.get("execution_results", {})
        print(f"✅ Workflow completed successfully")
        print(f"Total Agents: {exec_results.get('total_count', 'Unknown')}")
        print(f"Completed: {exec_results.get('completed_count', 'Unknown')}")
        print(f"Failed: {exec_results.get('failed_count', 'Unknown')}")
        print(f"Execution Time: {execution_time:.1f} seconds ({execution_time/60:.1f} minutes)")
        
        if "performance_report" in results:
            perf = results["performance_report"]["execution_summary"]
            print(f"Success Rate: {perf['success_rate']:.1f}%")
            print(f"Agents/Second: {perf['agents_per_second']:.2f}")
    else:
        print(f"❌ Workflow failed: {results.get('error', 'Unknown error')}")

if __name__ == "__main__":
    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    main()


==================================================
FILE: optimized_call_api.py
==================================================

#!/usr/bin/env python3

import asyncio
import aiohttp
import json
import time
import hashlib
from typing import Dict, List, Optional
from collections import deque
import logging
import os

logger = logging.getLogger("optimized_call_api")

class APIConnectionPool:
    """Manages HTTP connections and implements caching/rate limiting"""
    
    def __init__(self, config: Dict, max_connections: int = 100, 
                 rate_limit: float = 20.0, cache_dir: str = "./api_cache"):
        self.config = config
        self.max_connections = max_connections
        self.rate_limit = rate_limit
        self.cache_dir = cache_dir
        
        # Rate limiting
        self.request_times = deque()
        self.semaphore = asyncio.Semaphore(max_connections)
        
        # Connection pooling
        self.session = None
        self.connector = None
        
        # Response caching
        self.enable_cache = True
        os.makedirs(cache_dir, exist_ok=True)
        
        # Statistics
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "cache_misses": 0,
            "failed_requests": 0,
            "rate_limited": 0
        }
    
    async def __aenter__(self):
        """Initialize connection pool"""
        self.connector = aiohttp.TCPConnector(
            limit=self.max_connections,
            limit_per_host=30,
            keepalive_timeout=60,
            enable_cleanup_closed=True,
            use_dns_cache=True
        )
        
        timeout = aiohttp.ClientTimeout(total=300, connect=15, sock_read=120)
        self.session = aiohttp.ClientSession(
            connector=self.connector,
            timeout=timeout,
            headers={
                'Content-Type': 'application/json',
                'Connection': 'keep-alive'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Cleanup connections"""
        if self.session:
            await self.session.close()
        if self.connector:
            await self.connector.close()
    
    def _get_cache_key(self, conversation: List[Dict]) -> str:
        """Generate cache key for conversation"""
        content = json.dumps(conversation, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()
    
    def _get_cache_path(self, cache_key: str) -> str:
        """Get cache file path"""
        return os.path.join(self.cache_dir, f"{cache_key}.json")
    
    def _load_from_cache(self, cache_key: str) -> Optional[str]:
        """Load response from cache"""
        if not self.enable_cache:
            return None
            
        cache_path = self._get_cache_path(cache_key)
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'r', encoding='utf-8') as f:
                    cached_data = json.load(f)
                    
                # Check if cache is still valid (24 hours)
                cache_time = cached_data.get('timestamp', 0)
                if time.time() - cache_time < 86400:  # 24 hours
                    self.stats["cache_hits"] += 1
                    return cached_data.get('response', '')
                    
        except Exception as e:
            logger.debug(f"Cache read error: {e}")
        
        self.stats["cache_misses"] += 1
        return None
    
    def _save_to_cache(self, cache_key: str, response: str):
        """Save response to cache"""
        if not self.enable_cache:
            return
            
        cache_path = self._get_cache_path(cache_key)
        try:
            cache_data = {
                'response': response,
                'timestamp': time.time()
            }
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(cache_data, f)
        except Exception as e:
            logger.debug(f"Cache write error: {e}")
    
    async def _rate_limit(self):
        """Implement intelligent rate limiting"""
        async with self.semaphore:
            now = time.time()
            
            # Clean old request times
            while self.request_times and now - self.request_times[0] > 1.0:
                self.request_times.popleft()
            
            # Check rate limit
            if len(self.request_times) >= self.rate_limit:
                sleep_time = 1.0 - (now - self.request_times[0])
                if sleep_time > 0:
                    logger.debug(f"Rate limiting: sleeping {sleep_time:.2f}s")
                    await asyncio.sleep(sleep_time)
                    self.stats["rate_limited"] += 1
            
            self.request_times.append(now)
    
    async def call_api_async(self, conversation: List[Dict], retries: int = 3) -> str:
        """Make async API call with caching and rate limiting"""
        self.stats["total_requests"] += 1
        
        # Check cache first
        cache_key = self._get_cache_key(conversation)
        cached_response = self._load_from_cache(cache_key)
        if cached_response:
            logger.debug("Cache hit")
            return cached_response
        
        # Apply rate limiting
        await self._rate_limit()
        
        # Prepare request
        payload = {
            "model": self.config["default_model"],
            "messages": conversation,
            "temperature": 0.7,
            "max_tokens": 4000
        }
        
        headers = {}
        if self.config.get("api_key"):
            headers["Authorization"] = f"Bearer {self.config['api_key']}"
        
        # Retry logic with exponential backoff
        for attempt in range(retries):
            try:
                async with self.session.post(
                    self.config["endpoint"],
                    json=payload,
                    headers=headers
                ) as response:
                    
                    if response.status == 200:
                        data = await response.json()
                        content = (data.get('content', '') or 
                                 data.get('choices', [{}])[0].get('message', {}).get('content', ''))
                        
                        # Cache successful response
                        self._save_to_cache(cache_key, content)
                        return content
                        
                    elif response.status == 429:  # Rate limited
                        retry_after = response.headers.get('Retry-After', '60')
                        wait_time = min(int(retry_after), 2 ** attempt)
                        logger.warning(f"API rate limited, waiting {wait_time}s")
                        await asyncio.sleep(wait_time)
                        
                    elif response.status in [500, 502, 503, 504]:  # Server errors
                        wait_time = min(2 ** attempt, 60)
                        logger.warning(f"Server error {response.status}, retrying in {wait_time}s")
                        await asyncio.sleep(wait_time)
                        
                    else:
                        error_text = await response.text()
                        logger.error(f"API error {response.status}: {error_text}")
                        break
                        
            except asyncio.TimeoutError:
                wait_time = min(2 ** attempt, 60)
                logger.warning(f"Request timeout, retrying in {wait_time}s")
                await asyncio.sleep(wait_time)
                
            except Exception as e:
                logger.error(f"Request error: {e}")
                if attempt < retries - 1:
                    await asyncio.sleep(2 ** attempt)
        
        self.stats["failed_requests"] += 1
        return f"Error: API request failed after {retries} attempts"
    
    def get_stats(self) -> Dict:
        """Get connection pool statistics"""
        cache_hit_rate = 0
        if self.stats["total_requests"] > 0:
            cache_hit_rate = self.stats["cache_hits"] / self.stats["total_requests"] * 100
            
        return {
            **self.stats,
            "cache_hit_rate_percent": round(cache_hit_rate, 2)
        }

# Batch processing for multiple API calls
class BatchAPIProcessor:
    """Processes multiple API calls in optimized batches"""
    
    def __init__(self, connection_pool: APIConnectionPool, batch_size: int = 20):
        self.connection_pool = connection_pool
        self.batch_size = batch_size
    
    async def process_conversations_batch(self, conversations: List[List[Dict]]) -> List[str]:
        """Process multiple conversations in batches"""
        results = []
        
        # Process in batches to avoid overwhelming the API
        for i in range(0, len(conversations), self.batch_size):
            batch = conversations[i:i + self.batch_size]
            
            # Create tasks for concurrent execution
            tasks = [
                self.connection_pool.call_api_async(conv) 
                for conv in batch
            ]
            
            # Execute batch concurrently
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Handle results and exceptions
            for result in batch_results:
                if isinstance(result, Exception):
                    results.append(f"Error: {str(result)}")
                else:
                    results.append(result)
            
            # Small delay between batches
            if i + self.batch_size < len(conversations):
                await asyncio.sleep(0.1)
        
        return results

# Backward compatibility function
def call_api(conversation: List[Dict], config: Dict) -> str:
    """Synchronous wrapper for backward compatibility"""
    async def _async_call():
        async with APIConnectionPool(config) as pool:
            return await pool.call_api_async(conversation)
    
    # Run in event loop
    try:
        loop = asyncio.get_event_loop()
        if loop.is_running():
            # If loop is already running, create a task
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, _async_call())
                return future.result(timeout=300)
        else:
            return loop.run_until_complete(_async_call())
    except:
        # Fallback to new event loop
        return asyncio.run(_async_call())

# Utility functions
async def call_api_batch(conversations: List[List[Dict]], config: Dict, 
                        max_concurrent: int = 20) -> List[str]:
    """Process multiple conversations asynchronously"""
    async with APIConnectionPool(config, max_connections=max_concurrent) as pool:
        processor = BatchAPIProcessor(pool, batch_size=max_concurrent)
        return await processor.process_conversations_batch(conversations)

def estimate_api_cost(num_requests: int, cost_per_request: float = 0.01) -> float:
    """Estimate API costs for large workflows"""
    return num_requests * cost_per_request


==================================================
FILE: quick_setup.py
==================================================

#!/usr/bin/env python3
"""
Quick Setup Script for Async Framework
Run this first to set up everything
"""

import os
import sys
import subprocess
import json

def check_dependencies():
    """Check if required dependencies are installed"""
    print("🔍 Checking dependencies...")
    
    required = ["aiohttp", "networkx"]
    missing = []
    
    for pkg in required:
        try:
            __import__(pkg)
            print(f"✅ {pkg} - OK")
        except ImportError:
            missing.append(pkg)
            print(f"❌ {pkg} - Missing")
    
    return missing

def install_dependencies(packages):
    """Install missing dependencies"""
    if not packages:
        return True
    
    print(f"\n📦 Installing missing packages: {', '.join(packages)}")
    
    for pkg in packages:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", pkg])
            print(f"✅ Installed {pkg}")
        except subprocess.CalledProcessError:
            print(f"❌ Failed to install {pkg}")
            return False
    
    return True

def create_config():
    """Create basic configuration file"""
    config_content = '''#!/usr/bin/env python3

import os

# Basic configuration for async framework
CONFIG = {
    "output_dir": "./async_outputs",
    "default_model": "deepseek/deepseek-chat:free",
    "api_key": "",  # Add your API key here
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
}

# Ensure output directory exists
os.makedirs(CONFIG["output_dir"], exist_ok=True)
'''
    
    if not os.path.exists("config.py"):
        with open("config.py", "w") as f:
            f.write(config_content)
        print("✅ Created config.py")
    else:
        print("ℹ️  config.py already exists")

def create_simple_test():
    """Create a simple test workflow"""
    test_workflow = {
        "steps": [
            {
                "agent": "hello_agent",
                "content": "Say hello and introduce yourself as an AI assistant",
                "output_format": {
                    "type": "json",
                    "schema": {
                        "greeting": "string",
                        "introduction": "string"
                    }
                }
            },
            {
                "agent": "follow_up",
                "content": "Based on the greeting, ask the user how you can help them today",
                "readFrom": ["hello_agent"]
            }
        ]
    }
    
    with open("simple_test.json", "w") as f:
        json.dump(test_workflow, f, indent=2)
    
    print("✅ Created simple_test.json")

def main():
    """Main setup function"""
    print("🚀 Async Framework Quick Setup")
    print("=" * 40)
    
    # Check dependencies
    missing = check_dependencies()
    
    # Install missing dependencies
    if missing:
        if not install_dependencies(missing):
            print("❌ Setup failed - could not install dependencies")
            return False
    
    # Create configuration
    create_config()
    
    # Create test workflow
    create_simple_test()
    
    # Create output directory
    os.makedirs("async_outputs", exist_ok=True)
    
    print("\n🎉 Setup complete!")
    print("\n📖 Next steps:")
    print("1. Edit config.py and add your API key")
    print("2. Test the framework:")
    print("   python async_framework_main.py --workflow simple_test.json")
    print("3. Create example workflows:")
    print("   python async_framework_main.py --create-example")
    
    return True

if __name__ == "__main__":
    if main():
        sys.exit(0)
    else:
        sys.exit(1)


==================================================
FILE: setup_optimized.py
==================================================

#!/usr/bin/env python3

import subprocess
import sys
import os

def install_requirements():
    """Install required packages for optimized framework"""
    
    requirements = [
        "aiohttp>=3.8.0",
        "aiofiles>=23.0.0", 
        "networkx>=3.0",
        "requests>=2.25.0"
    ]
    
    # Add uvloop for better performance on Unix systems
    if sys.platform != "win32":
        requirements.append("uvloop>=0.17.0")
    
    print("🔧 Installing optimized framework requirements...")
    
    for req in requirements:
        try:
            subprocess.check_call([sys.executable, "-m", "pip", "install", req])
            print(f"✅ Installed: {req}")
        except subprocess.CalledProcessError as e:
            print(f"❌ Failed to install {req}: {e}")
            return False
    
    return True

def create_directories():
    """Create necessary directories"""
    directories = [
        "agent_outputs",
        "api_cache", 
        "logs",
        "workflows",
        "optimized_results"
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
        print(f"✅ Created directory: {directory}")

def create_optimized_config():
    """Create optimized configuration file"""
    
    config_content = '''#!/usr/bin/env python3

import os

# Optimized configuration for large-scale workflows
CONFIG = {
    "output_dir": "./agent_outputs",
    "memory_dir": "./agent_memory",
    "default_model": "deepseek/deepseek-chat:free",  # Change this to your model
    "api_key": "your-api-key-here",  # Add your OpenRouter API key
    "endpoint": "https://openrouter.ai/api/v1/chat/completions",
    "memory_db": "agent_memory.db",
    "sqlite_db": "test_sqlite.db",
    
    # Optimization settings
    "max_concurrent_agents": 50,
    "enable_caching": True,
    "cache_dir": "./api_cache",
    "rate_limit_per_second": 20.0,
    "max_connections": 100,
    "enable_compression": True
}

# Ensure output directories exist
os.makedirs(CONFIG["output_dir"], exist_ok=True)
os.makedirs(CONFIG["memory_dir"], exist_ok=True)
os.makedirs(CONFIG["cache_dir"], exist_ok=True)
'''
    
    with open("optimized_config.py", "w", encoding="utf-8") as f:
        f.write(config_content)
    
    print("✅ Created optimized_config.py")

def create_test_workflow():
    """Create a test workflow for validation"""
    
    test_workflow = {
        "steps": [
            {
                "agent": "test_agent_1",
                "content": "Analyze the concept of artificial intelligence and provide a brief summary.",
                "output_format": {
                    "type": "json",
                    "schema": {
                        "summary": "string",
                        "key_points": "array",
                        "complexity": "string"
                    }
                }
            },
            {
                "agent": "test_agent_2", 
                "content": "Based on the AI analysis, suggest practical applications.",
                "readFrom": ["test_agent_1"],
                "output_format": {
                    "type": "json",
                    "schema": {
                        "applications": "array",
                        "feasibility": "string",
                        "timeline": "string"
                    }
                }
            },
            {
                "agent": "test_agent_3",
                "content": "Create a summary report of the AI analysis and applications.",
                "readFrom": ["*"],
                "output_format": {
                    "type": "markdown",
                    "sections": [
                        "Overview",
                        "Key Insights", 
                        "Applications",
                        "Conclusion"
                    ]
                }
            }
        ]
    }
    
    with open("test_workflow.json", "w", encoding="utf-8") as f:
        json.dump(test_workflow, f, indent=2)
    
    print("✅ Created test_workflow.json")

def create_large_test_workflow():
    """Create a larger test workflow for performance testing"""
    
    # Generate 100 agents for stress testing
    steps = []
    
    # First batch: 10 independent analysis agents
    for i in range(10):
        steps.append({
            "agent": f"analyzer_{i+1}",
            "content": f"Analyze topic #{i+1}: Create analysis on different aspects of technology trends.",
            "output_format": {
                "type": "json",
                "schema": {
                    "analysis": "string",
                    "score": "number",
                    "recommendations": "array"
                }
            }
        })
    
    # Second batch: 20 agents that depend on first batch
    for i in range(20):
        dependency_idx = i % 10
        steps.append({
            "agent": f"synthesizer_{i+1}",
            "content": f"Synthesize insights from the analysis and provide recommendations.",
            "readFrom": [f"analyzer_{dependency_idx+1}"],
            "output_format": {
                "type": "json",
                "schema": {
                    "synthesis": "string",
                    "insights": "array",
                    "priority": "string"
                }
            }
        })
    
    # Third batch: 5 summary agents
    for i in range(5):
        start_idx = i * 4
        end_idx = start_idx + 4
        dependencies = [f"synthesizer_{j+1}" for j in range(start_idx, min(end_idx, 20))]
        
        steps.append({
            "agent": f"summarizer_{i+1}",
            "content": "Create comprehensive summary of synthesized insights.",
            "readFrom": dependencies,
            "output_format": {
                "type": "json",
                "schema": {
                    "summary": "string",
                    "key_findings": "array",
                    "conclusions": "string"
                }
            }
        })
    
    # Final agent: Overall report
    steps.append({
        "agent": "final_report",
        "content": "Generate comprehensive final report based on all analyses.",
        "readFrom": ["*"],
        "output_format": {
            "type": "markdown", 
            "sections": [
                "Executive Summary",
                "Key Findings",
                "Detailed Analysis",
                "Recommendations",
                "Conclusion"
            ]
        }
    })
    
    large_workflow = {"steps": steps}
    
    with open("large_test_workflow.json", "w", encoding="utf-8") as f:
        json.dump(large_workflow, f, indent=2)
    
    print(f"✅ Created large_test_workflow.json with {len(steps)} agents")

def main():
    """Main setup function"""
    print("🚀 Setting up Optimized Workflow Framework...")
    
    # Install requirements
    if not install_requirements():
        print("❌ Failed to install requirements")
        sys.exit(1)
    
    # Create directories
    create_directories()
    
    # Create configuration
    create_optimized_config()
    
    # Create test workflows
    import json
    create_test_workflow()
    create_large_test_workflow()
    
    print("\n🎉 Optimized framework setup complete!")
    print("\n📖 Next Steps:")
    print("1. Edit optimized_config.py and add your API key")
    print("2. Test small workflow: python optimized_agent_runner.py --workflow test_workflow.json")
    print("3. Analyze large workflow: python optimized_agent_runner.py --workflow large_test_workflow.json --analyze")
    print("4. Test optimized execution: python optimized_agent_runner.py --workflow large_test_workflow.json --concurrent 20")
    print("\n💡 Performance Tips:")
    print("- Start with --concurrent 10-20 for testing")
    print("- Use --analyze to estimate costs before running large workflows")
    print("- Enable caching in config for repeated testing")
    print("- Monitor API rate limits and adjust accordingly")

if __name__ == "__main__":
    main()


==================================================
FILE: tool_manager.py
==================================================

#!/usr/bin/env python3

import os
import sys
import json
import inspect
import importlib.util
import glob
from typing import Dict, Any, List, Optional, Callable, Set
from functools import wraps
import logging

# Set up logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger("tool_manager")

class ToolManager:
    """Enhanced tool manager with automatic function discovery"""
    
    def __init__(self):
        self.tools = {}  # Tool registry
        self.imported_modules = set()  # Track imported modules
        self.namespace_prefixes = {}  # Map module to namespace prefix
        self.excluded_files = {
            'agent_runner.py', 'tool_manager.py', 'utils.py', 'config.py', 'call_api.py',
            '__init__.py', 'cli.py', 'async_executor.py', 'async_framework_main.py',
            'workflow_executor.py', 'workflow_state.py', 'enhanced_agent_runner.py'
        }
        self.excluded_functions = {
            'main', '__init__', 'setup', 'teardown', 'test_', 'debug_'
        }
    
    def discover_tools(self, directories: List[str] = None) -> int:
        """Automatically discover all Python modules and their functions in given directories"""
        current_dir = os.path.dirname(os.path.abspath(__file__))
        if directories is None:
            directories = [current_dir]
            
            # IMPORTANT: Add COMPONENT directory if it exists
            component_dir = os.path.join(current_dir, "COMPONENT")
            if os.path.exists(component_dir):
                directories.append(component_dir)
                logger.info(f"Added COMPONENT directory: {component_dir}")
            
            # Also include any other directories within current_dir
            for item in os.listdir(current_dir):
                item_path = os.path.join(current_dir, item)
                if (os.path.isdir(item_path) and 
                    not item.startswith('.') and 
                    item not in ['__pycache__', 'COMPONENT', 'async_outputs', 'agent_outputs']):
                    directories.append(item_path)
        
        total_tools = 0
        logger.info(f"Scanning directories: {directories}")
        
        # Find all Python files
        for directory in directories:
            if not os.path.exists(directory):
                logger.warning(f"Directory does not exist: {directory}")
                continue
                
            python_files = glob.glob(os.path.join(directory, "*.py"))
            logger.info(f"Found {len(python_files)} Python files in {directory}")
            
            for py_file in python_files:
                filename = os.path.basename(py_file)
                
                # Skip excluded files
                if filename in self.excluded_files:
                    logger.debug(f"Skipping excluded file: {filename}")
                    continue
                
                module_name = filename[:-3]  # Remove .py
                
                # Skip if already imported
                if module_name in self.imported_modules:
                    logger.debug(f"Module already imported: {module_name}")
                    continue
                
                try:
                    # Import the module
                    spec = importlib.util.spec_from_file_location(module_name, py_file)
                    if spec is None or spec.loader is None:
                        logger.warning(f"Could not create spec for {py_file}")
                        continue
                        
                    module = importlib.util.module_from_spec(spec)
                    
                    # Add the module's directory to sys.path temporarily
                    module_dir = os.path.dirname(py_file)
                    if module_dir not in sys.path:
                        sys.path.insert(0, module_dir)
                        added_to_path = True
                    else:
                        added_to_path = False
                    
                    try:
                        spec.loader.exec_module(module)
                        self.imported_modules.add(module_name)
                        
                        # Determine namespace prefix
                        if hasattr(module, 'TOOL_NAMESPACE'):
                            prefix = getattr(module, 'TOOL_NAMESPACE')
                        else:
                            # By default, use the module name
                            prefix = module_name
                            
                            # Special case: if it ends with _adapter, remove that part
                            if prefix.endswith("_adapter"):
                                prefix = prefix[:-8]  # Remove "_adapter"
                        
                        self.namespace_prefixes[module_name] = prefix
                        
                        # Priority 1: If the module has TOOL_REGISTRY, register those tools
                        registry_tools = 0
                        if hasattr(module, 'TOOL_REGISTRY'):
                            tool_registry = getattr(module, 'TOOL_REGISTRY')
                            if isinstance(tool_registry, dict):
                                for tool_id, tool_handler in tool_registry.items():
                                    full_tool_id = f"{prefix}:{tool_id}" if ':' not in tool_id else tool_id
                                    self.register_tool(full_tool_id, tool_handler)
                                    registry_tools += 1
                                    total_tools += 1
                                logger.info(f"Registered {registry_tools} tools from TOOL_REGISTRY in {module_name}")
                        
                        # Priority 2: Auto-discover functions that should be registered as tools
                        auto_tools = self._discover_module_tools(module, prefix)
                        total_tools += len(auto_tools)
                        
                        logger.info(f"Imported module {module_name} with {registry_tools} registry tools + {len(auto_tools)} auto-discovered functions")
                        
                    finally:
                        # Remove from sys.path if we added it
                        if added_to_path and module_dir in sys.path:
                            sys.path.remove(module_dir)
                    
                except Exception as e:
                    logger.error(f"Error importing {module_name} from {py_file}: {e}")
                    # Continue with other modules even if one fails
                    continue
        
        logger.info(f"🔧 Total tools discovered: {total_tools}")
        return total_tools
    
    def _discover_module_tools(self, module, prefix: str) -> List[str]:
        """Discover and register tools from a module"""
        discovered_tools = []
        
        # Get all functions from the module
        for name, obj in inspect.getmembers(module):
            # Skip private functions, special methods, and non-functions
            if name.startswith('_') or not inspect.isfunction(obj):
                continue
            
            # Skip functions that start with excluded prefixes
            if any(name.startswith(excluded) for excluded in self.excluded_functions):
                continue
                
            # Skip functions that are already in TOOL_REGISTRY (avoid duplicates)
            if hasattr(module, 'TOOL_REGISTRY'):
                tool_registry = getattr(module, 'TOOL_REGISTRY')
                if isinstance(tool_registry, dict) and any(handler == obj for handler in tool_registry.values()):
                    continue
            
            # Check if function has a docstring (we only want documented functions)
            if obj.__doc__ and obj.__doc__.strip():
                # Create a tool ID based on the prefix and function name
                tool_id = f"{prefix}:{name}"
                self.register_tool(tool_id, obj)
                discovered_tools.append(tool_id)
                logger.debug(f"Auto-registered tool: {tool_id}")
        
        return discovered_tools
    
    def register_tool(self, tool_id: str, handler: Callable) -> None:
        """Register a function as a tool with flexible parameter handling"""
        @wraps(handler)
        def flexible_handler(**kwargs):
            try:
                # Get function signature
                sig = inspect.signature(handler)
                
                # If function takes **kwargs, pass everything
                if any(param.kind == param.VAR_KEYWORD for param in sig.parameters.values()):
                    return handler(**kwargs)
                
                # Otherwise, filter kwargs to match function signature
                filtered_kwargs = {}
                for param_name, param in sig.parameters.items():
                    if param_name in kwargs:
                        filtered_kwargs[param_name] = kwargs[param_name]
                    elif param.default == param.empty and param.kind not in (param.VAR_POSITIONAL, param.VAR_KEYWORD):
                        # Required parameter missing
                        logger.warning(f"Required parameter '{param_name}' missing for tool {tool_id}")
                        return {"error": f"Required parameter '{param_name}' missing"}
                
                return handler(**filtered_kwargs)
                
            except Exception as e:
                logger.error(f"Tool execution failed for {tool_id}: {str(e)}")
                return {"error": f"Tool execution failed: {str(e)}"}
        
        self.tools[tool_id] = flexible_handler
        logger.debug(f"Registered tool: {tool_id}")
    
    def execute_tool(self, tool_id: str, **kwargs) -> Any:
        """Execute a registered tool"""
        if tool_id not in self.tools:
            # Try to find it by prefix and auto-load
            if ':' in tool_id:
                prefix = tool_id.split(':', 1)[0]
                
                # Look for modules that might contain this tool
                for module_name, module_prefix in self.namespace_prefixes.items():
                    if module_prefix == prefix and module_name not in self.imported_modules:
                        # Try to import module
                        self._try_load_module(module_name, prefix)
                        break
        
        if tool_id not in self.tools:
            available_tools = self.get_available_tools_by_prefix(tool_id.split(':', 1)[0] if ':' in tool_id else '')
            error_msg = f"Unknown tool: {tool_id}"
            if available_tools:
                error_msg += f". Available tools with similar prefix: {', '.join(available_tools[:5])}"
            return {"error": error_msg}
        
        try:
            logger.info(f"Executing tool {tool_id} with params: {list(kwargs.keys())}")
            result = self.tools[tool_id](**kwargs)
            logger.debug(f"Tool {tool_id} executed successfully")
            return result
        except Exception as e:
            logger.error(f"Error executing tool {tool_id}: {str(e)}")
            return {"error": str(e)}
    
    def _try_load_module(self, module_name: str, prefix: str):
        """Try to load a module that might contain tools"""
        try:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            
            # Check multiple possible locations
            possible_paths = [
                os.path.join(current_dir, f"{module_name}.py"),
                os.path.join(current_dir, "COMPONENT", f"{module_name}.py"),
                os.path.join(current_dir, "tools", f"{module_name}.py"),
                os.path.join(current_dir, "adapters", f"{module_name}.py"),
            ]
            
            for module_path in possible_paths:
                if os.path.exists(module_path):
                    spec = importlib.util.spec_from_file_location(module_name, module_path)
                    if spec and spec.loader:
                        module = importlib.util.module_from_spec(spec)
                        spec.loader.exec_module(module)
                        self._discover_module_tools(module, prefix)
                        self.imported_modules.add(module_name)
                        logger.info(f"Dynamically loaded module: {module_name}")
                        break
                        
        except Exception as e:
            logger.warning(f"Failed to dynamically load module {module_name}: {e}")
    
    def get_all_tools(self) -> List[str]:
        """Get a list of all registered tool IDs"""
        return sorted(list(self.tools.keys()))
    
    def get_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get tools by namespace prefix"""
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(f"{prefix}:")]
    
    def get_available_tools_by_prefix(self, prefix: str) -> List[str]:
        """Get available tools that match a prefix"""
        if not prefix:
            return []
        return [tool_id for tool_id in self.tools.keys() if tool_id.startswith(prefix)]
    
    def is_tool_available(self, tool_id: str) -> bool:
        """Check if a tool is available"""
        return tool_id in self.tools
    
    def get_tool_info(self, tool_id: str) -> Dict[str, Any]:
        """Get information about a specific tool"""
        if tool_id not in self.tools:
            return {"error": f"Tool not found: {tool_id}"}
        
        try:
            # Get the original function from the wrapper
            original_func = self.tools[tool_id].__wrapped__ if hasattr(self.tools[tool_id], '__wrapped__') else self.tools[tool_id]
            
            # Get function signature and docstring
            sig = inspect.signature(original_func)
            
            return {
                "tool_id": tool_id,
                "name": original_func.__name__,
                "module": original_func.__module__ if hasattr(original_func, '__module__') else 'unknown',
                "docstring": original_func.__doc__ or "No documentation available",
                "parameters": {
                    name: {
                        "type": str(param.annotation) if param.annotation != param.empty else "Any",
                        "default": str(param.default) if param.default != param.empty else "Required",
                        "kind": str(param.kind)
                    }
                    for name, param in sig.parameters.items()
                }
            }
        except Exception as e:
            return {"error": f"Could not get tool info: {str(e)}"}
    
    def list_tools_by_module(self) -> Dict[str, List[str]]:
        """List tools organized by module/namespace"""
        tools_by_module = {}
        for tool_id in self.tools.keys():
            if ':' in tool_id:
                prefix, _ = tool_id.split(':', 1)
                if prefix not in tools_by_module:
                    tools_by_module[prefix] = []
                tools_by_module[prefix].append(tool_id)
            else:
                if 'global' not in tools_by_module:
                    tools_by_module['global'] = []
                tools_by_module['global'].append(tool_id)
        
        return tools_by_module
    
    def get_stats(self) -> Dict[str, Any]:
        """Get statistics about the tool manager"""
        tools_by_module = self.list_tools_by_module()
        
        return {
            "total_tools": len(self.tools),
            "total_modules": len(self.imported_modules),
            "tools_by_module": {k: len(v) for k, v in tools_by_module.items()},
            "namespaces": list(self.namespace_prefixes.values())
        }

# Create a global tool manager instance
tool_manager = ToolManager()

# Additional utility functions that can be used as tools themselves
def list_all_tools(**kwargs) -> Dict[str, Any]:
    """List all available tools"""
    return {
        "tools": tool_manager.get_all_tools(),
        "stats": tool_manager.get_stats(),
        "by_module": tool_manager.list_tools_by_module()
    }

def get_tool_info(**kwargs) -> Dict[str, Any]:
    """Get information about a specific tool"""
    tool_id = kwargs.get('tool_id', '')
    if not tool_id:
        return {"error": "tool_id parameter is required"}
    
    return tool_manager.get_tool_info(tool_id)

def reload_tools(**kwargs) -> Dict[str, Any]:
    """Reload and rediscover all tools"""
    try:
        # Clear current tools and modules
        tool_manager.tools.clear()
        tool_manager.imported_modules.clear()
        tool_manager.namespace_prefixes.clear()
        
        # Rediscover tools
        total_tools = tool_manager.discover_tools()
        
        return {
            "success": True,
            "total_tools": total_tools,
            "stats": tool_manager.get_stats()
        }
    except Exception as e:
        return {"error": f"Failed to reload tools: {str(e)}"}

# Register these utility tools
tool_manager.register_tool("tools:list_all", list_all_tools)
tool_manager.register_tool("tools:get_info", get_tool_info)
tool_manager.register_tool("tools:reload", reload_tools)


==================================================
FILE: utils.py
==================================================

#!/usr/bin/env python3

import os
import json
import re
import datetime
from typing import Dict, Any, List, Optional, Set

def log_api(agent_name, prompt, response):
    """Simple function to log API calls and responses to a file."""
    logs_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "logs")
    os.makedirs(logs_dir, exist_ok=True)
    
    timestamp = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_file = os.path.join(logs_dir, f"{agent_name}_api.log")
    
    with open(log_file, 'a', encoding='utf-8') as f:
        f.write(f"\n==== API CALL AT {timestamp} ====\n")
        f.write(f"PROMPT:\n{prompt}")
        f.write(f"\n\n==== API RESPONSE ====\n")
        f.write(f"{response}")
        f.write("\n\n")

def extract_json_from_text(text: str) -> Dict[str, Any]:
    """Extract JSON from text, handling various formats"""
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass
    
    # Try to find JSON within code blocks
    json_pattern = r'```(?:json)?\s*([\s\S]*?)\s*```'
    match = re.search(json_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    # Try to find anything that looks like a JSON object
    object_pattern = r'({[\s\S]*?})'
    match = re.search(object_pattern, text)
    if match:
        try:
            return json.loads(match.group(1))
        except json.JSONDecodeError:
            pass
    
    return {"error": "Could not extract valid JSON from response", "text": text[:500]}

def extract_tool_calls(response_content: str) -> List[Dict[str, Any]]:
    """Extract all tool calls from a response"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    tool_calls = []
    
    matches = re.finditer(tool_usage_pattern, response_content, re.DOTALL)
    for match in matches:
        tool_name = match.group(1).strip()
        params_text = "{" + match.group(2) + "}"
        try:
            params = json.loads(params_text)
            tool_calls.append({
                "tool_name": tool_name,
                "params": params,
                "full_text": match.group(0)
            })
        except json.JSONDecodeError:
            continue
    
    return tool_calls

def process_single_tool_call(response_content):
    """Process a response that may contain a single tool call"""
    tool_usage_pattern = r"I need to use the tool: ([a-zA-Z0-9_:]+)\s*\nParameters:\s*\{([^}]+)\}"
    match = re.search(tool_usage_pattern, response_content, re.DOTALL)
    
    if not match:
        return None, response_content
    
    tool_name = match.group(1).strip()
    params_text = "{" + match.group(2) + "}"
    
    try:
        params = json.loads(params_text)
        return {
            "tool_name": tool_name,
            "params": params
        }, response_content
    except json.JSONDecodeError:
        return None, response_content

def is_hashable(obj):
    """Check if an object can be used as a dictionary key"""
    try:
        hash(obj)
        return True
    except TypeError:
        return False

def get_config():
    """Get configuration or create default config"""
    try:
        from config import CONFIG
        return CONFIG
    except ImportError:
        try:
            from openrouter_config import CONFIG
            return CONFIG
        except ImportError:
            current_dir = os.path.dirname(os.path.abspath(__file__))
            CONFIG = {
                "api_key": os.environ.get("OPENROUTER_API_KEY", ""),
                "endpoint": "https://openrouter.ai/api/v1/chat/completions",
                "default_model": "deepseek/deepseek-chat:free",
                "output_dir": os.path.join(current_dir, "async_outputs")
            }
            os.makedirs(CONFIG["output_dir"], exist_ok=True)
            return CONFIG


==================================================
FILE: workflow_fix.py
==================================================

#!/usr/bin/env python3
"""
Quick fix to ensure workflows run with real API calls instead of simulated responses
"""

import asyncio
import json
import logging
import sys
import os
import time
from typing import Dict, Any, List, Optional
from pathlib import Path

# Simple logging setup
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger("workflow_fix")

# Import with error handling
try:
    from utils import get_config
except ImportError:
    def get_config():
        return {
            "output_dir": "./agent_outputs",
            "memory_dir": "./agent_memory",
            "default_model": "qwen2.5:7b",
            "api_key": "",  # Ollama doesn't need API key
            "endpoint": "http://localhost:11434/v1/chat/completions",  # OpenAI-compatible endpoint
            "memory_db": "agent_memory.db",
            "sqlite_db": "test_sqlite.db",
            "timeout": 1200  # Increase timeout
        }

try:
    from tool_manager import tool_manager
except ImportError:
    tool_manager = None

class WorkflowFixManager:
    """Simplified workflow manager that ensures real API calls"""
    
    def __init__(self, config: Optional[Dict[str, Any]] = None):
        self.config = config or get_config()
        logger.info("🔧 Workflow Fix Manager initialized")
        
        # Create output directories
        os.makedirs(self.config.get("output_dir", "./agent_outputs"), exist_ok=True)
        os.makedirs(self.config.get("memory_dir", "./agent_memory"), exist_ok=True)
        
        logger.info(f"🤖 Model: {self.config['default_model']}")
        logger.info(f"🌐 Endpoint: {self.config['endpoint']}")
        
        # Ollama doesn't need API key, but check if endpoint is accessible
        if "localhost" in self.config["endpoint"] or "127.0.0.1" in self.config["endpoint"]:
            logger.info("🔧 Using local Ollama endpoint (no API key required)")
        else:
            logger.info("🔑 Using external endpoint")
        
    async def execute_workflow_with_real_calls(self, workflow_file: str, 
                                             data_file: Optional[str] = None,
                                             max_concurrent: int = 5) -> Dict[str, Any]:
        """Execute workflow ensuring real API calls"""
        
        # Load workflow
        workflow = self._load_workflow(workflow_file)
        if not workflow:
            return {"error": "Failed to load workflow"}
        
        logger.info(f"📋 Loaded workflow with {len(workflow)} steps")
        
        # Auto-discover tools
        if tool_manager:
            tool_count = tool_manager.discover_tools()
            logger.info(f"🔧 Discovered {tool_count} tools")
        
        # Execute using tool-enabled async executor
        try:
            from async_tool_integration import AsyncToolIntegratedExecutor
            
            logger.info("🚀 Using AsyncToolIntegratedExecutor for real API calls")
            
            async with AsyncToolIntegratedExecutor(self.config, max_concurrent) as executor:
                results = await executor.execute_workflow_with_tools(workflow, data_file)
                
                # Log execution summary
                completed = results.get("completed_count", 0)
                failed = results.get("failed_count", 0)
                total = results.get("total_count", len(workflow))
                
                logger.info(f"📊 Execution complete: {completed}/{total} agents succeeded")
                
                return results
                
        except ImportError as e:
            logger.error(f"❌ AsyncToolIntegratedExecutor not available: {e}")
            
            # Fallback to basic async executor
            logger.info("🔄 Falling back to basic AsyncWorkflowExecutor")
            
            from async_executor import AsyncWorkflowExecutor
            
            async with AsyncWorkflowExecutor(self.config, max_concurrent) as executor:
                results = await executor.execute_workflow_async(workflow, data_file)
                
                completed = results.get("completed_count", 0)
                failed = results.get("failed_count", 0)
                total = results.get("total_count", len(workflow))
                
                logger.info(f"📊 Execution complete: {completed}/{total} agents succeeded")
                
                return results
    
    def _load_workflow(self, workflow_file: str) -> Optional[List[Dict[str, Any]]]:
        """Load workflow from file"""
        try:
            with open(workflow_file, 'r', encoding='utf-8') as f:
                workflow_data = json.load(f)
            
            if isinstance(workflow_data, dict) and "steps" in workflow_data:
                return workflow_data["steps"]
            elif isinstance(workflow_data, list):
                return workflow_data
            else:
                logger.error("❌ Invalid workflow format")
                return None
                
        except Exception as e:
            logger.error(f"❌ Failed to load workflow {workflow_file}: {e}")
            return None

def run_workflow_with_real_calls(workflow_file: str, 
                                data_file: Optional[str] = None,
                                max_concurrent: int = 5) -> Dict[str, Any]:
    """Synchronous wrapper to run workflow with real API calls"""
    
    async def _run():
        manager = WorkflowFixManager()
        return await manager.execute_workflow_with_real_calls(workflow_file, data_file, max_concurrent)
    
    return asyncio.run(_run())

def main():
    """Main execution function"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Workflow Fix - Force Real API Calls")
    parser.add_argument("--workflow", required=True, help="Workflow JSON file")
    parser.add_argument("--data", help="Data file to process")
    parser.add_argument("--concurrent", type=int, default=5, help="Max concurrent agents")
    parser.add_argument("--test-api", action="store_true", help="Test API connection first")
    
    args = parser.parse_args()
    
    # Test API connection if requested
    if args.test_api:
        print("🧪 Testing API connection...")
        
        config = get_config()
        print(f"🌐 Testing endpoint: {config['endpoint']}")
        print(f"🤖 Using model: {config['default_model']}")
        
        # Test with a simple request
        try:
            import requests
            
            headers = {"Content-Type": "application/json"}
            # Only add Authorization header if API key exists
            if config.get("api_key"):
                headers["Authorization"] = f"Bearer {config['api_key']}"
            
            payload = {
                "model": config["default_model"],
                "messages": [{"role": "user", "content": "Hello, this is a test"}],
                "max_tokens": 10
            }
            
            response = requests.post(config["endpoint"], json=payload, headers=headers, timeout=30)
            
            if response.status_code == 200:
                print("✅ API connection successful!")
                data = response.json()
                content = data.get('choices', [{}])[0].get('message', {}).get('content', '')
                print(f"📝 Response: {content}")
            else:
                print(f"❌ API error {response.status_code}")
                print(f"📋 Response: {response.text}")
                
        except Exception as e:
            print(f"❌ API test failed: {e}")
        
        return
    
    # Execute workflow
    print(f"🚀 Starting workflow with real API calls...")
    print(f"📋 Workflow: {args.workflow}")
    print(f"🔄 Max concurrent: {args.concurrent}")
    
    try:
        start_time = time.time()
        
        results = run_workflow_with_real_calls(
            workflow_file=args.workflow,
            data_file=args.data,
            max_concurrent=args.concurrent
        )
        
        end_time = time.time()
        execution_time = end_time - start_time
        
        # Print summary
        print(f"\n{'='*50}")
        print("🎉 EXECUTION SUMMARY")
        print(f"{'='*50}")
        
        if isinstance(results, dict) and "error" not in results:
            if "results" in results:
                exec_results = results["results"]
                total_count = results.get("total_count", len(exec_results))
                completed_count = results.get("completed_count", total_count)
                failed_count = results.get("failed_count", 0)
            else:
                total_count = len([k for k in results.keys() if k != 'workflow_state'])
                completed_count = total_count
                failed_count = 0
            
            print(f"✅ Status: Completed")
            print(f"📊 Agents: {completed_count}/{total_count} successful")
            print(f"❌ Failed: {failed_count}")
            print(f"⏱️ Time: {execution_time:.1f}s ({execution_time/60:.1f}min)")
            
            if completed_count > 0:
                print(f"⚡ Speed: {completed_count/execution_time:.2f} agents/second")
            
            # Save results
            output_dir = Path(config.get("output_dir", "./agent_outputs"))
            output_dir.mkdir(exist_ok=True)
            results_file = output_dir / f"fixed_results_{int(start_time)}.json"
            
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=2, default=str)
            
            print(f"📁 Results saved to: {results_file}")
            
            # Show first few results
            if "results" in results:
                print(f"\n📋 Sample Results:")
                for i, (agent, result) in enumerate(list(results["results"].items())[:3]):
                    if isinstance(result, dict) and "content" in result:
                        content = result["content"][:100] + "..." if len(result["content"]) > 100 else result["content"]
                        print(f"  {i+1}. {agent}: {content}")
            
        else:
            print(f"❌ Workflow failed: {results.get('error', 'Unknown error')}")
            
    except Exception as e:
        print(f"❌ Execution failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()


==================================================
FILE: workflow_optimizer.py
==================================================

#!/usr/bin/env python3

import json
import networkx as nx
from typing import Dict, Any, List, Set, Tuple
from dataclasses import dataclass
import logging

logger = logging.getLogger("workflow_optimizer")

@dataclass
class OptimizationMetrics:
    """Metrics for workflow optimization"""
    total_agents: int
    max_parallel: int
    estimated_batches: int
    estimated_time_seconds: float
    estimated_cost_usd: float
    memory_usage_mb: float
    critical_path_length: int

class WorkflowOptimizer:
    """Optimizes workflows for large-scale execution"""
    
    def __init__(self, avg_agent_time: float = 2.0, cost_per_request: float = 0.01):
        self.avg_agent_time = avg_agent_time  # seconds per agent
        self.cost_per_request = cost_per_request  # USD per API call
        self.avg_result_size = 5  # KB per result
    
    def analyze_workflow(self, workflow: List[Dict]) -> OptimizationMetrics:
        """Analyze workflow and provide optimization metrics"""
        
        # Build dependency graph
        graph = self._build_networkx_graph(workflow)
        
        # Calculate metrics
        total_agents = len(workflow)
        critical_path = self._find_critical_path(graph)
        critical_path_length = len(critical_path)
        
        # Estimate parallelization
        levels = self._calculate_execution_levels(graph)
        max_parallel = max(len(level) for level in levels) if levels else 1
        estimated_batches = len(levels)
        
        # Time estimation (critical path determines minimum time)
        estimated_time = critical_path_length * self.avg_agent_time
        
        # Cost estimation
        estimated_cost = total_agents * self.cost_per_request
        
        # Memory estimation
        memory_usage = total_agents * self.avg_result_size / 1024  # MB
        
        return OptimizationMetrics(
            total_agents=total_agents,
            max_parallel=max_parallel,
            estimated_batches=estimated_batches,
            estimated_time_seconds=estimated_time,
            estimated_cost_usd=estimated_cost,
            memory_usage_mb=memory_usage,
            critical_path_length=critical_path_length
        )
    
    def optimize_workflow(self, workflow: List[Dict], max_concurrent: int = 50) -> Dict[str, Any]:
        """Optimize workflow for execution"""
        
        # Analyze current workflow
        metrics = self.analyze_workflow(workflow)
        
        # Build recommendations
        recommendations = []
        optimizations = {}
        
        # Memory optimization
        if metrics.memory_usage_mb > 1000:  # > 1GB
            recommendations.append("Consider implementing result streaming for large workflows")
            optimizations["use_disk_cache"] = True
        
        # Parallelization optimization
        if metrics.max_parallel > max_concurrent:
            recommendations.append(f"Workflow can utilize {metrics.max_parallel} parallel agents, "
                                 f"but limited to {max_concurrent} by configuration")
            optimizations["recommended_concurrent"] = min(metrics.max_parallel, max_concurrent * 2)
        
        # Cost optimization
        if metrics.estimated_cost_usd > 10:
            recommendations.append("Consider result caching to reduce API costs")
            optimizations["enable_caching"] = True
        
        # Execution order optimization
        optimized_batches = self._create_optimal_batches(workflow, max_concurrent)
        
        return {
            "metrics": metrics,
            "recommendations": recommendations,
            "optimizations": optimizations,
            "execution_plan": {
                "batches": len(optimized_batches),
                "max_batch_size": max(len(batch) for batch in optimized_batches),
                "execution_order": optimized_batches
            }
        }
    
    def _build_networkx_graph(self, workflow: List[Dict]) -> nx.DiGraph:
        """Build NetworkX directed graph from workflow"""
        graph = nx.DiGraph()
        
        # Add nodes
        for step in workflow:
            agent_name = step.get("agent", "")
            if agent_name:
                graph.add_node(agent_name, **step)
        
        # Add edges (dependencies)
        for step in workflow:
            agent_name = step.get("agent", "")
            if not agent_name:
                continue
                
            read_from = step.get("readFrom", [])
            for ref in read_from:
                if isinstance(ref, str) and ref != "*":
                    if ref in graph.nodes:
                        graph.add_edge(ref, agent_name)
                elif ref == "*":
                    # Depends on all previous agents
                    for prev_step in workflow:
                        if prev_step.get("agent") == agent_name:
                            break
                        prev_agent = prev_step.get("agent")
                        if prev_agent and prev_agent in graph.nodes:
                            graph.add_edge(prev_agent, agent_name)
        
        return graph
    
    def _find_critical_path(self, graph: nx.DiGraph) -> List[str]:
        """Find the critical path (longest path) in the dependency graph"""
        try:
            # For DAG, find longest path
            if nx.is_directed_acyclic_graph(graph):
                topo_order = list(nx.topological_sort(graph))
                
                # Calculate longest path to each node
                distances = {node: 0 for node in graph.nodes}
                predecessors = {}
                
                for node in topo_order:
                    for successor in graph.successors(node):
                        if distances[node] + 1 > distances[successor]:
                            distances[successor] = distances[node] + 1
                            predecessors[successor] = node
                
                # Find node with maximum distance
                end_node = max(distances, key=distances.get)
                
                # Reconstruct path
                path = []
                current = end_node
                while current is not None:
                    path.append(current)
                    current = predecessors.get(current)
                
                return list(reversed(path))
            else:
                logger.warning("Graph contains cycles - using topological sort")
                return list(nx.topological_sort(graph))[:10]  # Limit to prevent issues
                
        except Exception as e:
            logger.error(f"Error finding critical path: {e}")
            return []
    
    def _calculate_execution_levels(self, graph: nx.DiGraph) -> List[List[str]]:
        """Calculate execution levels for parallel processing"""
        try:
            if not nx.is_directed_acyclic_graph(graph):
                logger.warning("Graph contains cycles - cannot calculate levels properly")
                return [[node] for node in graph.nodes]
            
            levels = []
            remaining_nodes = set(graph.nodes)
            
            while remaining_nodes:
                # Find nodes with no dependencies in remaining nodes
                current_level = []
                for node in remaining_nodes.copy():
                    predecessors = set(graph.predecessors(node))
                    if not predecessors.intersection(remaining_nodes):
                        current_level.append(node)
                        remaining_nodes.remove(node)
                
                if current_level:
                    levels.append(current_level)
                else:
                    # Handle remaining nodes (possible cycle)
                    levels.append(list(remaining_nodes))
                    break
            
            return levels
            
        except Exception as e:
            logger.error(f"Error calculating execution levels: {e}")
            return [[node] for node in graph.nodes]
    
    def _create_optimal_batches(self, workflow: List[Dict], max_concurrent: int) -> List[List[str]]:
        """Create optimal execution batches"""
        graph = self._build_networkx_graph(workflow)
        levels = self._calculate_execution_levels(graph)
        
        # Split large levels into smaller batches
        batches = []
        for level in levels:
            if len(level) <= max_concurrent:
                batches.append(level)
            else:
                # Split level into smaller batches
                for i in range(0, len(level), max_concurrent):
                    batch = level[i:i + max_concurrent]
                    batches.append(batch)
        
        return batches
    
    def generate_optimization_report(self, workflow_file: str, max_concurrent: int = 50) -> str:
        """Generate a comprehensive optimization report"""
        
        # Load workflow
        with open(workflow_file, 'r', encoding='utf-8') as f:
            workflow_data = json.load(f)
        
        if isinstance(workflow_data, dict) and "steps" in workflow_data:
            workflow = workflow_data["steps"]
        else:
            workflow = workflow_data
        
        # Analyze and optimize
        optimization = self.optimize_workflow(workflow, max_concurrent)
        metrics = optimization["metrics"]
        
        # Generate report
        report = f"""
# Workflow Optimization Report

## Workflow Overview
- **Total Agents**: {metrics.total_agents}
- **Critical Path Length**: {metrics.critical_path_length} agents
- **Maximum Parallelization**: {metrics.max_parallel} concurrent agents

## Performance Estimates
- **Estimated Execution Time**: {metrics.estimated_time_seconds:.1f} seconds ({metrics.estimated_time_seconds/60:.1f} minutes)
- **Estimated Cost**: ${metrics.estimated_cost_usd:.2f}
- **Memory Usage**: {metrics.memory_usage_mb:.1f} MB
- **Execution Batches**: {metrics.estimated_batches}

## Optimization Recommendations
"""
        
        for rec in optimization["recommendations"]:
            report += f"- {rec}\n"
        
        report += f"""
## Execution Plan
- **Number of Batches**: {optimization['execution_plan']['batches']}
- **Largest Batch Size**: {optimization['execution_plan']['max_batch_size']} agents
- **Recommended Concurrency**: {optimization['optimizations'].get('recommended_concurrent', max_concurrent)}

## Optimization Settings
"""
        
        for key, value in optimization["optimizations"].items():
            report += f"- **{key}**: {value}\n"
        
        return report

# Utility functions
def analyze_workflow_file(workflow_file: str) -> OptimizationMetrics:
    """Quick analysis of a workflow file"""
    optimizer = WorkflowOptimizer()
    
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow_data = json.load(f)
    
    if isinstance(workflow_data, dict) and "steps" in workflow_data:
        workflow = workflow_data["steps"]
    else:
        workflow = workflow_data
    
    return optimizer.analyze_workflow(workflow)

def optimize_workflow_file(workflow_file: str, max_concurrent: int = 50) -> Dict[str, Any]:
    """Optimize a workflow file"""
    optimizer = WorkflowOptimizer()
    
    with open(workflow_file, 'r', encoding='utf-8') as f:
        workflow_data = json.load(f)
    
    if isinstance(workflow_data, dict) and "steps" in workflow_data:
        workflow = workflow_data["steps"]
    else:
        workflow = workflow_data
    
    return optimizer.optimize_workflow(workflow, max_concurrent)


==================================================
SUMMARY: Processed 16 Python files
Output saved to: all_python_files.txt
